From ee8485e1b79de27b4ce5573dce128d429ccaae03 Mon Sep 17 00:00:00 2001
From: Barry Song <song.bao.hua@hisilicon.com>
Date: Mon, 17 Oct 2022 15:34:27 +0800
Subject: [PATCH 765/774] sched/fair: Scan cluster before scanning LLC in
 wake-up path

kunpeng inclusion
category: feature
bugzilla: https://gitee.com/openeuler/kernel/issues/I5W44S
Reference: https://lore.kernel.org/lkml/20220915073423.25535-1-yangyicong@huawei.com/

----------------------------------------------------------------------

For platforms having clusters like Kunpeng920, CPUs within the same cluster
have lower latency when synchronizing and accessing shared resources like
cache. Thus, this patch tries to find an idle cpu within the cluster of the
target CPU before scanning the whole LLC to gain lower latency.

Testing has been done on Kunpeng920 by pinning tasks to one numa and two
numa. On Kunpeng920, Each numa has 8 clusters and each cluster has 4 CPUs.

With this patch, We noticed enhancement on tbench within one numa or cross
two numa.

On numa 0:
                             6.0-rc1                patched
Hmean     1        351.20 (   0.00%)      396.45 *  12.88%*
Hmean     2        700.43 (   0.00%)      793.76 *  13.32%*
Hmean     4       1404.42 (   0.00%)     1583.62 *  12.76%*
Hmean     8       2833.31 (   0.00%)     3147.85 *  11.10%*
Hmean     16      5501.90 (   0.00%)     6089.89 *  10.69%*
Hmean     32     10428.59 (   0.00%)    10619.63 *   1.83%*
Hmean     64      8223.39 (   0.00%)     8306.93 *   1.02%*
Hmean     128     7042.88 (   0.00%)     7068.03 *   0.36%*

On numa 0-1:
                             6.0-rc1                patched
Hmean     1        363.06 (   0.00%)      397.13 *   9.38%*
Hmean     2        721.68 (   0.00%)      789.84 *   9.44%*
Hmean     4       1435.15 (   0.00%)     1566.01 *   9.12%*
Hmean     8       2776.17 (   0.00%)     3007.05 *   8.32%*
Hmean     16      5471.71 (   0.00%)     6103.91 *  11.55%*
Hmean     32     10164.98 (   0.00%)    11531.81 *  13.45%*
Hmean     64     17143.28 (   0.00%)    20078.68 *  17.12%*
Hmean     128    14552.70 (   0.00%)    15156.41 *   4.15%*
Hmean     256    12827.37 (   0.00%)    13326.86 *   3.89%*

Note neither Kunpeng920 nor x86 Jacobsville supports SMT, so the SMT branch
in the code has not been tested but it supposed to work.

Suggested-by: Peter Zijlstra <peterz@infradead.org>
[https://lore.kernel.org/lkml/Ytfjs+m1kUs0ScSn@worktop.programming.kicks-ass.net]
Tested-by: Yicong Yang <yangyicong@hisilicon.com>
Signed-off-by: Barry Song <song.bao.hua@hisilicon.com>
Signed-off-by: Yicong Yang <yangyicong@hisilicon.com>
Reviewed-by: Tim Chen <tim.c.chen@linux.intel.com>
Reviewed-by: Chen Yu <yu.c.chen@intel.com>
Signed-off-by: Jie Liu <liujie375@h-partners.com>

Conflicts:
	kernel/sched/fair.c
	kernel/sched/sched.h
	kernel/sched/topology.c

Signed-off-by: Jiang Yi <jiangyi38@hisilicon.com>
---
 kernel/sched/fair.c     | 50 +++++++++++++++++++++++++++++++++++++----
 kernel/sched/sched.h    |  1 +
 kernel/sched/topology.c | 11 +++++++++
 3 files changed, 58 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6d0ec315f7be..11fc12369c7d 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6800,6 +6800,30 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 	cpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);
 #endif
 
+	if (static_branch_unlikely(&sched_cluster_active)) {
+		struct sched_domain *sdc =
+			rcu_dereference(per_cpu(sd_cluster, target));
+
+		if (sdc) {
+			for_each_cpu_wrap(core, sched_domain_span(sdc), target) {
+				bool idle = true;
+
+				if (!cpumask_test_cpu(core, cpus))
+					continue;
+
+				for_each_cpu(cpu, cpu_smt_mask(core)) {
+					cpumask_clear_cpu(cpu, cpus);
+					if (!available_idle_cpu(cpu))
+						idle = false;
+				}
+
+				if (idle)
+					return core;
+			}
+			cpumask_andnot(cpus, cpus, sched_domain_span(sdc));
+		}
+	}
+
 	for_each_cpu_wrap(core, cpus, target) {
 		bool idle = true;
 
@@ -6905,8 +6929,26 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 	cpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);
 #endif
 
+	if (static_branch_unlikely(&sched_cluster_active)) {
+		struct sched_domain *sdc =
+			rcu_dereference(per_cpu(sd_cluster, target));
+
+		if (sdc) {
+			for_each_cpu_wrap(cpu, sched_domain_span(sdc), target) {
+				if (!cpumask_test_cpu(cpu, cpus))
+					continue;
+				if (--nr <= 0)
+					return -1;
+				if (available_idle_cpu(cpu) ||
+					sched_idle_cpu(cpu))
+					return cpu;
+			}
+			cpumask_andnot(cpus, cpus, sched_domain_span(sdc));
+		}
+	}
+
 	for_each_cpu_wrap(cpu, cpus, target) {
-		if (!--nr)
+		if (--nr <= 0)
 			return -1;
 		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
 			break;
@@ -6956,11 +6998,11 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 #ifdef CONFIG_QOS_SCHED_DYNAMIC_AFFINITY
-	if (prev != target && cpus_share_cache(prev, target) &&
+	if (prev != target && cpus_share_lowest_cache(prev, target) &&
 	    cpumask_test_cpu(prev, p->select_cpus) &&
 	    (available_idle_cpu(prev) || sched_idle_cpu(prev))) {
 #else
-	if (prev != target && cpus_share_cache(prev, target) &&
+	if (prev != target && cpus_share_lowest_cache(prev, target) &&
 	    (available_idle_cpu(prev) || sched_idle_cpu(prev))) {
 #endif
 		SET_STAT(found_idle_cpu_easy);
@@ -6971,7 +7013,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	recent_used_cpu = p->recent_used_cpu;
 	if (recent_used_cpu != prev &&
 	    recent_used_cpu != target &&
-	    cpus_share_cache(recent_used_cpu, target) &&
+	    cpus_share_lowest_cache(recent_used_cpu, target) &&
 	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
 #ifdef CONFIG_QOS_SCHED_DYNAMIC_AFFINITY
 	    cpumask_test_cpu(p->recent_used_cpu, p->select_cpus)) {
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c9019e1a6296..131228b5c268 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1312,6 +1312,7 @@ DECLARE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
 DECLARE_PER_CPU(struct sched_domain __rcu *, sd_cluster);
 DECLARE_PER_CPU(struct sched_domain *, sd_numa);
 DECLARE_PER_CPU(struct sched_domain *, sd_asym);
+extern struct static_key_false sched_cluster_active;
 
 struct sched_group_capacity {
 	atomic_t		ref;
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 0b299f9d60cf..eda15f08577f 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -418,6 +418,8 @@ DEFINE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
 DEFINE_PER_CPU(struct sched_domain *, sd_numa);
 DEFINE_PER_CPU(struct sched_domain *, sd_asym);
 
+DEFINE_STATIC_KEY_FALSE(sched_cluster_active);
+
 static void update_top_cache_domain(int cpu)
 {
 #ifdef CONFIG_SCHED_STEAL
@@ -1856,6 +1858,7 @@ build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *att
 	struct s_data d;
 	struct rq *rq = NULL;
 	int i, ret = -ENOMEM;
+	bool has_cluster = false;
 
 	alloc_state = __visit_domain_allocation_hell(&d, cpu_map);
 	if (alloc_state != sa_rootdomain)
@@ -1868,6 +1871,7 @@ build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *att
 		sd = NULL;
 		for_each_sd_topology(tl) {
 			sd = build_sched_domain(tl, cpu_map, attr, sd, i);
+			has_cluster |= sd->flags & SD_CLUSTER;
 			if (tl == sched_domain_topology)
 				*per_cpu_ptr(d.sd, i) = sd;
 			if (tl->flags & SDTL_OVERLAP)
@@ -1924,6 +1928,9 @@ build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *att
 	}
 	rcu_read_unlock();
 
+	if (has_cluster)
+		static_branch_inc_cpuslocked(&sched_cluster_active);
+
 	if (rq && sched_debug_enabled) {
 		pr_info("root domain span: %*pbl (max cpu_capacity = %lu)\n",
 			cpumask_pr_args(cpu_map), rq->rd->max_cpu_capacity);
@@ -2018,8 +2025,12 @@ int sched_init_domains(const struct cpumask *cpu_map)
  */
 static void detach_destroy_domains(const struct cpumask *cpu_map)
 {
+	unsigned int cpu = cpumask_any(cpu_map);
 	int i;
 
+	if (rcu_access_pointer(per_cpu(sd_cluster, cpu)))
+		static_branch_dec_cpuslocked(&sched_cluster_active);
+
 	rcu_read_lock();
 	for_each_cpu(i, cpu_map)
 		cpu_attach_domain(NULL, &def_root_domain, i);
-- 
2.33.0

