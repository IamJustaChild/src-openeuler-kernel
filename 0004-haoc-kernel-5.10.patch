From c2f3b8bd3ac3bbd8533d56549fb78a06b0b12dd8 Mon Sep 17 00:00:00 2001
From: liuzh <liuzhh@zgclab.edu.cn>
Date: Fri, 19 Apr 2024 15:34:15 +0800
Subject: [PATCH] Squashed commit of the following:

commit a1e1571596cbaf99788a6b215740099af65ba463
Author: liuzh <liuzhh@zgclab.edu.cn>
Date:   Fri Apr 19 15:31:20 2024 +0800

    fix some problems before compiling.

commit f529096a18512ceb04909c3e80c76597447fc0a1
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Mar 7 10:58:26 2024 +0800

    Fix a compiling error of CONFIG_IEE.

commit 1a54a137c449dd42546ab691171f0876c966d5c7
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Mar 5 17:24:33 2024 +0800

    Fix CONFIG bug.

commit 2c6d97714a6e8adf4248613f0540f2966a4df13f
Author: zhangshiyang17@mails.ucas.ac.cn <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Feb 29 11:57:17 2024 +0000

    Fix some warnings of credp.

commit 872e4865da7c42aa06a300a9437d347a0e2c031f
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Feb 28 18:17:40 2024 +0800

    Opt interruption support of iee.

commit b45d7f069d912b3e6a5a6c6105d164949dba77d1
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Feb 28 16:36:58 2024 +0800

    Change support of iee interruption as a config and optimize the EL1 exception.

commit 2a30964391ba3aa36166540ebd671aa77a8ab3d3
Author: zhangshiyang17@mails.ucas.ac.cn <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Feb 26 12:05:58 2024 +0000

    Fix ELR_EL1 set error.

commit 751f38e5d31c7dc86f02b00e78102984f95696b2
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Feb 26 17:21:55 2024 +0800

    Normalize the iee code.

commit 24a6ec0c5c28503ea6d66af3aa55f2c79f21a857
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Feb 26 11:36:37 2024 +0800

    Normalize interfaces of setting iee and simplify iee_exception_gate.

commit b9546b0f27ca944a551cc470c188fdeb6bec1684
Author: ljl <ljl2013@163.com>
Date:   Fri Feb 23 12:01:32 2024 +0000

    IEE SI: Commented all DBG breakpoint codes to prepare remaking rwx gate again :)

commit a3b23062ae112c3b9ae56fcdc6025933d1d64145
Author: zhangshiyang17@mails.ucas.ac.cn <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Feb 23 06:57:10 2024 +0000

    Optimize HPDS switch.

commit aef655de45fad61da9b7dd2f18419244e2a5074d
Author: ljl <ljl2013@163.com>
Date:   Fri Feb 23 05:18:25 2024 +0000

    IEE SI: Optimized DBG Breakpoint ctrl.

commit 108fb7f3338d834e1cf0a449f7f1957c4d954040
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Feb 22 09:19:27 2024 +0800

    Fix kaslr bug when opening KPTI.

commit 476918716ac19c149c2fbf704d3f4971df4e0d66
Author: clingfei <1599101385@qq.com>
Date:   Tue Feb 6 14:10:05 2024 +0800

    Fix koi token alloc and free

commit 57a54027ec49ec11dcff66aa5049efbca245990c
Author: clingfei <1599101385@qq.com>
Date:   Sun Feb 4 17:42:45 2024 +0800

    use user process's asid when calling into driver view, and use corresponding kernel's asid when returning from driver view

commit f19530507a31263adea445de880a7c954758ce3d
Author: ljl <ljl2013@163.com>
Date:   Tue Feb 6 04:11:12 2024 +0000

    IEE tested on Unixbench without KOI.

commit aee28d1909dd08b274cf0b5ee902caf88af2c0a4
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Feb 5 20:54:34 2024 +0800

    Fix Unix bench bugs.

commit 6dfab07825fa6c1a1e1e02ae788b82fb826e2de6
Author: ljl <lvjinglin2013@163.com>
Date:   Sun Feb 4 19:04:48 2024 +0800

    IEE SI: Temporaily remove modification in arch_local_irq_restore() to eliminate deadlock BUG.

commit 23ec73c79168144cf0d14182bc9ff8ca738bb1c0
Author: clingfei <1599101385@qq.com>
Date:   Sun Feb 4 17:42:45 2024 +0800

    use user process's asid when calling into driver view, and use corresponding kernel's asid when returning from driver view

commit d2aa1a9f4772d5599b5265f5e6c7397c884eb5df
Author: clingfei <1599101385@qq.com>
Date:   Sun Feb 4 16:23:57 2024 +0800

    free koi stack when free task_struct

commit 24d8fe0b01eaaedada61c164a50248c695e92515
Author: ljl <lvjinglin2013@163.com>
Date:   Sat Feb 3 21:43:10 2024 +0800

    IEE SI: MFit in KOI switch with new ASID design.

commit 0c895c70dd9fa5df9339e624d56febf7b532c3b2
Author: ljl <lvjinglin2013@163.com>
Date:   Wed Jan 31 12:20:38 2024 +0800

    IEE SI: Remove PSTATE D to protect iee rwx gate.

commit 1e2d62980e23f8a0374474ab9d2fca114f6bf018
Author: clingfei <1599101385@qq.com>
Date:   Fri Feb 2 20:52:37 2024 +0800

    modify koi to adapt to UEFI

commit 6c3ece8694038b00293fc5b98b36119ba04534e0
Author: clingfei <1599101385@qq.com>
Date:   Thu Feb 1 19:46:38 2024 +0800

    modify ASID design in KOI

commit 1005798d6060beabcf5659256b7ad2d2006e602a
Author: clingfei <1599101385@qq.com>
Date:   Thu Feb 1 15:25:00 2024 +0800

    fix koi token init and switch stack bugs

commit c732d730b17bb94a15ae42af53d13d6d3dfad3ea
Author: clingfei <1599101385@qq.com>
Date:   Thu Feb 1 10:06:21 2024 +0800

    put ko's pgtable in IEE region when PTP is enabled

commit 790ee5fd252e47f4a7a35f01eed6d09f37263aca
Author: clingfei <1599101385@qq.com>
Date:   Wed Jan 31 09:22:07 2024 +0800

    fix dead lock when mapping from kernel pgtable to driver pgtable, which is caused by spin_lock used in irq context

commit 83a396582effd375ef3574565ccb6305b2c07e41
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Feb 1 16:21:33 2024 +0800

    Fix rw gate bugs and modify ASID design.

commit 650b0eaa401cf18d7ae539b587b84e69b5037a47
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Jan 31 16:26:37 2024 +0800

    Fix cred KABI.

commit 01dbf157b7cda0503957c70af828bc9887d447c4
Author: clingfei <1599101385@qq.com>
Date:   Sat Jan 27 15:16:54 2024 +0800

    Add koi_irq_current_ttbr1 to record the ttbr1 status in softirq context, fix compile errors when koi enabled only

commit 869cfba1b66056d08a5d9a331ffb9826f1930fd2
Author: clingfei <1599101385@qq.com>
Date:   Tue Jan 23 21:24:19 2024 +0800

    modify koi interfaces to adapt to iee_rw_gate and iee_rwx_gate

commit 29e594d70fa04c7288e217f5c0534713eb6e84d5
Author: ljl <lvjinglin2013@163.com>
Date:   Tue Jan 23 11:01:17 2024 +0800

    IEE SI: fixed KASLR bug in smp.

commit bdf21ea87d291639ffbb67a95d7c812580daa340
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Jan 22 16:07:09 2024 +0800

    Delete set iee_pg_dir rw.

commit 573a263791266e1689209d66c3340456cc83c92d
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Jan 22 15:03:28 2024 +0800

    Fix DEP error.

commit 34662b227b0ab383109497436ec6024fbcf60565
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Jan 20 17:57:03 2024 +0800

    Fix iee si error when opening CONFIG_IEE only with nokaslr, but remaining problem on kaslr.

commit 0396087a324f36019e056035483d84fe028ced29
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Jan 20 15:16:06 2024 +0800

    Merge IEE:oooo patch.

commit 117158e23117e405de6493688968241828539e58
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Jan 20 14:58:58 2024 +0800

    Fix memblock size.

commit dce48b19591ae16fc899f28d797195e6c44cf9a3
Author: clingfei <1599101385@qq.com>
Date:   Fri Jan 19 17:09:02 2024 +0800

    add iee_rw_gate handlers to support access to task_token in koi, use iee_rwx_gate_entry to switch kernel and ko pgtable, and modify koi interfaces to use iee_rw_gate to access task_token

commit 758b0ccbb194e949fea1c26d6be41ca1f36ac243
Author: clingfei <1599101385@qq.com>
Date:   Fri Jan 19 09:31:14 2024 +0800

    optimize implementation of switch pgtable and stack between kernel and ko, now tpiddro_el0 is not needed

commit e566f9e4854965e53d96fac88aa41deca826c43d
Author: clingfei <1599101385@qq.com>
Date:   Wed Jan 17 22:02:17 2024 +0800

    add koi support for softirq

commit c5fda7f6c536c470acd446c63071c0bd0620e775
Author: losky <502878997@qq.com>
Date:   Wed Jan 17 09:16:30 2024 +0800

    wrap iee_rw_gate, make interface looks more concise

commit b4fa5fd8ecdf92180bb7529756a6a847a5f7d8e7
Author: ljl <lvjinglin2013@163.com>
Date:   Wed Jan 17 19:54:05 2024 +0800

    IEE SI: testing on lmbench successfully.

commit b09171345cadf8f5b0ca985283114b1983ac1843
Author: ljl <lvjinglin2013@163.com>
Date:   Wed Jan 17 15:33:21 2024 +0800

    IEE SI: Remake again. Who cares what's new? :)

commit 0aef94e84eafc9faccd756be35802ab5e1e7044f
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Jan 17 18:58:05 2024 +0800

    Fix smp sp_el0 check error.

commit face06c439d4e867449c317cfb2a6e29c3146846
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Jan 17 13:56:12 2024 +0800

    Fix task->children!=lm_va error.

commit 2afab7541b19543eed2c1f6a1d0f57c6c2a2f585
Author: clingfei <1599101385@qq.com>
Date:   Wed Jan 17 12:48:20 2024 +0800

    Fix ko stack alloc policy

commit 2a656d8df0f2d98169813d373d38f35d5a98078a
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Wed Jan 17 10:12:32 2024 +0800

    Fix SP_EL0 check error.

commit fc1166d2a952e0ef00cfdcb684589d85d1ad8d54
Author: clingfei <1599101385@qq.com>
Date:   Tue Jan 16 19:47:16 2024 +0800

    Fix koi

commit 8e8f129ffc13b00da69e592190fb4834ae577168
Author: clingfei <1599101385@qq.com>
Date:   Tue Jan 16 19:12:11 2024 +0800

    Fix conflict between KOI NG and set_pte/pmd/pud_pre_init

commit e5677b9b6fcc7284e548e424901c19e09ef79e8a
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Jan 16 16:52:06 2024 +0800

    Fix conflict between KOI NG and IEE_RWX_GATE.

commit d2ffc0b260905db10912d33bad58cf68547013d2
Author: clingfei <1599101385@qq.com>
Date:   Tue Jan 16 16:42:53 2024 +0800

    fix koi, now dm-zero can run successfully with KOI Enabled, IEE/PTP/CREDP and kaslr disabled

commit d6178b1c333daaca5fbf4b11c0d666522abfefb6
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Jan 16 12:32:34 2024 +0800

    Add __entry_task protection and SP_EL0 check.

commit 6f058849fc4842346dd6f6b27ff7964fa4cc3504
Author: clingfei <1599101385@qq.com>
Date:   Tue Jan 16 08:58:17 2024 +0800

    fix kabi breakage caused by koi

commit 31ffe32b395d7f2aa43b4993405ac85317df89d3
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Jan 15 16:57:07 2024 +0800

    Adapted to openeuler_cfg.

commit 6b12addbbe24adaa38e622e2a4e0d5da10b0565d
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Jan 15 16:09:21 2024 +0800

    Fix smp error and optimize rw_gate.

commit eb9b41c79577f10d72536c143d05ea83a08dcc8c
Author: clingfei <1599101385@qq.com>
Date:   Mon Jan 15 11:24:02 2024 +0800

    refactor koi

commit a76b0010d8c79da97b7fcf5a731e7af5e0bad677
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Jan 13 20:46:01 2024 +0800

    Modify design of task_token

commit 94f8b00a81de44011567d002d33eff66dcd2b3e6
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Jan 12 19:08:09 2024 +0800

    Adapted DEP to EFI.

commit bcde7a6a429286a063e0774dc5a86ef2cebe5df0
Author: zsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Jan 12 14:01:58 2024 +0800

    IEE adapted to EFI.

commit 8c134dcf4fcb4577af5ba82246255d13dfe63918
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Thu Jan 11 09:40:40 2024 +0800

    asid set  ttbr

commit a9bd3e26180a50357998e33382df319d32657bf4
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Thu Jan 11 09:12:06 2024 +0800

    asid set  ttbr

commit a5c922e2700a321c278a85132baa2654223dccd6
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Tue Jan 9 22:51:12 2024 +0800

    asid

commit a8a8917c81d4403053398e6711410c302a2db014
Author: ljl <lvjinglin2013@163.com>
Date:   Sat Jan 6 12:58:56 2024 +0800

    IEE SI: fixed merge bugs.

commit ef956f75e304d6942576d6c65f1304af921da525
Author: ljl <lvjinglin2013@163.com>
Date:   Sat Jan 6 11:12:57 2024 +0800

    IEE SI: added DBG ctrl protection and fixed some bug.

commit 006264e1ed0fd203179c20b67ee7b41c27709cf4
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Jan 5 15:05:23 2024 +0800

    Revert "Merge branch 'openeuler-commit' of http://10.208.128.169/hhz/linux-kernel-pti into openeuler-commit"

    This reverts commit 7ba23365ca846837af0857e2eab196bc4c196d46, reversing
    changes made to 551cbb14d965ae2b147c11eb953a0da2cfe02bf5.

commit 546f24f3c7ab646ad45a9c4f209c40723f8b9a35
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Jan 5 14:04:22 2024 +0800

    Add iee stack check.

commit 6d1704a7e4ec335d7b230d35bea1a663799586e7
Author: clingfei <1599101385@qq.com>
Date:   Thu Jan 4 09:45:27 2024 +0000

    modify iee_rwx_gate_entry to support msr ttbr1 in koi

commit 1df2c945b96456d051fa35fbc987ee615dfc2aed
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Jan 2 12:23:07 2024 +0000

    Fix some merge errors.

commit 58c3e09b8034a5d2325b40797ff959d631083124
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Jan 2 11:53:02 2024 +0000

    Add owner registing but not checked iee stack and more.

commit 6650e8911d53a3da9b91762a04295c6418be771a
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Jan 2 08:37:03 2024 +0000

    Fix the random error and add IEE selfprotect.

commit 7f80e0c94a8ba1c11d732d2b6a237c78169e987c
Author: ljl <lvjinglin2013@163.com>
Date:   Mon Jan 1 14:39:09 2024 +0000

    IEE SI: expend iee si stack to 4 pg with 8 pg alignment.

commit ee56d516b2cb5f03e39a335731aaa9c11af2134d
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Jan 1 11:10:31 2024 +0000

    Add check for remapping IEE region.

commit 530d0771ed0dc38ce1d227c5e2a2d76901d5ed91
Author: clingfei <1599101385@qq.com>
Date:   Mon Jan 1 09:57:07 2024 +0000

    set all kernel pte to NG

commit 88e28f0cc61baa5ccd00963e8ead88d88095de78
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Mon Jan 1 04:35:33 2024 +0000

    add notes finish

commit 3ad83431c166ed4c72d0354a16f65e20559419b7
Author: clingfei <1599101385@qq.com>
Date:   Sun Dec 31 16:22:41 2023 +0000

    add switch pgtable, buffer management, heap allocator wrapper interfaces

commit 0e2e9410fa6f94a76f5c188dd584473522cf4c99
Author: zyf <zhouyangfan20s@ict.ac.cn>
Date:   Sun Dec 31 16:17:22 2023 +0000

    Replaced switch with function pointers in iee.c and added documentation.

commit a649b34d334fbfcc8ee51fb67b7a0ecc4f58f32f
Author: ljl <lvjinglin2013@163.com>
Date:   Sun Dec 31 14:55:06 2023 +0000

    replaced el1 sctlr, tcr, ttbr0 inst by iee si gate in kernel.

commit 183d6938c90319096f1d9e89db40f8c9e2bdb74f
Author: zyf <zhouyangfan20s@ict.ac.cn>
Date:   Sun Dec 31 12:13:33 2023 +0000

    Standardize IEE InterfacIEEing Convention

commit 279748fd6edd9d5ad9456b0043db3ac1d9036a40
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Sun Dec 31 06:49:37 2023 +0000

    add notes 2nd

commit 5ac90c58e325916cd5c83f5050ae3451c6cdbeba
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sun Dec 31 05:02:42 2023 +0000

    Add TCR check.

commit 33c0449a8194042b46a61c7c16bb7d80db4d1f16
Author: ljl <lvjinglin2013@163.com>
Date:   Sun Dec 31 03:25:25 2023 +0000

    implemented iee si base functions.

commit 30fdd16d7c6f96a166070d4ada0c70018e96f06d
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Sun Dec 31 03:42:51 2023 +0000

    add notes 2

commit 987fdac2e1f4723c1a2c81662752417d1a3b005c
Author: zzw <zhangzhenwei22b@ict.ac.cn>
Date:   Sun Dec 31 03:33:31 2023 +0000

    add notes 1

commit 6501b248ba5d3061f8a2177ac580b63136b81968
Author: clingfei <1599101385@qq.com>
Date:   Sun Dec 31 02:30:25 2023 +0000

    add ko pagetable construction interfaces

commit dfc8a45abf7ccb23f74c9299383e32e1475f6ac4
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Dec 30 09:23:45 2023 +0000

    Modify config name.

commit ae43c54117586a4e5ebe091b8822d26bad67f921
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Dec 30 09:11:53 2023 +0000

    Add achitecture check.

commit b3ec308809054cdedcc987df9fa0d8dbc492ad24
Author: clingfei <1599101385@qq.com>
Date:   Sat Dec 30 04:58:09 2023 +0000

    merge koi modification for Kconfig, kernel_ventry, vmlinux.lds, pgd_alloc and set_pte/set_pud/set_pmd, BUT set_pte will cause kernel crash, remains to be fixed

commit 34a35c0076aa9a27bbd75fd2139f4b9ef5eeb3bc
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Dec 29 13:49:11 2023 +0000

    Add kernel DEP.

commit d3d2329db114d6e950868807393a170b5f3f6ca5
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Dec 29 12:51:19 2023 +0000

    Fix rcu problem and set track problem. Disable interrupt in iee_exception_exit.

commit 029b593f9e4d14d5a3a4a92426a2549328291b0c
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Dec 21 06:28:32 2023 +0000

    Adapted to openeuler_defconfig.

commit a6552b145fd9bcf2b445f2b04cbb2cba30134411
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Dec 19 06:26:46 2023 +0000

    Add cred protection.

commit 97d32b177b5948bae5dd1e784eb923109d2c5451
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Dec 15 08:45:05 2023 +0000

    Fix flush tlb bug.

commit bf8a3a5e0883cf89696c925200b986d3a6fabdff
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Sat Dec 9 11:34:37 2023 +0000

    Fix synchronize bugs.

commit c88a076a8aeb4aa5f5a8401d34bc51ced35ad72d
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Mon Dec 4 08:39:30 2023 +0000

    Switch ASID instead of flush TLB.

commit ff6eee41bfa4e52150cb8eefb0afcc900203422b
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Nov 30 09:18:45 2023 +0000

    Fix KASLR Error.

commit a7628bd6371b6bee2d6dbc9b131474b051822097
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Thu Sep 28 03:58:04 2023 +0000

    Adapted to GCC.

commit 42a9d98c49277b003c6882db1e78791c7408c5e8
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Tue Sep 26 07:43:59 2023 +0000

    Fix set pte by cmpxchg bug.

commit df800cbc8fd69099d12080da6b8d05982db1f0d9
Author: zhangsy <zhangshiyang17@mails.ucas.ac.cn>
Date:   Fri Sep 22 02:52:26 2023 +0000

    Add Page Table Isolation.
---
 .gitignore                              |    9 +
 Makefile                                |    4 +-
 arch/arm64/Kconfig                      |   18 +
 arch/arm64/include/asm/assembler.h      |   75 +
 arch/arm64/include/asm/daifflags.h      |   16 +
 arch/arm64/include/asm/efi.h            |    4 +
 arch/arm64/include/asm/fixmap.h         |    3 +
 arch/arm64/include/asm/hw_breakpoint.h  |   12 +
 arch/arm64/include/asm/iee-access.h     |   36 +
 arch/arm64/include/asm/iee-cred.h       |  145 ++
 arch/arm64/include/asm/iee-def.h        |   73 +
 arch/arm64/include/asm/iee-si.h         |   70 +
 arch/arm64/include/asm/iee-slab.h       |   16 +
 arch/arm64/include/asm/iee-token.h      |   40 +
 arch/arm64/include/asm/iee.h            |   10 +
 arch/arm64/include/asm/irqflags.h       |   12 +
 arch/arm64/include/asm/kernel-pgtable.h |   12 +
 arch/arm64/include/asm/koi.h            |  335 ++++
 arch/arm64/include/asm/memory.h         |   24 +
 arch/arm64/include/asm/mmu_context.h    |   12 +
 arch/arm64/include/asm/pgalloc.h        |    4 +
 arch/arm64/include/asm/pgtable-hwdef.h  |    4 +
 arch/arm64/include/asm/pgtable.h        |  269 ++-
 arch/arm64/include/asm/sysreg.h         |   56 +
 arch/arm64/include/asm/tlb.h            |    9 +
 arch/arm64/include/asm/tlbflush.h       |   33 +-
 arch/arm64/kernel/Makefile              |    2 +
 arch/arm64/kernel/armv8_deprecated.c    |   16 +
 arch/arm64/kernel/asm-offsets.c         |   11 +
 arch/arm64/kernel/cpu_errata.c          |   12 +
 arch/arm64/kernel/cpufeature.c          |   18 +
 arch/arm64/kernel/debug-monitors.c      |    4 +
 arch/arm64/kernel/entry.S               |  744 +++++++-
 arch/arm64/kernel/head.S                |   38 +
 arch/arm64/kernel/hibernate.c           |   68 +
 arch/arm64/kernel/hw_breakpoint.c       |   99 ++
 arch/arm64/kernel/iee/Makefile          |    1 +
 arch/arm64/kernel/iee/iee-func.c        |  189 +++
 arch/arm64/kernel/iee/iee-gate.S        |  229 +++
 arch/arm64/kernel/iee/iee.c             | 1363 +++++++++++++++
 arch/arm64/kernel/irq.c                 |    4 +-
 arch/arm64/kernel/koi/Makefile          |    1 +
 arch/arm64/kernel/koi/koi.c             | 1327 +++++++++++++++
 arch/arm64/kernel/mte.c                 |    4 +
 arch/arm64/kernel/process.c             |   23 +-
 arch/arm64/kernel/proton-pack.c         |    8 +
 arch/arm64/kernel/setup.c               |   34 +
 arch/arm64/kernel/syscall.c             |    8 +
 arch/arm64/kernel/traps.c               |    3 +-
 arch/arm64/kernel/vmlinux.lds.S         |   70 +
 arch/arm64/mm/context.c                 |   91 +-
 arch/arm64/mm/fault.c                   |    4 +
 arch/arm64/mm/init.c                    |   40 +
 arch/arm64/mm/mmu.c                     | 2069 ++++++++++++++++++++---
 arch/arm64/mm/pgd.c                     |   39 +
 arch/arm64/mm/proc.S                    |   22 +
 drivers/firmware/efi/arm-runtime.c      |    4 +
 drivers/firmware/efi/memmap.c           |   20 +
 drivers/tty/serial/earlycon.c           |    4 +
 drivers/usb/early/ehci-dbgp.c           |    4 +
 fs/cifs/cifs_spnego.c                   |    9 +
 fs/cifs/cifsacl.c                       |    9 +
 fs/coredump.c                           |    8 +
 fs/exec.c                               |   20 +
 fs/nfs/flexfilelayout/flexfilelayout.c  |    9 +
 fs/nfs/nfs4idmap.c                      |    9 +
 fs/nfsd/auth.c                          |   38 +
 fs/nfsd/nfs4callback.c                  |    8 +
 fs/nfsd/nfs4recover.c                   |    9 +
 fs/nfsd/nfsfh.c                         |    9 +
 fs/open.c                               |   23 +
 fs/overlayfs/dir.c                      |    9 +
 fs/overlayfs/super.c                    |   12 +
 include/asm-generic/early_ioremap.h     |    3 +
 include/asm-generic/fixmap.h            |   18 +
 include/asm-generic/pgalloc.h           |   36 +
 include/asm-generic/vmlinux.lds.h       |   24 +-
 include/linux/cred.h                    |   42 +
 include/linux/efi.h                     |    3 +
 include/linux/iee-func.h                |   27 +
 include/linux/module.h                  |    1 +
 include/linux/sched.h                   |   19 +
 include/linux/stacktrace.h              |    4 +
 init/main.c                             |  162 ++
 kernel/cred.c                           |  139 ++
 kernel/exit.c                           |    8 +
 kernel/fork.c                           |  434 +++--
 kernel/groups.c                         |    7 +
 kernel/kthread.c                        |   12 +
 kernel/module.c                         |   61 +
 kernel/smpboot.c                        |   10 +
 kernel/stacktrace.c                     |   38 +
 kernel/sys.c                            |  107 ++
 kernel/umh.c                            |   10 +
 kernel/user_namespace.c                 |   18 +
 mm/Kconfig                              |   12 +
 mm/damon/ops-common.c                   |    8 +
 mm/debug_vm_pgtable.c                   |   24 +
 mm/early_ioremap.c                      |   57 +
 mm/huge_memory.c                        |   30 +-
 mm/init-mm.c                            |   17 +
 mm/memory.c                             |   36 +-
 mm/slub.c                               |  215 ++-
 mm/sparse-vmemmap.c                     |   23 +
 mm/swap.c                               |   13 +
 mm/vmalloc.c                            |    4 +
 net/dns_resolver/dns_key.c              |    9 +
 security/commoncap.c                    |  153 ++
 security/keys/keyctl.c                  |   23 +
 security/keys/process_keys.c            |   45 +
 security/security.c                     |   15 +
 111 files changed, 9513 insertions(+), 400 deletions(-)
 create mode 100644 arch/arm64/include/asm/iee-access.h
 create mode 100644 arch/arm64/include/asm/iee-cred.h
 create mode 100644 arch/arm64/include/asm/iee-def.h
 create mode 100644 arch/arm64/include/asm/iee-si.h
 create mode 100644 arch/arm64/include/asm/iee-slab.h
 create mode 100644 arch/arm64/include/asm/iee-token.h
 create mode 100644 arch/arm64/include/asm/iee.h
 create mode 100644 arch/arm64/include/asm/koi.h
 create mode 100644 arch/arm64/kernel/iee/Makefile
 create mode 100644 arch/arm64/kernel/iee/iee-func.c
 create mode 100644 arch/arm64/kernel/iee/iee-gate.S
 create mode 100644 arch/arm64/kernel/iee/iee.c
 create mode 100644 arch/arm64/kernel/koi/Makefile
 create mode 100644 arch/arm64/kernel/koi/koi.c
 create mode 100644 include/linux/iee-func.h

diff --git a/.gitignore b/.gitignore
index 67d2f3503128..3760f5e0b19b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -45,6 +45,7 @@
 *.tar
 *.xz
 *.zst
+*.log
 Module.symvers
 modules.builtin
 modules.order
@@ -156,3 +157,11 @@ x509.genkey
 
 # Documentation toolchain
 sphinx_*/
+
+#command
+command.txt
+
+#build
+/build/*
+
+dm-zero.c.koi
\ No newline at end of file
diff --git a/Makefile b/Makefile
index e1e4ca4737a7..e1c0e2a50679 100644
--- a/Makefile
+++ b/Makefile
@@ -504,12 +504,12 @@ LINUXINCLUDE    := \
 		-I$(objtree)/include \
 		$(USERINCLUDE)
 
-KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE
+KBUILD_AFLAGS   := -D__ASSEMBLY__ -fno-PIE -march=armv8.1-a
 KBUILD_CFLAGS   := -Wall -Wundef -Werror=strict-prototypes -Wno-trigraphs \
 		   -fno-strict-aliasing -fno-common -fshort-wchar -fno-PIE \
 		   -Werror=implicit-function-declaration -Werror=implicit-int \
 		   -Werror=return-type -Wno-format-security \
-		   -std=gnu89
+		   -std=gnu89 -march=armv8.1-a
 KBUILD_CPPFLAGS := -D__KERNEL__
 KBUILD_AFLAGS_KERNEL :=
 KBUILD_CFLAGS_KERNEL :=
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index ebd2439dc660..7d3a0f6023ce 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -1394,6 +1394,24 @@ config UNMAP_KERNEL_AT_EL0
 
 	  If unsure, say Y.
 
+# Config for iee
+config IEE
+	depends on ARM64
+	depends on ARM64_PAN
+	depends on ARM64_VA_BITS_48
+	depends on ARM64_4K_PAGES
+	def_bool y
+
+# Config for support of interruption of iee
+config IEE_INTERRUPTABLE
+	depends on IEE
+	def_bool n
+
+# Config for credentials isolation
+config CREDP
+	depends on IEE
+	def_bool n
+
 config MITIGATE_SPECTRE_BRANCH_HISTORY
 	bool "Mitigate Spectre style attacks against branch history" if EXPERT
 	default y
diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5e6bacda05d8..37325ff39e38 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -24,6 +24,41 @@
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
 
+#ifdef CONFIG_IEE
+    .macro iee_si_restore_daif, flags:req
+    msr daifclr, #0xf
+    tbnz \flags, #6, 114221f
+    tbnz \flags, #7, 114210f
+    tbnz \flags, #8, 114100f
+    msr daifset, #0b000
+    b 114514f
+114221:
+    tbnz \flags, #7, 114211f
+    tbnz \flags, #8, 114101f
+    msr daifset, #0b001
+    b 114514f
+114211:
+    tbnz \flags, #8, 114111f
+    msr daifset, #0b011
+    b 114514f
+114210:
+    tbnz \flags, #8, 114110f
+    msr daifset, #0b010
+    b 114514f
+114100:
+    msr daifset, #0b100
+    b 114514f
+114101:
+    msr daifset, #0b101
+    b 114514f
+114110:
+    msr daifset, #0b110
+    b 114514f
+114111:
+    msr daifset, #0b111
+114514:
+    .endm
+#endif
 	/*
 	 * Provide a wxN alias for each wN register so what we can paste a xN
 	 * reference after a 'w' to obtain the 32-bit version.
@@ -34,11 +69,19 @@
 
 	.macro save_and_disable_daif, flags
 	mrs	\flags, daif
+#ifdef CONFIG_IEE
+	msr	daifset, #0x7
+#else
 	msr	daifset, #0xf
+#endif
 	.endm
 
 	.macro disable_daif
+#ifdef CONFIG_IEE
+	msr	daifset, #0x7
+#else
 	msr	daifset, #0xf
+#endif
 	.endm
 
 	.macro enable_daif
@@ -46,7 +89,11 @@
 	.endm
 
 	.macro	restore_daif, flags:req
+#ifdef CONFIG_IEE
+    iee_si_restore_daif \flags
+#else
 	msr	daif, \flags
+#endif
 	.endm
 
 	/* IRQ is the lowest priority flag, unconditionally unmask the rest. */
@@ -63,7 +110,11 @@
 	.endm
 
 	.macro	restore_irq, flags
+#ifdef CONFIG_IEE
+    iee_si_restore_daif \flags
+#else
 	msr	daif, \flags
+#endif
 	.endm
 
 	.macro	enable_dbg
@@ -71,20 +122,44 @@
 	.endm
 
 	.macro	disable_step_tsk, flgs, tmp
+// #ifdef CONFIG_IEE
+// 1145:
+//     tbz	\flgs, #TIF_SINGLESTEP, 9990f
+// 	mrs	\tmp, mdscr_el1
+// 	bic	\tmp, \tmp, #DBG_MDSCR_SS
+//     orr	\tmp, \tmp, #DBG_MDSCR_MDE
+// 	msr	mdscr_el1, \tmp
+// 	isb	// Synchronise with enable_dbg
+//     mrs \tmp, mdscr_el1
+//     tbz \tmp, #15, 1145b
+// #else
 	tbz	\flgs, #TIF_SINGLESTEP, 9990f
 	mrs	\tmp, mdscr_el1
 	bic	\tmp, \tmp, #DBG_MDSCR_SS
 	msr	mdscr_el1, \tmp
 	isb	// Synchronise with enable_dbg
+// #endif
 9990:
 	.endm
 
 	/* call with daif masked */
 	.macro	enable_step_tsk, flgs, tmp
+// #ifdef CONFIG_IEE
+// 1146:
+//     tbz	\flgs, #TIF_SINGLESTEP, 9990f
+// 	mrs	\tmp, mdscr_el1
+// 	orr	\tmp, \tmp, #DBG_MDSCR_SS
+//     orr	\tmp, \tmp, #DBG_MDSCR_MDE
+// 	msr	mdscr_el1, \tmp
+// 	isb	// Synchronise with enable_dbg
+//     mrs \tmp, mdscr_el1
+//     tbz \tmp, #15, 1146b
+// #else
 	tbz	\flgs, #TIF_SINGLESTEP, 9990f
 	mrs	\tmp, mdscr_el1
 	orr	\tmp, \tmp, #DBG_MDSCR_SS
 	msr	mdscr_el1, \tmp
+// #endif
 9990:
 	.endm
 
diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index cfdde3a56805..c1195cd4c648 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -25,11 +25,19 @@ static inline void local_daif_mask(void)
 		(read_sysreg_s(SYS_ICC_PMR_EL1) == (GIC_PRIO_IRQOFF |
 						    GIC_PRIO_PSR_I_SET)));
 
+#ifdef CONFIG_IEE
+	asm volatile(
+		"msr	daifset, #0x7		// local_daif_mask\n"
+		:
+		:
+		: "memory");
+#else
 	asm volatile(
 		"msr	daifset, #0xf		// local_daif_mask\n"
 		:
 		:
 		: "memory");
+#endif
 
 	/* Don't really care for a dsb here, we don't intend to enable IRQs */
 	if (system_uses_irq_prio_masking())
@@ -114,7 +122,11 @@ static inline void local_daif_restore(unsigned long flags)
 		gic_write_pmr(pmr);
 	}
 
+#ifdef CONFIG_IEE
+    iee_si_write_daif(flags);
+#else
 	write_sysreg(flags, daif);
+#endif
 
 	if (irq_disabled)
 		trace_hardirqs_off();
@@ -139,6 +151,10 @@ static inline void local_daif_inherit(struct pt_regs *regs)
 	 * system_has_prio_mask_debugging() won't restore the I bit if it can
 	 * use the pmr instead.
 	 */
+#ifdef CONFIG_IEE
+    iee_si_write_daif(flags);
+#else
 	write_sysreg(flags, daif);
+#endif
 }
 #endif
diff --git a/arch/arm64/include/asm/efi.h b/arch/arm64/include/asm/efi.h
index 8fa8a3c9af3e..f3b2f11c3e6a 100644
--- a/arch/arm64/include/asm/efi.h
+++ b/arch/arm64/include/asm/efi.h
@@ -52,7 +52,11 @@ efi_status_t __efi_rt_asm_wrapper(void *, const char *, ...);
 #define arch_efi_save_flags(state_flags)		\
 	((void)((state_flags) = read_sysreg(daif)))
 
+#ifdef CONFIG_IEE
+#define arch_efi_restore_flags(state_flags)	iee_si_write_daif(state_flags)
+#else
 #define arch_efi_restore_flags(state_flags)	write_sysreg(state_flags, daif)
+#endif
 
 
 /* arch specific definitions used by the stub code */
diff --git a/arch/arm64/include/asm/fixmap.h b/arch/arm64/include/asm/fixmap.h
index daff882883f9..7511e89ed954 100644
--- a/arch/arm64/include/asm/fixmap.h
+++ b/arch/arm64/include/asm/fixmap.h
@@ -106,6 +106,9 @@ void __init early_fixmap_init(void);
 #define __late_clear_fixmap(idx) __set_fixmap((idx), 0, FIXMAP_PAGE_CLEAR)
 
 extern void __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot);
+#ifdef CONFIG_PTP
+extern void __iee_set_fixmap_pre_init(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot);
+#endif
 
 #include <asm-generic/fixmap.h>
 
diff --git a/arch/arm64/include/asm/hw_breakpoint.h b/arch/arm64/include/asm/hw_breakpoint.h
index bc7aaed4b34e..16e0e8cfea1d 100644
--- a/arch/arm64/include/asm/hw_breakpoint.h
+++ b/arch/arm64/include/asm/hw_breakpoint.h
@@ -104,6 +104,18 @@ static inline void decode_ctrl_reg(u32 reg,
 	write_sysreg(VAL, dbg##REG##N##_el1);\
 } while (0)
 
+#ifdef CONFIG_IEE
+#define IEE_SI_AARCH64_DBG_READ(N, REG, VAL) do{\
+    VAL = this_cpu_read(iee_si_user_##REG##N);\
+} while (0)
+
+#define IEE_SI_AARCH64_DBG_WRITE(N, REG, VAL) do{\
+    u64 __val = (u64)(VAL);  \
+	this_cpu_write(iee_si_user_##REG##N, __val);\
+    iee_rwx_gate_entry(IEE_WRITE_AFSR0);\
+} while (0)
+#endif
+
 struct task_struct;
 struct notifier_block;
 struct perf_event_attr;
diff --git a/arch/arm64/include/asm/iee-access.h b/arch/arm64/include/asm/iee-access.h
new file mode 100644
index 000000000000..c60193476050
--- /dev/null
+++ b/arch/arm64/include/asm/iee-access.h
@@ -0,0 +1,36 @@
+#ifndef _LINUX_IEE_ACCESS_H
+#define _LINUX_IEE_ACCESS_H
+
+#include <asm/iee-def.h>
+#include <asm/iee-slab.h>
+
+extern unsigned long long iee_rw_gate(int flag, ...);
+
+#ifdef CONFIG_IEE
+void iee_write_in_byte(void *ptr, u64 data, int length)
+{
+	iee_rw_gate(IEE_WRITE_IN_BYTE, ptr, data, length);
+}
+
+void iee_memset(void *ptr, int data, size_t n)
+{
+	iee_rw_gate(IEE_MEMSET, ptr, data, n);
+}
+
+void iee_set_track(struct track *ptr, struct track *data)
+{
+	iee_rw_gate(IEE_OP_SET_TRACK, ptr, data);
+}
+
+void iee_set_freeptr(void **pptr, void *ptr)
+{
+	iee_rw_gate(IEE_OP_SET_FREEPTR, pptr, ptr);
+}
+
+void iee_write_entry_task(struct task_struct *tsk)
+{
+	iee_rw_gate(IEE_WRITE_ENTRY_TASK, tsk);
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee-cred.h b/arch/arm64/include/asm/iee-cred.h
new file mode 100644
index 000000000000..d98e3645d8a3
--- /dev/null
+++ b/arch/arm64/include/asm/iee-cred.h
@@ -0,0 +1,145 @@
+#ifndef _LINUX_IEE_CRED_H
+#define _LINUX_IEE_CRED_H
+
+#include <linux/cred.h>
+#include <asm/iee-def.h>
+
+extern unsigned long long iee_rw_gate(int flag, ...);
+
+#ifdef CONFIG_CREDP
+static void __maybe_unused iee_copy_cred(const struct cred *old, struct cred *new)
+{
+    iee_rw_gate(IEE_OP_COPY_CRED,old,new);
+}
+
+static void __maybe_unused iee_set_cred_uid(struct cred *cred, kuid_t uid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_UID,cred,uid);
+}
+
+static void __maybe_unused iee_set_cred_gid(struct cred *cred, kgid_t gid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_GID,cred,gid);
+}
+
+static void __maybe_unused iee_set_cred_suid(struct cred *cred, kuid_t suid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_SUID,cred,suid);
+}
+
+static void __maybe_unused iee_set_cred_sgid(struct cred *cred, kgid_t sgid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_SGID,cred,sgid);
+}
+
+static void __maybe_unused iee_set_cred_euid(struct cred *cred, kuid_t euid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_EUID,cred,euid);
+}
+
+static void __maybe_unused iee_set_cred_egid(struct cred *cred, kgid_t egid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_EGID,cred,egid);
+}
+
+static void __maybe_unused iee_set_cred_fsuid(struct cred *cred, kuid_t fsuid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_FSUID,cred,fsuid);
+}
+
+static void __maybe_unused iee_set_cred_fsgid(struct cred *cred, kgid_t fsgid)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_FSGID,cred,fsgid);
+}
+
+static void __maybe_unused iee_set_cred_user(struct cred *cred, struct user_struct *user)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_USER,cred,user);
+}
+
+static void __maybe_unused iee_set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_USER_NS,cred,user_ns);
+}
+
+static void __maybe_unused iee_set_cred_group_info(struct cred *cred, struct group_info *group_info)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_GROUP_INFO,cred,group_info);
+}
+
+static void __maybe_unused iee_set_cred_securebits(struct cred *cred, unsigned securebits)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_SECUREBITS,cred,securebits);
+}
+
+static void __maybe_unused iee_set_cred_cap_inheritable(struct cred *cred, kernel_cap_t cap_inheritable)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_CAP_INHER,cred,cap_inheritable);
+}
+
+static void __maybe_unused iee_set_cred_cap_permitted(struct cred *cred, kernel_cap_t cap_permitted)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_CAP_PERM,cred,cap_permitted);
+}
+
+static void __maybe_unused iee_set_cred_cap_effective(struct cred *cred, kernel_cap_t cap_effective)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_CAP_EFFECT,cred,cap_effective);
+}
+
+static void __maybe_unused iee_set_cred_cap_bset(struct cred *cred, kernel_cap_t cap_bset)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_CAP_BSET,cred,cap_bset);
+}
+
+static void __maybe_unused iee_set_cred_cap_ambient(struct cred *cred, kernel_cap_t cap_ambient)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_CAP_AMBIENT,cred,cap_ambient);
+}
+
+#ifdef CONFIG_KEYS
+static void __maybe_unused iee_set_cred_jit_keyring(struct cred *cred, unsigned char jit_keyring)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_JIT_KEYRING,cred,jit_keyring);
+}
+
+static void __maybe_unused iee_set_cred_session_keyring(struct cred *cred, struct key *session_keyring)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_SESS_KEYRING,cred,session_keyring);
+}
+
+static void __maybe_unused iee_set_cred_process_keyring(struct cred *cred, struct key *process_keyring)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_PROC_KEYRING,cred,process_keyring);
+}
+
+static void __maybe_unused iee_set_cred_thread_keyring(struct cred *cred, struct key *thread_keyring)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_THREAD_KEYRING,cred,thread_keyring);
+}
+
+static void __maybe_unused iee_set_cred_request_key_auth(struct cred *cred, struct key *request_key_auth)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_REQ_KEYRING,cred,request_key_auth);
+}
+#endif
+
+static void __maybe_unused iee_set_cred_atomic_set_usage(struct cred *cred, int i)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_ATSET_USAGE,cred,i);
+}
+
+#ifdef CONFIG_SECURITY
+static void __maybe_unused iee_set_cred_security(struct cred *cred, void *security)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_SECURITY,cred,security);
+}
+#endif
+
+static void __maybe_unused iee_set_cred_rcu(struct cred *cred, struct rcu_head *rcu)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_RCU,cred,rcu);
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee-def.h b/arch/arm64/include/asm/iee-def.h
new file mode 100644
index 000000000000..ff37f352f823
--- /dev/null
+++ b/arch/arm64/include/asm/iee-def.h
@@ -0,0 +1,73 @@
+// Function Identifiers with Parameters Description
+
+#define IEE_WRITE_IN_BYTE 0                 // Parameters: void *ptr, __u64 data, int length
+#define IEE_OP_SET_PTE 1                    // Parameters: pte_t *ptep, pte_t pte
+#define IEE_OP_SET_PMD 2                    // Parameters: pmd_t *pmdp, pmd_t pmd
+#define IEE_OP_SET_PUD 3                    // Parameters: pud_t *pudp, pud_t pud
+#define IEE_OP_SET_P4D 4                    // Parameters: p4d_t *p4dp, p4d_t p4d
+#define IEE_OP_SET_BM_PTE 5                 // Parameters: pte_t *ptep, pte_t pte
+#define IEE_OP_SET_SWAPPER_PGD 6            // Parameters: pgd_t *pgdp, pgd_t pgd
+#define IEE_OP_SET_TRAMP_PGD 7              // Parameters: pgd_t *pgdp, pgd_t pgd
+#define IEE_OP_SET_CMPXCHG 8                // Parameters: pte_t *ptep, pteval_t old_pteval, pteval_t new_pteval
+#define IEE_OP_SET_XCHG 9                   // Parameters: pte_t *ptep, pteval_t pteval
+#define IEE_OP_COPY_CRED 10                 // Parameters: struct cred *old, struct cred *new
+#define IEE_OP_SET_CRED_UID 11              // Parameters: struct cred *cred, kuid_t uid
+#define IEE_OP_SET_CRED_GID 12              // Parameters: struct cred *cred, kgid_t gid
+#define IEE_OP_SET_CRED_SUID 13             // Parameters: struct cred *cred, kuid_t suid
+#define IEE_OP_SET_CRED_SGID 14             // Parameters: struct cred *cred, kgid_t sgid
+#define IEE_OP_SET_CRED_EUID 15             // Parameters: struct cred *cred, kuid_t euid
+#define IEE_OP_SET_CRED_EGID 16             // Parameters: struct cred *cred, kgid_t egid
+#define IEE_OP_SET_CRED_FSUID 17            // Parameters: struct cred *cred, kuid_t fsuid
+#define IEE_OP_SET_CRED_FSGID 18            // Parameters: struct cred *cred, kgid_t fsgid
+#define IEE_OP_SET_CRED_USER 19             // Parameters: struct cred *cred, struct user_struct *user
+#define IEE_OP_SET_CRED_USER_NS 20          // Parameters: struct cred *cred, struct user_namespace *user_ns
+#define IEE_OP_SET_CRED_GROUP_INFO 21       // Parameters: struct cred *cred, struct group_info *group_info
+#define IEE_OP_SET_CRED_SECUREBITS 22       // Parameters: struct cred *cred, unsigned securebits
+#define IEE_OP_SET_CRED_CAP_INHER 23        // Parameters: struct cred *cred, kernel_cap_t cap_inheritable
+#define IEE_OP_SET_CRED_CAP_PERM 24         // Parameters: struct cred *cred, kernel_cap_t cap_permitted
+#define IEE_OP_SET_CRED_CAP_EFFECT 25       // Parameters: struct cred *cred, kernel_cap_t cap_effective
+#define IEE_OP_SET_CRED_CAP_BSET 26         // Parameters: struct cred *cred, kernel_cap_t cap_bset
+#define IEE_OP_SET_CRED_CAP_AMBIENT 27      // Parameters: struct cred *cred, kernel_cap_t cap_ambient
+#define IEE_OP_SET_CRED_JIT_KEYRING 28      // Parameters: struct cred *cred, unsigned char jit_keyring
+#define IEE_OP_SET_CRED_SESS_KEYRING 29     // Parameters: struct cred *cred, struct key *session_keyring
+#define IEE_OP_SET_CRED_PROC_KEYRING 30     // Parameters: struct cred *cred, struct key *process_keyring
+#define IEE_OP_SET_CRED_THREAD_KEYRING 31   // Parameters: struct cred *cred, struct key *thread_keyring
+#define IEE_OP_SET_CRED_REQ_KEYRING 32      // Parameters: struct cred *cred, struct key *request_key_auth
+#define IEE_OP_SET_CRED_NON_RCU 33          // Parameters: struct cred *cred, int non_rcu
+#define IEE_OP_SET_CRED_ATSET_USAGE 34      // Parameters: struct cred *cred, int i
+#define IEE_OP_SET_CRED_ATOP_USAGE 35       // Parameters: struct cred *cred, int flag
+#define IEE_OP_SET_CRED_SECURITY 36         // Parameters: struct cred *cred, void *security
+#define IEE_OP_SET_CRED_RCU 37              // Parameters: struct cred *cred, struct rcu_head *rcu
+#define IEE_MEMSET 38                       // Parameters: void *ptr, int data, size_t n
+#define IEE_OP_SET_TRACK 39                 // Parameters: struct track *ptr, struct track *data
+#define IEE_OP_SET_FREEPTR 40               // Parameters: void **pptr, void *ptr
+#define IEE_OP_SET_PTE_U 41                 // Parameters: pte_t *ptep, pte_t pte
+#define IEE_OP_SET_PTE_P 42                 // Parameters: pte_t *ptep, pte_t pte
+#define IEE_SET_TOKEN_MM 43                 // Parameters: struct task_token *token, struct mm_struct *mm
+#define IEE_SET_TOKEN_PGD 44                // Parameters: struct task_token *token, pgd_t *pgd
+#define IEE_INIT_TOKEN 45                   // Parameters: struct task_struct *tsk, void *kernel_stack, void *iee_stack
+#define IEE_FREE_TOKEN 46                   // Parameters: struct task_struct *tsk
+#define IEE_READ_TOKEN_STACK 47             // Parameters: struct task_struct *tsk
+#define IEE_WRITE_ENTRY_TASK 48             // Parameters: struct task_struct *tsk
+#ifdef CONFIG_KOI
+#define IEE_READ_KOI_STACK 49               // Parameters: struct task_struct *tsk
+#define IEE_WRITE_KOI_STACK 50              // Parameters: struct task_struct *tsk, unsigned long koi_stack
+#define IEE_READ_TOKEN_TTBR1 51             // Parameters: struct task_struct *tsk
+#define IEE_WRITE_TOKEN_TTBR1 52            // Parameters: struct task_struct *tsk, unsigned long current_ttbr1
+#define IEE_READ_KOI_KERNEL_STACK 53            // Parameters: struct task_struct *tsk
+#define IEE_WRITE_KOI_KERNEL_STACK 54           // Parameters: struct task_struct *tsk, unsigned long kernel_stack
+#define IEE_READ_KOI_STACK_BASE 55          // Parameters: struct task_struct *tsk
+#define IEE_WRITE_KOI_STACK_BASE 56         // Parameters: struct task_struct *tsk, unsigned long koi_stack_base
+#endif
+
+/* Add new IEE ops here */
+
+#define AT_INC 1
+#define AT_INC_NOT_ZERO 2
+#define AT_DEC_AND_TEST 3
+/* Atomic ops for atomic_t */
+
+#ifdef CONFIG_KOI
+#define IEE_SWITCH_TO_KERNEL    7
+#define IEE_SWITCH_TO_KOI       8
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee-si.h b/arch/arm64/include/asm/iee-si.h
new file mode 100644
index 000000000000..54f4e94c559d
--- /dev/null
+++ b/arch/arm64/include/asm/iee-si.h
@@ -0,0 +1,70 @@
+#ifndef _LINUX_IEE_SI_H
+#define _LINUX_IEE_SI_H
+
+#include <asm/sysreg.h>
+#define __iee_si_code   __section(".iee.si_text")
+#define __iee_si_base   __section(".iee.si_base")
+#define __iee_si_data   __section(".iee.si_data")
+
+/* Used for copying globals that iee rwx gate needs. */
+// extern unsigned long iee_base_kimage_voffset;
+// extern unsigned long iee_base_memstart_addr;
+extern unsigned long iee_base_iee_pg_dir;
+extern unsigned long iee_base_swapper_pg_dir;
+extern unsigned long iee_base_idmap_pg_dir;
+extern unsigned long iee_base_reserved_pg_dir;
+extern unsigned long iee_base__bp_harden_el1_vectors;
+extern pgd_t iee_pg_dir[PTRS_PER_PGD];
+extern bool iee_init_done;
+
+DECLARE_PER_CPU(unsigned long, iee_si_user_bvr0);
+DECLARE_PER_CPU(unsigned long, iee_si_user_bcr0);
+
+/* The following are __init functions used for iee si initialization. */
+extern void mark_idmap_vmallc_map_ROU(void);
+extern void iee_si_set_base_swapper_cnp(void);
+extern void isolate_iee_si(void);
+
+// Handler function for sensitive inst
+u64 iee_si_handler(int flag, ...);
+/* 
+ * TODO: scan a page to check whether it contains sensitive instructions 
+ * return 1 when finding sensitive inst, 0 on safe page.
+ */
+extern int iee_si_scan_page(unsigned long addr);
+
+
+#define DBG_MDSCR_SS		(1 << 0)
+#define DBG_MDSCR_MDE		(1 << 15)
+
+#define IEE_SI_TEST 0
+#define IEE_WRITE_SCTLR 1
+#define IEE_WRITE_TTBR0 2
+#define IEE_WRITE_VBAR  3
+#define IEE_WRITE_TCR   4
+#define IEE_WRITE_MDSCR   5
+#define IEE_CONTEXT_SWITCH      6
+#define IEE_WRITE_AFSR0     10
+/* Provide ttbr1 switch gate for KOI */
+#ifdef CONFIG_KOI
+#define IEE_SWITCH_TO_KERNEL    7
+#define IEE_SWITCH_TO_KOI       8
+#endif
+/* MASK modify-permitted bits on IEE protected sys registers */
+#define IEE_SCTLR_MASK  (SCTLR_EL1_CP15BEN | SCTLR_EL1_SED | SCTLR_EL1_UCT | SCTLR_EL1_UCI |\
+                    SCTLR_EL1_BT0 | SCTLR_EL1_BT1 | SCTLR_EL1_TCF0_MASK | SCTLR_ELx_DSSBS)
+#define IEE_TTBR0_MASK  ~0
+#define IEE_TTBR1_MASK  ~0
+#define IEE_TCR_MASK    (TCR_HD | TCR_T0SZ_MASK)
+#define IEE_MDSCR_MASK  (DBG_MDSCR_SS | DBG_MDSCR_MDE)
+
+#define IEE_DBGBCR_BT 0b0000 << 20
+#define IEE_DBGBCR_SSC 0b00 << 14
+#define IEE_DBGBCR_HMC 0b1 << 13
+#define IEE_DBGBCR_BAS 0b1111 << 5
+#define IEE_DBGBCR_PMC 0b11 << 1
+#define IEE_DBGBCR_E 0b1
+#define IEE_DBGBCR IEE_DBGBCR_BT | IEE_DBGBCR_SSC | IEE_DBGBCR_HMC | IEE_DBGBCR_BAS \
+                | IEE_DBGBCR_PMC | IEE_DBGBCR_E
+
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee-slab.h b/arch/arm64/include/asm/iee-slab.h
new file mode 100644
index 000000000000..7a0bba204483
--- /dev/null
+++ b/arch/arm64/include/asm/iee-slab.h
@@ -0,0 +1,16 @@
+#ifndef _LINUX_IEE_SLAB_H
+#define _LINUX_IEE_SLAB_H
+
+#define TRACK_ADDRS_COUNT 16
+
+struct track {
+	unsigned long addr;	/* Called from address */
+#ifdef CONFIG_STACKTRACE
+	unsigned long addrs[TRACK_ADDRS_COUNT];	/* Called from address */
+#endif
+	int cpu;		/* Was running on cpu */
+	int pid;		/* Pid context */
+	unsigned long when;	/* When did the operation occur */
+};
+
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee-token.h b/arch/arm64/include/asm/iee-token.h
new file mode 100644
index 000000000000..152474e1a187
--- /dev/null
+++ b/arch/arm64/include/asm/iee-token.h
@@ -0,0 +1,40 @@
+#ifndef _LINUX_IEE_TOKEN_H
+#define _LINUX_IEE_TOKEN_H
+
+#include <asm/iee-def.h>
+
+extern unsigned long long iee_rw_gate(int flag, ...);
+struct task_token;
+struct task_struct;
+struct mm_struct;
+
+#ifdef CONFIG_IEE
+void iee_set_token_mm(struct task_struct *tsk, struct mm_struct *mm)
+{
+	iee_rw_gate(IEE_SET_TOKEN_MM, tsk, mm);
+}
+
+void iee_set_token_pgd(struct task_struct *tsk, pgd_t *pgd)
+{
+	iee_rw_gate(IEE_SET_TOKEN_PGD, tsk, pgd);
+}
+
+void iee_init_token(struct task_struct *tsk, void *kernel_stack, void *iee_stack)
+{
+	iee_rw_gate(IEE_INIT_TOKEN, tsk, kernel_stack, iee_stack);
+}
+
+void iee_free_token(struct task_struct *tsk)
+{
+	iee_rw_gate(IEE_FREE_TOKEN, tsk);
+}
+
+unsigned long iee_read_token_stack(struct task_struct *tsk)
+{
+	unsigned long ret;
+	ret = iee_rw_gate(IEE_READ_TOKEN_STACK, tsk);
+	return ret;
+}
+#endif
+
+#endif
\ No newline at end of file
diff --git a/arch/arm64/include/asm/iee.h b/arch/arm64/include/asm/iee.h
new file mode 100644
index 000000000000..598f6d0b2626
--- /dev/null
+++ b/arch/arm64/include/asm/iee.h
@@ -0,0 +1,10 @@
+#ifndef _LINUX_IEE_H
+#define _LINUX_IEE_H
+#define __iee_code		__section(".iee.text")
+#define __iee_header  __section(".iee.text.header")
+
+u64 iee_dispatch(int flag, ...);
+
+#include <asm/iee-def.h>
+
+#endif
diff --git a/arch/arm64/include/asm/irqflags.h b/arch/arm64/include/asm/irqflags.h
index ff328e5bbb75..022acbea44cc 100644
--- a/arch/arm64/include/asm/irqflags.h
+++ b/arch/arm64/include/asm/irqflags.h
@@ -121,6 +121,17 @@ static inline unsigned long arch_local_irq_save(void)
  */
 static inline void arch_local_irq_restore(unsigned long flags)
 {
+// #ifdef CONFIG_IEE
+// 	asm volatile(ALTERNATIVE(
+// 		IEE_SI_WRITE_DAIF_SEL,
+// 		__msr_s(SYS_ICC_PMR_EL1, "%0")"\n\tb 114f\n\tnop\n\tnop\n\tnop\n\tnop"
+//         "\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop"
+//         "\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\tnop\n\t114:\n\t",
+// 		ARM64_HAS_IRQ_PRIO_MASKING)
+// 		:
+// 		: "r" (flags)
+// 		: "memory");
+// #else
 	asm volatile(ALTERNATIVE(
 		"msr	daif, %0",
 		__msr_s(SYS_ICC_PMR_EL1, "%0"),
@@ -129,6 +140,7 @@ static inline void arch_local_irq_restore(unsigned long flags)
 		: "r" (flags)
 		: "memory");
 
+// #endif
 	pmr_sync();
 }
 
diff --git a/arch/arm64/include/asm/kernel-pgtable.h b/arch/arm64/include/asm/kernel-pgtable.h
index 4b06cf9a8c8a..67b9b6c04093 100644
--- a/arch/arm64/include/asm/kernel-pgtable.h
+++ b/arch/arm64/include/asm/kernel-pgtable.h
@@ -115,6 +115,18 @@
 #define SWAPPER_MM_MMUFLAGS	(PTE_ATTRINDX(MT_NORMAL) | SWAPPER_PTE_FLAGS)
 #endif
 
+#ifdef CONFIG_IEE
+
+#define SWAPPER_PTE_FLAGS_IDMAP	(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED | PTE_RDONLY)
+#define SWAPPER_PMD_FLAGS_IDMAP	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S | PMD_SECT_RDONLY)
+
+#if ARM64_SWAPPER_USES_SECTION_MAPS
+#define SWAPPER_MM_MMUFLAGS_IDMAP	(PMD_ATTRINDX(MT_NORMAL) | SWAPPER_PMD_FLAGS_IDMAP)
+#else
+#define SWAPPER_MM_MMUFLAGS_IDMAP	(PTE_ATTRINDX(MT_NORMAL) | SWAPPER_PTE_FLAGS_IDMAP)
+#endif
+
+#endif
 /*
  * To make optimal use of block mappings when laying out the linear
  * mapping, round down the base of physical memory to a size that can
diff --git a/arch/arm64/include/asm/koi.h b/arch/arm64/include/asm/koi.h
new file mode 100644
index 000000000000..48d9a1378a1d
--- /dev/null
+++ b/arch/arm64/include/asm/koi.h
@@ -0,0 +1,335 @@
+#include "linux/mm.h"
+#include "asm/current.h"
+#include "asm/pgtable-hwdef.h"
+#include "asm/pgtable-types.h"
+#include "asm/pgtable.h"
+#include "linux/mm_types.h"
+#include "linux/pgtable.h"
+#include "linux/printk.h"
+#include "linux/slab.h"
+#include "linux/string.h"
+#include <linux/sched.h>
+#include "linux/hashtable.h"
+#include "linux/module.h"
+#include "linux/vmalloc.h"
+#include "stacktrace.h"
+#include "asm/mmu.h"
+#ifdef CONFIG_IEE
+#include "asm/iee-si.h"
+#include "asm/iee-def.h"
+#endif
+
+#define HASH_TABLE_BIT 10
+#define HASH_TABLE_LEN (1 << HASH_TABLE_BIT)
+#define HASH_KEY_MASK ((1 << HASH_TABLE_BIT) - 1)
+
+#define MAX_VAR_NAME 64
+#define DRIVER_ISOLATION_VAR_ARRAY_SIZE 32
+#define DRIVER_ISOLATION_MAX_VAL 256
+
+extern struct hlist_head koi_mem_htbl[1024];
+extern spinlock_t koi_mem_htbl_spin_lock;
+extern unsigned long koi_swapper_ttbr1;
+extern s64 koi_offset;
+
+#ifdef CONFIG_IEE
+extern unsigned long long iee_rw_gate(int flag, ...);
+#endif
+
+DECLARE_PER_CPU(unsigned long[PAGE_SIZE / sizeof(unsigned long)],
+		koi_irq_current_ttbr1);
+
+/**
+* struct koi_mem_hash_node - 
+*@mod:pointer to driver module
+*@mem_list_head:free memory list head
+*@ko_mm: mm_struct in each driver
+*@pgdp:entry to Page Global Directory :pgd
+*@node:hash linked list node
+*@addr_htbl[1 << (HASH_TABLE_BIT)]: 
+*@rcu:
+*/
+struct koi_mem_hash_node {
+	struct module *mod;
+	struct list_head mem_list_head;
+	struct mm_struct *ko_mm;
+	pgd_t *pgdp;
+	unsigned long ko_ttbr1;
+	struct hlist_node node;
+	struct hlist_head addr_htbl[1 << (HASH_TABLE_BIT)];
+	struct rcu_head rcu;
+	// used to protect free mem list
+	spinlock_t spin_lock;
+	// used to protect addr hashtable
+	spinlock_t addr_htbl_spin_lock;
+};
+//describe the global shared var
+struct shared_variable_descriptor {
+	unsigned int id;
+	unsigned int type;
+	char name[MAX_VAR_NAME];
+	unsigned long offset;
+	unsigned int size;
+	unsigned int self_ptr_ids[DRIVER_ISOLATION_VAR_ARRAY_SIZE];
+};
+
+int koi_do_switch_to_kernel_pgtbl(void);
+
+int koi_copy_pagetable(struct mm_struct *ko_mm, pgd_t *koi_pg_dir,
+		       unsigned long addr, unsigned long end);
+
+void koi_create_pagetable(struct module *mod);
+
+void koi_map_kostack(struct module *mod);
+unsigned long koi_mem_alloc(struct module *mod, unsigned long orig_addr,
+			    unsigned long size);
+void koi_mem_free(struct module *mod, unsigned long addr, unsigned long size,
+		  bool is_const, int count, ...);
+void *koi_mem_lookup(struct module *mod, unsigned long addr);
+void koi_mem_free_callback(struct module *mod, unsigned long addr,
+			   unsigned long size, void (*func)(void *));
+void koi_map_mem(struct module *mod, unsigned long addr, unsigned long size);
+void koi_mem_free_to_user(struct module *mod, unsigned long addr,
+			  unsigned long size);
+
+unsigned long koi_ttbr_ctor(struct module *mod);
+extern void koi_do_switch_to_kernel_stack(void);
+extern void koi_do_switch_to_ko_stack(void);
+
+#define switch_pgtable(ttbr1)                                                  \
+	do {                                                                   \
+		write_sysreg(ttbr1, ttbr1_el1);                                \
+		isb();                                                         \
+		asm volatile(ALTERNATIVE("nop; nop; nop",                      \
+					 "ic iallu; dsb nsh; isb",             \
+					 ARM64_WORKAROUND_CAVIUM_27456));      \
+	} while (0);
+
+#ifndef CONFIG_IEE
+#define koi_switch_to_ko()                                                     \
+	do {                                                                   \
+		unsigned long flags, ko_ttbr1, cur_sp;                         \
+		unsigned long *ptr;                                            \
+		struct task_token *token;                                      \
+		asm volatile("mrs %0, daif\n"                                  \
+			     "msr daifset, #2\n"                               \
+			     "isb\n"                                           \
+			     "mov %1, sp\n"                                    \
+			     : "=r"(flags), "=r"(cur_sp)                       \
+			     :);                                               \
+		if (!on_irq_stack(cur_sp, NULL)) {                             \
+			koi_do_switch_to_ko_stack();                           \
+			ko_ttbr1 = koi_ttbr_ctor(THIS_MODULE);                 \
+			token = (struct task_token *)((unsigned long)current + \
+						      koi_offset);             \
+			token->current_ttbr1 = ko_ttbr1 & (~TTBR_ASID_MASK);   \
+		} else {                                                       \
+			ko_ttbr1 = koi_ttbr_ctor(THIS_MODULE);                 \
+			ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1,          \
+					       __kern_my_cpu_offset());        \
+			*ptr = ko_ttbr1 & ~(TTBR_ASID_MASK);                   \
+		}                                                              \
+		switch_pgtable(ko_ttbr1);                                      \
+		asm volatile("msr daif, %0\n"                                  \
+			     "isb\n"                                           \
+			     :                                                 \
+			     : "r"(flags));                                    \
+	} while (0);
+
+#define koi_switch_to_kernel()                                                 \
+	do {                                                                   \
+		unsigned long cur_sp, flags, asid;                                   \
+		unsigned long *ptr;                                            \
+		struct task_token *token;                                      \
+		asm volatile("mrs %0, daif\n"                                  \
+			     "msr daifset, #2\n"                               \
+			     "isb\n"                                           \
+			     "mov %1, sp\n"                                    \
+                 "mov %2, ttbr0_el1\n"                              \
+			     : "=r"(flags), "=r"(cur_sp), "=r"(asid)                     \
+			     :);                                               \
+        asid &= ~USER_ASID_FLAG;            \
+        asid &= TTBR_ASID_MASK;             \
+		switch_pgtable(koi_swapper_ttbr1);                             \
+		if (!on_irq_stack(cur_sp, NULL)) {                             \
+			token = (struct task_token *)((unsigned long)current + \
+						      koi_offset);             \
+			token->current_ttbr1 = koi_swapper_ttbr1;              \
+			koi_do_switch_to_kernel_stack();                       \
+		} else {                                                       \
+			ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1,              \
+					       __kern_my_cpu_offset());        \
+			*ptr = koi_swapper_ttbr1;                                  \
+		}                                                              \
+		asm volatile("msr daif, %0\n"                                  \
+			     "isb\n"                                           \
+			     :                                                 \
+			     : "r"(flags));                                    \
+	} while (0);
+#else
+#define koi_switch_to_ko()                                                     \
+	do {                                                                   \
+		unsigned long cur_sp, flags, ko_ttbr1;                         \
+		unsigned long *ptr;                                            \
+		asm volatile("mrs %0, daif\n"                                  \
+			     "msr daifset, #2\n"                               \
+			     "isb\n"                                           \
+			     "mov %1, sp\n"                                    \
+			     : "=r"(flags), "=r"(cur_sp)                       \
+			     :);                                               \
+		if (!on_irq_stack(cur_sp, NULL)) {                             \
+			koi_do_switch_to_ko_stack();                           \
+			ko_ttbr1 = koi_ttbr_ctor(THIS_MODULE);                 \
+			iee_rw_gate(IEE_WRITE_TOKEN_TTBR1, current,            \
+				    ko_ttbr1 &(~TTBR_ASID_MASK));              \
+		} else {                                                       \
+			ko_ttbr1 = koi_ttbr_ctor(THIS_MODULE);                 \
+			ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1,          \
+					       __kern_my_cpu_offset());        \
+			*ptr = ko_ttbr1 & (~TTBR_ASID_MASK);                   \
+		}                                                              \
+		iee_rwx_gate_entry(IEE_SWITCH_TO_KOI, ko_ttbr1);               \
+		asm volatile("msr daif, %0\n"                                  \
+			     "isb\n"                                           \
+			     :                                                 \
+			     : "r"(flags));                                    \
+	} while (0);
+
+#define koi_switch_to_kernel()                                                 \
+	do {                                                                   \
+		unsigned long flags, cur_sp;                                   \
+		unsigned long *ptr;                                            \
+		asm volatile("mrs %0, daif\n"                                  \
+			     "msr daifset, #2\n"                               \
+			     "isb\n"                                           \
+			     "mov %1, sp\n"                                    \
+			     : "=r"(flags), "=r"(cur_sp)                       \
+			     :);                                               \
+		iee_rwx_gate_entry(IEE_SWITCH_TO_KERNEL);                      \
+		if (!on_irq_stack(cur_sp, NULL)) {                             \
+			iee_rw_gate(IEE_WRITE_TOKEN_TTBR1, current,            \
+				    koi_swapper_ttbr1);                        \
+			koi_do_switch_to_kernel_stack();                       \
+		} else {                                                       \
+			ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1,              \
+					       __kern_my_cpu_offset());        \
+			*ptr = koi_swapper_ttbr1;                                  \
+		}                                                              \
+		asm volatile("msr daif, %0\n"                                  \
+			     "isb\n"                                           \
+			     :                                                 \
+			     : "r"(flags));                                    \
+	} while (0);
+#endif
+//kzalloc function in driver space
+static __maybe_unused noinline void *
+koi_kzalloc_wrapper(struct module *mod, size_t size, gfp_t flags)
+{
+	int cnt = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	void *addr;
+	struct koi_mem_hash_node *target = NULL;
+	koi_switch_to_kernel();
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk("mem node for module: %s not found\n", mod->name);
+		return NULL;
+	}
+
+	addr = kzalloc(size, flags);
+	koi_copy_pagetable(target->ko_mm, target->pgdp, (unsigned long)addr,
+			   (unsigned long)addr + PAGE_SIZE * cnt);
+	koi_switch_to_ko();
+	return addr;
+}
+//kmalloc function in driver space
+static __maybe_unused __always_inline void *
+koi_kmalloc_wrapper(struct module *mod, size_t size, gfp_t flags)
+{
+	int cnt = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	void *addr;
+	struct koi_mem_hash_node *target = NULL;
+	koi_switch_to_kernel();
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk("mem node for module: %s not found\n", mod->name);
+		return 0;
+	}
+
+	addr = kmalloc(cnt * PAGE_SIZE, flags);
+	koi_copy_pagetable(target->ko_mm, target->pgdp, (unsigned long)addr,
+			   (unsigned long)addr + PAGE_SIZE * cnt);
+	koi_switch_to_ko();
+	return (void *)addr;
+}
+//vmalloc function in driver space
+static __maybe_unused void *koi_vmalloc_wrapper(struct module *mod,
+						unsigned long size)
+{
+	int cnt = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+	void *addr;
+	struct koi_mem_hash_node *target = NULL;
+	koi_switch_to_kernel();
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk("mem node for module: %s not found\n", mod->name);
+		koi_switch_to_ko();
+		return 0;
+	}
+	addr = vmalloc(cnt * PAGE_SIZE);
+	koi_copy_pagetable(target->ko_mm, target->pgdp, (unsigned long)addr,
+			   (unsigned long)addr + PAGE_SIZE * cnt);
+	koi_switch_to_ko();
+	return addr;
+}
+//kmalloc_array function in driver space
+static __maybe_unused void *koi_kmalloc_array_wrapper(struct module *mod,
+						      size_t n, size_t size,
+						      gfp_t flags)
+{
+	int kpage;
+	void *addr;
+	struct koi_mem_hash_node *target = NULL;
+	koi_switch_to_kernel();
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk("mem node for module: %s not found\n", mod->name);
+		koi_switch_to_ko();
+		return 0;
+	}
+	kpage = (n * size + PAGE_SIZE - 1) / PAGE_SIZE;
+	n = (kpage * PAGE_SIZE) / size;
+	addr = kmalloc_array(n, size, flags);
+	koi_copy_pagetable(target->ko_mm, target->pgdp, (unsigned long)addr,
+			   (unsigned long)addr + PAGE_SIZE * kpage);
+	koi_switch_to_ko();
+	return addr;
+}
\ No newline at end of file
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 644cfa3284a7..89ae2e67e6c0 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -170,6 +170,13 @@ extern u64			vabits_actual;
 #define PAGE_END		(_PAGE_END(vabits_actual))
 
 extern s64			memstart_addr;
+
+#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+extern s64			memstart_addr_init;
+extern s64			iee_offset;
+#define LOGICAL_RANDOM	(long long int)((long unsigned int)__va(memstart_addr_init) & (~PAGE_OFFSET))
+#endif
+
 /* PHYS_OFFSET - the physical address of the start of memory. */
 #define PHYS_OFFSET		({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })
 
@@ -265,6 +272,23 @@ extern phys_addr_t __phys_addr_symbol(unsigned long x);
 #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
 #define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
 
+#ifdef CONFIG_KOI
+#define KOI_OFFSET ((unsigned long)0x4 << 44)
+#endif
+
+#ifdef CONFIG_IEE
+#ifdef CONFIG_IEE_OFFSET
+#define IEE_OFFSET	((CONFIG_IEE_OFFSET) - LOGICAL_RANDOM)
+#else
+#define IEE_OFFSET	(((unsigned long)0x4 << 44) - LOGICAL_RANDOM)
+#endif
+#define __phys_to_iee(x)	(__phys_to_virt(x) + IEE_OFFSET)
+#define SET_UPAGE(x)	__pgprot(pgprot_val(x) | PTE_USER)
+#define SET_PPAGE(x)	__pgprot(pgprot_val(x) & (~PTE_USER))
+#define SET_INVALID(x)	__pgprot(pgprot_val(x) & (~PTE_VALID))
+#define SET_NG(x)	__pgprot(pgprot_val(x) | PTE_NG)
+#endif
+
 /*
  * Convert a page to/from a physical address
  */
diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
index 5a54a5ab5f92..0dc9cf20d99c 100644
--- a/arch/arm64/include/asm/mmu_context.h
+++ b/arch/arm64/include/asm/mmu_context.h
@@ -42,7 +42,11 @@ static inline void cpu_set_reserved_ttbr0(void)
 {
 	unsigned long ttbr = phys_to_ttbr(__pa_symbol(reserved_pg_dir));
 
+#ifdef CONFIG_IEE
+    iee_rwx_gate_entry(IEE_WRITE_ttbr0_el1, ttbr);
+#else
 	write_sysreg(ttbr, ttbr0_el1);
+#endif
 	isb();
 }
 
@@ -90,7 +94,11 @@ static inline void __cpu_set_tcr_t0sz(unsigned long t0sz)
 	tcr = read_sysreg(tcr_el1);
 	tcr &= ~TCR_T0SZ_MASK;
 	tcr |= t0sz << TCR_T0SZ_OFFSET;
+#ifdef CONFIG_IEE
+    iee_rwx_gate_entry(IEE_WRITE_tcr_el1, tcr);
+#else
 	write_sysreg(tcr, tcr_el1);
+#endif
 	isb();
 }
 
@@ -155,6 +163,10 @@ static inline void cpu_replace_ttbr1(pgd_t *pgdp)
 		ttbr1 |= TTBR_CNP_BIT;
 	}
 
+	#ifdef CONFIG_IEE
+	ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, ASID(current->active_mm));
+	#endif
+
 	replace_phys = (void *)__pa_symbol(idmap_cpu_replace_ttbr1);
 
 	cpu_install_idmap();
diff --git a/arch/arm64/include/asm/pgalloc.h b/arch/arm64/include/asm/pgalloc.h
index 3c6a7f5988b1..157aead56290 100644
--- a/arch/arm64/include/asm/pgalloc.h
+++ b/arch/arm64/include/asm/pgalloc.h
@@ -57,6 +57,10 @@ static inline void __p4d_populate(p4d_t *p4dp, phys_addr_t pudp, p4dval_t prot)
 extern pgd_t *pgd_alloc(struct mm_struct *mm);
 extern void pgd_free(struct mm_struct *mm, pgd_t *pgdp);
 
+#ifdef CONFIG_KOI
+pgd_t *koi_pgd_alloc(void);
+#endif
+
 static inline void __pmd_populate(pmd_t *pmdp, phys_addr_t ptep,
 				  pmdval_t prot)
 {
diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index a5cff5b376f6..bf8f376b6219 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -91,6 +91,10 @@
 #define CONT_PMD_SIZE		(CONT_PMDS * PMD_SIZE)
 #define CONT_PMD_MASK		(~(CONT_PMD_SIZE - 1))
 
+#define PGD_APT		(_AT(pudval_t, 1) << 61)
+#define PGD_PXN		(_AT(pudval_t, 1) << 59)
+#define PGD_UXN		(_AT(pudval_t, 1) << 60)
+
 /*
  * Hardware page table definitions.
  *
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 1999bda3be61..a45a34cfd128 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -35,6 +35,9 @@
 #include <linux/mmdebug.h>
 #include <linux/mm_types.h>
 #include <linux/sched.h>
+#ifdef CONFIG_PTP
+#include <asm/iee.h>
+#endif
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define __HAVE_ARCH_FLUSH_PMD_TLB_RANGE
@@ -156,6 +159,30 @@ static inline pteval_t __phys_to_pte_val(phys_addr_t phys)
 #define pud_access_permitted(pud, write) \
 	(pte_access_permitted(pud_pte(pud), (write)))
 
+#ifdef CONFIG_PTP
+static inline bool in_tramp_pgdir(void *addr);
+extern unsigned long long iee_rw_gate(int flag, ...);
+
+static void iee_set_tramp_pgd_pre_init(pgd_t *pgdp, pgd_t pgd)
+{
+	iee_rw_gate(IEE_OP_SET_TRAMP_PGD, pgdp, pgd);
+}
+
+static noinline pteval_t iee_set_xchg_relaxed(pte_t *ptep, pteval_t pteval)
+{
+	pteval_t ret;
+	ret = iee_rw_gate(IEE_OP_SET_XCHG, ptep, pteval);
+	return (pteval_t)ret;
+}
+
+static noinline pteval_t iee_set_cmpxchg_relaxed(pte_t *ptep, pteval_t old_pteval, pteval_t new_pteval)
+{
+	pteval_t ret;
+	ret = iee_rw_gate(IEE_OP_SET_CMPXCHG, ptep, old_pteval, new_pteval);
+	return (pteval_t)ret;
+}
+#endif
+
 static inline pte_t clear_pte_bit(pte_t pte, pgprot_t prot)
 {
 	pte_val(pte) &= ~pgprot_val(prot);
@@ -260,8 +287,28 @@ static inline pte_t pte_mkdevmap(pte_t pte)
 	return set_pte_bit(pte, __pgprot(PTE_DEVMAP | PTE_SPECIAL));
 }
 
-static inline void set_pte(pte_t *ptep, pte_t pte)
+#ifdef CONFIG_PTP
+static inline void iee_set_bm_pte(pte_t *ptep, pte_t pte)
+{
+	iee_rw_gate(IEE_OP_SET_BM_PTE, ptep, pte);
+
+	/*
+	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
+	 * or update_mmu_cache() have the necessary barriers.
+	 */
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
+}
+
+static inline void iee_set_fixmap_pte_pre_init(pte_t *ptep, pte_t pte)
 {
+#ifdef CONFIG_KOI
+    if (!pte_none(pte)) {
+        pte = __pte(pte_val(pte) | PTE_NG);
+    }
+#endif
 	WRITE_ONCE(*ptep, pte);
 
 	/*
@@ -273,6 +320,57 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 		isb();
 	}
 }
+#endif
+
+static inline void set_pte(pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_KOI
+    if (!pte_none(pte)) {
+		pte = __pte(pte_val(pte) | PTE_NG);
+    }
+#endif
+#ifdef CONFIG_PTP
+	iee_rw_gate(IEE_OP_SET_PTE, ptep, pte);
+	dsb(ishst);
+	isb();
+#else
+	WRITE_ONCE(*ptep, pte);
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
+#endif
+}
+
+#ifdef CONFIG_IEE
+static inline void iee_set_pte_upage(pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_PTP
+	iee_rw_gate(IEE_OP_SET_PTE_U, ptep, pte);
+	dsb(ishst);
+	isb();
+#else
+	WRITE_ONCE(*ptep, pte);
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
+#endif
+}
+
+static inline void iee_set_pte_ppage(pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_PTP
+	iee_rw_gate(IEE_OP_SET_PTE_P, ptep, pte);
+#else
+	WRITE_ONCE(*ptep, pte);
+#endif
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
+}
+#endif
 
 extern void __sync_icache_dcache(pte_t pteval);
 
@@ -476,8 +574,42 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 #define pud_pfn(pud)		((__pud_to_phys(pud) & PUD_MASK) >> PAGE_SHIFT)
 #define pfn_pud(pfn,prot)	__pud(__phys_to_pud_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 
+#ifdef CONFIG_PTP
+static inline void set_pmd(pmd_t *pmdp, pmd_t pmd);
+static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			      pmd_t *pmdp, pmd_t pmd)
+{
+	if (pte_present(pmd_pte(pmd)) && pte_user_exec(pmd_pte(pmd)) && !pte_special(pmd_pte(pmd)))
+		__sync_icache_dcache(pmd_pte(pmd));
+
+	if (system_supports_mte() &&
+	    pte_present(pmd_pte(pmd)) && pte_tagged(pmd_pte(pmd)) && !pte_special(pmd_pte(pmd)))
+		mte_sync_tags((pte_t *)pmdp, pmd_pte(pmd));
+
+	__check_racy_pte_update(mm, (pte_t *)pmdp, pmd_pte(pmd));
+
+	set_pmd(pmdp, pmd);
+}
+
+static inline void set_pud(pud_t *pudp, pud_t pud);
+static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
+			      pud_t *pudp, pud_t pud)
+{
+	if (pte_present(pud_pte(pud)) && pte_user_exec(pud_pte(pud)) && !pte_special(pud_pte(pud)))
+		__sync_icache_dcache(pud_pte(pud));
+
+	if (system_supports_mte() &&
+	    pte_present(pud_pte(pud)) && pte_tagged(pud_pte(pud)) && !pte_special(pud_pte(pud)))
+		mte_sync_tags((pte_t *)pudp, pud_pte(pud));
+
+	__check_racy_pte_update(mm, (pte_t *)pudp, pud_pte(pud));
+
+	set_pud(pudp, pud);
+}
+#else
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 #define set_pud_at(mm, addr, pudp, pud)	set_pte_at(mm, addr, (pte_t *)pudp, pud_pte(pud))
+#endif
 
 #define __p4d_to_phys(p4d)	__pte_to_phys(p4d_pte(p4d))
 #define __phys_to_p4d_val(phys)	__phys_to_pte_val(phys)
@@ -568,6 +700,7 @@ static inline bool pud_table(pud_t pud) { return true; }
 extern pgd_t init_pg_dir[PTRS_PER_PGD];
 extern pgd_t init_pg_end[];
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+extern pgd_t iee_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_end[];
 extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
@@ -581,7 +714,22 @@ static inline bool in_swapper_pgdir(void *addr)
 	        ((unsigned long)swapper_pg_dir & PAGE_MASK);
 }
 
-static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
+#ifdef CONFIG_IEE
+static inline bool in_iee_pgdir(void *addr)
+{
+	return ((unsigned long)addr & PAGE_MASK) ==
+	        ((unsigned long)iee_pg_dir & PAGE_MASK);
+}
+#endif
+
+#ifdef CONFIG_PTP
+static inline bool in_tramp_pgdir(void *addr)
+{
+	return ((unsigned long)addr & PAGE_MASK) ==
+	        ((unsigned long)tramp_pg_dir & PAGE_MASK);
+}
+
+static inline void iee_set_fixmap_pmd_pre_init(pmd_t *pmdp, pmd_t pmd)
 {
 #ifdef __PAGETABLE_PMD_FOLDED
 	if (in_swapper_pgdir(pmdp)) {
@@ -589,7 +737,6 @@ static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 		return;
 	}
 #endif /* __PAGETABLE_PMD_FOLDED */
-
 	WRITE_ONCE(*pmdp, pmd);
 
 	if (pmd_valid(pmd)) {
@@ -597,6 +744,32 @@ static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 		isb();
 	}
 }
+#endif
+
+static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
+{
+#ifdef __PAGETABLE_PMD_FOLDED
+	if (in_swapper_pgdir(pmdp)) {
+		set_swapper_pgd((pgd_t *)pmdp, __pgd(pmd_val(pmd)));
+		return;
+	}
+#endif /* __PAGETABLE_PMD_FOLDED */
+#ifdef CONFIG_KOI
+    pmdval_t val = pmd_val(pmd);
+    if (pmd_valid(pmd) && !(val & PMD_TABLE_BIT)) {
+        pmd = __pmd(val | PMD_SECT_NG);
+    }
+#endif
+#ifdef CONFIG_PTP
+	iee_rw_gate(IEE_OP_SET_PMD, pmdp, pmd);
+#else
+	WRITE_ONCE(*pmdp, pmd);
+#endif
+	if (pmd_valid(pmd)) {
+		dsb(ishst);
+		isb();
+	}
+}
 
 static inline void pmd_clear(pmd_t *pmdp)
 {
@@ -616,6 +789,12 @@ static inline unsigned long pmd_page_vaddr(pmd_t pmd)
 /* Find an entry in the third-level page table. */
 #define pte_offset_phys(dir,addr)	(pmd_page_paddr(READ_ONCE(*(dir))) + pte_index(addr) * sizeof(pte_t))
 
+#ifdef CONFIG_PTP
+#define pte_set_fixmap_init(addr)		((pte_t *)iee_set_fixmap_offset_pre_init(FIX_PTE, addr))
+#define pte_set_fixmap_offset_init(pmd, addr)	pte_set_fixmap_init(pte_offset_phys(pmd, addr))
+#define pte_clear_fixmap_init()		clear_fixmap_init(FIX_PTE)
+#endif
+
 #define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))
 #define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
 #define pte_clear_fixmap()		clear_fixmap(FIX_PTE)
@@ -642,7 +821,9 @@ static inline unsigned long pmd_page_vaddr(pmd_t pmd)
 #define pud_leaf(pud)		(pud_present(pud) && !pud_table(pud))
 #define pud_valid(pud)		pte_valid(pud_pte(pud))
 
-static inline void set_pud(pud_t *pudp, pud_t pud)
+
+#ifdef CONFIG_PTP
+static inline void iee_set_fixmap_pud_pre_init(pud_t *pudp, pud_t pud)
 {
 #ifdef __PAGETABLE_PUD_FOLDED
 	if (in_swapper_pgdir(pudp)) {
@@ -650,7 +831,6 @@ static inline void set_pud(pud_t *pudp, pud_t pud)
 		return;
 	}
 #endif /* __PAGETABLE_PUD_FOLDED */
-
 	WRITE_ONCE(*pudp, pud);
 
 	if (pud_valid(pud)) {
@@ -658,6 +838,33 @@ static inline void set_pud(pud_t *pudp, pud_t pud)
 		isb();
 	}
 }
+#endif
+
+static inline void set_pud(pud_t *pudp, pud_t pud)
+{
+#ifdef __PAGETABLE_PUD_FOLDED
+	if (in_swapper_pgdir(pudp)) {
+		set_swapper_pgd((pgd_t *)pudp, __pgd(pud_val(pud)));
+		return;
+	}
+#endif /* __PAGETABLE_PUD_FOLDED */
+#ifdef CONFIG_KOI
+    pudval_t val = pud_val(pud);
+    if (pud_valid(pud) && !(val & PUD_TABLE_BIT)) {
+        // There is no PUD_SEC_NG, so we use PMD_SECT_NG instead.
+        pud = __pud(val | PMD_SECT_NG);
+    }
+#endif
+#ifdef CONFIG_PTP
+	iee_rw_gate(IEE_OP_SET_PUD, pudp, pud);
+#else
+	WRITE_ONCE(*pudp, pud);
+#endif
+	if (pud_valid(pud)) {
+		dsb(ishst);
+		isb();
+	}
+}
 
 static inline void pud_clear(pud_t *pudp)
 {
@@ -677,6 +884,12 @@ static inline pmd_t *pud_pgtable(pud_t pud)
 /* Find an entry in the second-level page table. */
 #define pmd_offset_phys(dir, addr)	(pud_page_paddr(READ_ONCE(*(dir))) + pmd_index(addr) * sizeof(pmd_t))
 
+#ifdef CONFIG_PTP
+#define pmd_set_fixmap_init(addr)		((pmd_t *)iee_set_fixmap_offset_pre_init(FIX_PMD, addr))
+#define pmd_set_fixmap_offset_init(pud, addr)	pmd_set_fixmap_init(pmd_offset_phys(pud, addr))
+#define pmd_clear_fixmap_init()		clear_fixmap_init(FIX_PMD)
+#endif
+
 #define pmd_set_fixmap(addr)		((pmd_t *)set_fixmap_offset(FIX_PMD, addr))
 #define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
 #define pmd_clear_fixmap()		clear_fixmap(FIX_PMD)
@@ -707,15 +920,30 @@ static inline pmd_t *pud_pgtable(pud_t pud)
 #define p4d_none(p4d)		(!p4d_val(p4d))
 #define p4d_bad(p4d)		(!(p4d_val(p4d) & 2))
 #define p4d_present(p4d)	(p4d_val(p4d))
+#define p4d_valid(p4d)		pte_valid(p4d_pte(p4d))
 
 static inline void set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
-	if (in_swapper_pgdir(p4dp)) {
+	#ifdef CONFIG_IEE
+	if (in_swapper_pgdir(p4dp) | in_iee_pgdir(p4dp))
+	#else
+	if (in_swapper_pgdir(p4dp))
+	#endif
+	{
 		set_swapper_pgd((pgd_t *)p4dp, __pgd(p4d_val(p4d)));
 		return;
 	}
 
+#ifdef CONFIG_PTP
+	if(in_tramp_pgdir(p4dp))
+	{
+		iee_set_tramp_pgd_pre_init((pgd_t *)p4dp, __pgd(p4d_val(p4d)));
+		return;
+	}
+	iee_rw_gate(IEE_OP_SET_P4D, p4dp, p4d);
+#else
 	WRITE_ONCE(*p4dp, p4d);
+#endif
 	dsb(ishst);
 	isb();
 }
@@ -738,6 +966,12 @@ static inline pud_t *p4d_pgtable(p4d_t p4d)
 /* Find an entry in the frst-level page table. */
 #define pud_offset_phys(dir, addr)	(p4d_page_paddr(READ_ONCE(*(dir))) + pud_index(addr) * sizeof(pud_t))
 
+#ifdef CONFIG_PTP
+#define pud_set_fixmap_init(addr)		((pud_t *)iee_set_fixmap_offset_pre_init(FIX_PUD, addr))
+#define pud_set_fixmap_offset_init(p4d, addr)	pud_set_fixmap_init(pud_offset_phys(p4d, addr))
+#define pud_clear_fixmap_init()		clear_fixmap_init(FIX_PUD)
+#endif
+
 #define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))
 #define pud_set_fixmap_offset(p4d, addr)	pud_set_fixmap(pud_offset_phys(p4d, addr))
 #define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
@@ -764,6 +998,10 @@ static inline pud_t *p4d_pgtable(p4d_t p4d)
 #define pgd_ERROR(e)	\
 	pr_err("%s:%d: bad pgd %016llx.\n", __FILE__, __LINE__, pgd_val(e))
 
+#ifdef CONFIG_PTP
+#define pgd_set_fixmap_init(addr) ((pgd_t *)iee_set_fixmap_offset_pre_init(FIX_PGD, addr))
+#define pgd_clear_fixmap_init() clear_fixmap_init(FIX_PGD)
+#endif
 #define pgd_set_fixmap(addr)	((pgd_t *)set_fixmap_offset(FIX_PGD, addr))
 #define pgd_clear_fixmap()	clear_fixmap(FIX_PGD)
 
@@ -825,8 +1063,13 @@ static inline int __ptep_test_and_clear_young(pte_t *ptep)
 	do {
 		old_pte = pte;
 		pte = pte_mkold(pte);
+		#ifdef CONFIG_PTP
+		pte_val(pte) = iee_set_cmpxchg_relaxed(ptep,
+					       pte_val(old_pte), pte_val(pte));
+		#else
 		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
 					       pte_val(old_pte), pte_val(pte));
+		#endif
 	} while (pte_val(pte) != pte_val(old_pte));
 
 	return pte_young(pte);
@@ -874,7 +1117,13 @@ static inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm,
 				       unsigned long address, pte_t *ptep)
 {
+	#ifdef CONFIG_PTP
+	pteval_t pteval= iee_set_xchg_relaxed(ptep, (pteval_t)0);
+	pte_t ret = __pte(pteval);
+	return ret;
+	#else
 	return __pte(xchg_relaxed(&pte_val(*ptep), 0));
+	#endif
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
@@ -899,8 +1148,12 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long addres
 	do {
 		old_pte = pte;
 		pte = pte_wrprotect(pte);
+		#ifdef CONFIG_PTP
+		pte_val(pte) = iee_set_cmpxchg_relaxed(ptep,pte_val(old_pte), pte_val(pte));
+		#else
 		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
 					       pte_val(old_pte), pte_val(pte));
+		#endif
 	} while (pte_val(pte) != pte_val(old_pte));
 }
 
@@ -916,7 +1169,11 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
 		unsigned long address, pmd_t *pmdp, pmd_t pmd)
 {
+	#ifdef CONFIG_PTP
+	return __pmd(iee_set_xchg_relaxed((pte_t *)pmdp, pmd_val(pmd)));
+	#else
 	return __pmd(xchg_relaxed(&pmd_val(*pmdp), pmd_val(pmd)));
+	#endif
 }
 #endif
 
diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 8bcc9ac9963e..401076e588f8 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -1784,6 +1784,62 @@
 		write_sysreg_s(__scs_new, sysreg);			\
 } while (0)
 
+
+#ifdef CONFIG_IEE
+
+extern void iee_rwx_gate_entry(int flag, ...);
+#define IEE_SI_TEST 0
+#define IEE_WRITE_sctlr_el1 1
+#define IEE_WRITE_ttbr0_el1 2
+#define IEE_WRITE_vbar_el1  3
+#define IEE_WRITE_tcr_el1   4
+#define IEE_WRITE_mdscr_el1   5
+#define IEE_WRITE_AFSR0   10
+
+#define sysreg_clear_set_iee_si(sysreg, clear, set) do {			\
+	u64 __scs_val = read_sysreg(sysreg);				\
+	u64 __scs_new = (__scs_val & ~(u64)(clear)) | (set);		\
+	if (__scs_new != __scs_val)					\
+		iee_rwx_gate_entry(IEE_WRITE_##sysreg, __scs_new);			\
+} while (0)
+
+#define IEE_SI_WRITE_DAIF_SEL  "msr daifclr, #0xf\n\t"	\
+	"tbnz %x0, #6, 114221f\n\t"    \
+    "tbnz %x0, #7, 114210f\n\t"                 \
+    "tbnz %x0, #8, 114100f\n\t"                 \
+    "msr daifset, #0b000\n\t"                   \
+    "b 114514f\n\t"                            \
+"114221:\n\t"                                  \
+    "tbnz %x0, #7, 114211f\n\t"     \
+    "tbnz %x0, #8, 114101f\n\t"     \
+    "msr daifset, #0b001\n\t"       \
+    "b 114514f\n\t"                \
+"114211:\n\t"              \
+    "tbnz %x0, #8, 114111f\n\t"     \
+    "msr daifset, #0b011\n\t"       \
+    "b 114514f\n\t"        \
+"114210:\n\t"              \
+    "tbnz %x0, #8, 114110f\n\t"     \
+    "msr daifset, #0b010\n\t"       \
+    "b 114514f\n\t"        \
+"114100:\n\t"              \
+    "msr daifset, #0b100\n\t"       \
+    "b 114514f\n\t"        \
+"114101:\n\t"              \
+    "msr daifset, #0b101\n\t"       \
+    "b 114514f\n\t"                \
+"114110:\n\t"              \
+    "msr daifset, #0b110\n\t"       \
+    "b 114514f\n\t"        \
+"114111:\n\t"              \
+    "msr daifset, #0b111\n\t"       \
+"114514:\n\t" 
+
+#define iee_si_write_daif(v) do {           \
+    u64 __val = (u64)(v);                   \
+    asm volatile(IEE_SI_WRITE_DAIF_SEL: : "rZ" (__val));} while (0)
+#endif
+
 #define read_sysreg_par() ({						\
 	u64 par;							\
 	asm(ALTERNATIVE("nop", "dmb sy", ARM64_WORKAROUND_1508412));	\
diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h
index c995d1f4594f..d6520a2ba83d 100644
--- a/arch/arm64/include/asm/tlb.h
+++ b/arch/arm64/include/asm/tlb.h
@@ -11,8 +11,17 @@
 #include <linux/pagemap.h>
 #include <linux/swap.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 static inline void __tlb_remove_table(void *_table)
 {
+#ifdef CONFIG_PTP
+    unsigned long iee_addr = __phys_to_iee(page_to_phys((struct page *)_table));
+    set_iee_page_invalid(iee_addr);
+    iee_set_logical_mem_rw((unsigned long)_table);
+#endif
 	free_page_and_swap_cache((struct page *)_table);
 }
 
diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h
index 4c28c6c4acba..34ad36df4eb6 100644
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@ -48,6 +48,7 @@
 
 #define __tlbi(op, ...)		__TLBI_N(op, ##__VA_ARGS__, 1, 0)
 
+
 #define __tlbi_user(op, arg) do {						\
 	if (arm64_kernel_unmapped_at_el0())					\
 		__tlbi(op, (arg) | USER_ASID_FLAG);				\
@@ -251,6 +252,10 @@ static inline void flush_tlb_mm(struct mm_struct *mm)
 	asid = __TLBI_VADDR(0, ASID(mm));
 	__tlbi(aside1is, asid);
 	__tlbi_user(aside1is, asid);
+	#if defined(CONFIG_IEE) || defined (CONFIG_KOI)
+        if (!arm64_kernel_unmapped_at_el0())	
+	        __tlbi(aside1is, asid | USER_ASID_FLAG);
+	#endif
 	dsb(ish);
 }
 
@@ -263,6 +268,10 @@ static inline void __flush_tlb_page_nosync(struct mm_struct *mm,
 	addr = __TLBI_VADDR(uaddr, ASID(mm));
 	__tlbi(vale1is, addr);
 	__tlbi_user(vale1is, addr);
+	#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+    	if (!arm64_kernel_unmapped_at_el0())	
+	        __tlbi(vale1is, addr | USER_ASID_FLAG);
+	#endif
 }
 
 static inline void flush_tlb_page_nosync(struct vm_area_struct *vma,
@@ -369,9 +378,17 @@ static inline void __flush_tlb_range(struct vm_area_struct *vma,
 			addr = __TLBI_VADDR(start, asid);
 			if (last_level) {
 				__tlbi_level(vale1is, addr, tlb_level);
+				#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+                    if (!arm64_kernel_unmapped_at_el0())
+				        __tlbi_level(vale1is, addr | USER_ASID_FLAG, tlb_level);
+				#endif
 				__tlbi_user_level(vale1is, addr, tlb_level);
 			} else {
 				__tlbi_level(vae1is, addr, tlb_level);
+				#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+                    if (!arm64_kernel_unmapped_at_el0())
+				        __tlbi_level(vae1is, addr | USER_ASID_FLAG, tlb_level);
+				#endif
 				__tlbi_user_level(vae1is, addr, tlb_level);
 			}
 			start += stride;
@@ -385,9 +402,17 @@ static inline void __flush_tlb_range(struct vm_area_struct *vma,
 						  num, tlb_level);
 			if (last_level) {
 				__tlbi(rvale1is, addr);
+				#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+				    if (!arm64_kernel_unmapped_at_el0())
+				        __tlbi(rvale1is, addr | USER_ASID_FLAG);
+				#endif
 				__tlbi_user(rvale1is, addr);
 			} else {
 				__tlbi(rvae1is, addr);
+				#if defined(CONFIG_IEE) || defined (CONFIG_KOI)
+				    if (!arm64_kernel_unmapped_at_el0())
+				        __tlbi(rvae1is, addr | USER_ASID_FLAG);
+				#endif
 				__tlbi_user(rvae1is, addr);
 			}
 			start += __TLBI_RANGE_PAGES(num, scale) << PAGE_SHIFT;
@@ -418,7 +443,7 @@ static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end
 		return;
 	}
 
-	start = __TLBI_VADDR(start, 0);
+ 	start = __TLBI_VADDR(start, 0);
 	end = __TLBI_VADDR(end, 0);
 
 	dsb(ishst);
@@ -434,9 +459,9 @@ static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end
  */
 static inline void __flush_tlb_kernel_pgtable(unsigned long kaddr)
 {
-	unsigned long addr = __TLBI_VADDR(kaddr, 0);
-
-	dsb(ishst);
+ 	unsigned long addr = __TLBI_VADDR(kaddr, 0);
+	
+    dsb(ishst);
 	__tlbi(vaae1is, addr);
 	dsb(ish);
 	isb();
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index 312c164db2ed..bbeb4975d571 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -30,6 +30,8 @@ OBJCOPYFLAGS := --prefix-symbols=__efistub_
 $(obj)/%.stub.o: $(obj)/%.o FORCE
 	$(call if_changed,objcopy)
 
+obj-y 							+= iee/
+obj-$(CONFIG_KOI) 				+= koi/
 obj-$(CONFIG_AARCH32_EL0)			+= binfmt_elf32.o sys32.o signal32.o			\
 					   sys_compat.o
 obj-$(CONFIG_AARCH32_EL0)			+= sigreturn32.o
diff --git a/arch/arm64/kernel/armv8_deprecated.c b/arch/arm64/kernel/armv8_deprecated.c
index 637e57a4868e..f69775f3a8b0 100644
--- a/arch/arm64/kernel/armv8_deprecated.c
+++ b/arch/arm64/kernel/armv8_deprecated.c
@@ -312,11 +312,19 @@ static int cp15barrier_handler(struct pt_regs *regs, u32 instr)
 
 static int cp15_barrier_set_hw_mode(bool enable)
 {
+#ifdef CONFIG_IEE
+    if (enable)
+		sysreg_clear_set_iee_si(sctlr_el1, 0, SCTLR_EL1_CP15BEN);
+	else
+		sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_CP15BEN, 0);
+	return 0;
+#else
 	if (enable)
 		sysreg_clear_set(sctlr_el1, 0, SCTLR_EL1_CP15BEN);
 	else
 		sysreg_clear_set(sctlr_el1, SCTLR_EL1_CP15BEN, 0);
 	return 0;
+#endif
 }
 
 static bool try_emulate_cp15_barrier(struct pt_regs *regs, u32 insn)
@@ -347,11 +355,19 @@ static int setend_set_hw_mode(bool enable)
 	if (!cpu_supports_mixed_endian_el0())
 		return -EINVAL;
 
+#ifdef CONFIG_IEE
+    if (enable)
+		sysreg_clear_set_iee_si(sctlr_el1, 0, SCTLR_EL1_CP15BEN);
+	else
+		sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_CP15BEN, 0);
+	return 0;
+#else
 	if (enable)
 		sysreg_clear_set(sctlr_el1, SCTLR_EL1_SED, 0);
 	else
 		sysreg_clear_set(sctlr_el1, 0, SCTLR_EL1_SED);
 	return 0;
+#endif
 }
 
 static int __a32_setend_handler(struct pt_regs *regs, u32 big_endian)
diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
index c247e11130db..67721ae9a4a4 100644
--- a/arch/arm64/kernel/asm-offsets.c
+++ b/arch/arm64/kernel/asm-offsets.c
@@ -75,6 +75,17 @@ int main(void)
   DEFINE(S_STACKFRAME,		offsetof(struct pt_regs, stackframe));
   DEFINE(S_ORIG_X0,		offsetof(struct pt_regs, orig_x0));
   DEFINE(S_FRAME_SIZE,		sizeof(struct pt_regs));
+#ifdef CONFIG_IEE
+  DEFINE(iee_from_token_offset,		offsetof(struct task_token, iee_stack));
+  DEFINE(kernel_from_token_offset,		offsetof(struct task_token, kernel_stack));
+  DEFINE(mm_from_task_offset,		offsetof(struct task_struct, mm));
+#endif
+#ifdef CONFIG_KOI
+  DEFINE(koi_kernel_from_token_offset,		offsetof(struct task_token, koi_kernel_stack));
+  DEFINE(koi_from_token_offset,  offsetof(struct task_token, koi_stack));
+  DEFINE(ttbr1_from_token_offset, offsetof(struct task_token, current_ttbr1));
+  DEFINE(koi_stack_base_from_token_offset, offsetof(struct task_token, koi_stack_base));
+#endif
   BLANK();
 #ifdef CONFIG_AARCH32_EL0
   DEFINE(COMPAT_SIGFRAME_REGS_OFFSET,		offsetof(struct a32_sigframe, uc.uc_mcontext.arm_r0));
diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 7f175b3aac15..2cd625a17ecb 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -81,7 +81,11 @@ hisilicon_1980005_enable(const struct arm64_cpu_capabilities *__unused)
 	cpus_set_cap(ARM64_HAS_CACHE_IDC);
 	arm64_ftr_reg_ctrel0.sys_val |= BIT(CTR_IDC_SHIFT);
 	arm64_ftr_reg_ctrel0.strict_mask &= ~BIT(CTR_IDC_SHIFT);
+#ifdef CONFIG_IEE
+    sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_UCT, 0);
+#else
 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
+#endif
 }
 #endif
 
@@ -133,7 +137,11 @@ cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *cap)
 		enable_uct_trap = true;
 
 	if (enable_uct_trap)
+#ifdef CONFIG_IEE
+        sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_UCT, 0);
+#else
 		sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
+#endif
 }
 
 #ifdef CONFIG_ARM64_ERRATUM_1463225
@@ -150,7 +158,11 @@ has_cortex_a76_erratum_1463225(const struct arm64_cpu_capabilities *entry,
 static void __maybe_unused
 cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 {
+#ifdef CONFIG_IEE
+    sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_UCI, 0);
+#else
 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCI, 0);
+#endif
 }
 
 #ifdef CONFIG_HISILICON_ERRATUM_HIP08_RU_PREFETCH
diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
index 9a4193991be5..b01bed575e3f 100644
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@ -1375,7 +1375,11 @@ static void cpu_emulate_effective_ctr(const struct arm64_cpu_capabilities *__unu
 	 * value.
 	 */
 	if (!(read_cpuid_cachetype() & BIT(CTR_IDC_SHIFT)))
+#ifdef CONFIG_IEE
+        sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_UCT, 0);
+#else
 		sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
+#endif
 }
 
 static bool has_cache_dic(const struct arm64_cpu_capabilities *entry,
@@ -1583,7 +1587,11 @@ static inline void __cpu_enable_hw_dbm(void)
 {
 	u64 tcr = read_sysreg(tcr_el1) | TCR_HD;
 
+#ifdef CONFIG_IEE
+    iee_rwx_gate_entry(IEE_WRITE_tcr_el1, tcr);
+#else
 	write_sysreg(tcr, tcr_el1);
+#endif
 	isb();
 	local_flush_tlb_all();
 }
@@ -1963,7 +1971,9 @@ static void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	#ifndef CONFIG_IEE
 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_SPAN, 0);
+	#endif
 	set_pstate_pan(1);
 }
 #endif /* CONFIG_ARM64_PAN */
@@ -2045,7 +2055,11 @@ static bool has_generic_auth(const struct arm64_cpu_capabilities *entry,
 static void cpu_enable_e0pd(struct arm64_cpu_capabilities const *cap)
 {
 	if (this_cpu_has_cap(ARM64_HAS_E0PD))
+#ifdef CONFIG_IEE
+        sysreg_clear_set_iee_si(tcr_el1, 0, TCR_E0PD1);
+#else
 		sysreg_clear_set(tcr_el1, 0, TCR_E0PD1);
+#endif
 }
 #endif /* CONFIG_ARM64_E0PD */
 
@@ -2075,7 +2089,11 @@ static void bti_enable(const struct arm64_cpu_capabilities *__unused)
 	 * So, be strict and forbid other BRs using other registers to
 	 * jump onto a PACIxSP instruction:
 	 */
+#ifdef CONFIG_IEE
+    sysreg_clear_set_iee_si(sctlr_el1, 0, SCTLR_EL1_BT0 | SCTLR_EL1_BT1);
+#else
 	sysreg_clear_set(sctlr_el1, 0, SCTLR_EL1_BT0 | SCTLR_EL1_BT1);
+#endif
 	isb();
 }
 #endif /* CONFIG_ARM64_BTI */
diff --git a/arch/arm64/kernel/debug-monitors.c b/arch/arm64/kernel/debug-monitors.c
index 9e0816d04370..76fe9e121c5c 100644
--- a/arch/arm64/kernel/debug-monitors.c
+++ b/arch/arm64/kernel/debug-monitors.c
@@ -36,10 +36,14 @@ u8 debug_monitors_arch(void)
  */
 static void mdscr_write(u32 mdscr)
 {
+// #ifdef CONFIG_IEE
+//     iee_rwx_gate_entry(IEE_WRITE_mdscr_el1, mdscr);
+// #else
 	unsigned long flags;
 	flags = local_daif_save();
 	write_sysreg(mdscr, mdscr_el1);
 	local_daif_restore(flags);
+// #endif
 }
 NOKPROBE_SYMBOL(mdscr_write);
 
diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
index ab8ed1b62da1..7b45f9719359 100644
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@ -28,6 +28,9 @@
 #include <asm/thread_info.h>
 #include <asm/asm-uaccess.h>
 #include <asm/unistd.h>
+#ifdef CONFIG_IEE
+#include <asm/iee-def.h>
+#endif
 
 /*
  * Context tracking and irqflag tracing need to instrument transitions between
@@ -59,10 +62,383 @@
 #define BAD_IRQ		1
 #define BAD_FIQ		2
 #define BAD_ERROR	3
+#define BAD_IEE		4
+
+
+#ifdef CONFIG_KOI
+#ifdef CONFIG_IEE
+/*
+ * This function is used to switch to ko stack in glue code
+ */
+SYM_FUNC_START(koi_do_switch_to_ko_stack)
+    sub sp, sp, #48
+    stp x29, x30, [sp]
+    str x2, [sp, #16]
+    stp x0, x1, [sp, #32]
+    
+    // iee_rw_gate(IEE_WRITE_KERNEL_STACK, current, sp)
+    mov x0, #IEE_WRITE_KOI_KERNEL_STACK
+    mrs x1, sp_el0
+    add x2, sp, #48
+
+    bl iee_rw_gate
+
+    // iee_rw_gate(IEE_READ_KOI_STACK, current)
+    mov x0, #IEE_READ_KOI_STACK
+    mrs x1, sp_el0
+    bl iee_rw_gate
+
+    ldp x29, x30, [sp]
+    ldr x2, [sp, #16]
+    add x1, sp, #32
+    mov sp, x0
+    ldp x0, x1, [x1]
+
+    isb
+    ret
+SYM_FUNC_END(koi_do_switch_to_ko_stack)
+
+/*
+ * This fucntion is used to switch to kernel stack in glue code
+ */
+SYM_FUNC_START(koi_do_switch_to_kernel_stack)
+    sub sp, sp, #48
+    stp x29, x30, [sp]
+    str x2, [sp, #16]
+    stp x0, x1, [sp, #32]
+    // iee_rw_gate(IEE_WRITE_KOI_STACK, current, sp)
+    mov x0, #IEE_WRITE_KOI_STACK
+    mrs x1, sp_el0
+    add x2, sp, #48
+    bl iee_rw_gate
+
+    // iee_rw_gate(IEE_READ_KOI_KERNEL_STACK, current)
+    mov x0, #IEE_READ_KOI_KERNEL_STACK
+    mrs x1, sp_el0
+    bl iee_rw_gate
+
+    ldp x29, x30, [sp]
+    ldr x2, [sp, #16]
+    add x1, sp, #32
+	mov sp, x0
+    ldp x0, x1, [x1]
+    isb
+    ret
+SYM_FUNC_END(koi_do_switch_to_kernel_stack)
+
+/*
+ * Before switch to ko's pgtable, we must switch current stack to ko's stack.
+ * We have stored registers to kernel stack, and we need to restore them from ko's stack after switching,
+ * so we need to copy from kernel stack to ko stack
+ * the memory region to copy is [sp, stack_top)
+ * void koi_switch_to_ko_stack(void);
+ */
+SYM_FUNC_START(koi_switch_to_ko_stack)
+    mrs x17, pan
+    msr pan, 0x0
+
+    sub sp, sp, #32
+    str x17, [sp, #16]
+    stp x30, x29, [sp]
+    
+    // current sp stores in x1
+	add x1, x1, #176
+    // current sp_el0 stores in x0
+    bl _iee_write_koi_kernel_stack
+    
+    mrs x0, sp_el0
+    bl _iee_read_koi_stack
+
+    ldr x17, [sp, #16]
+    ldp x30, x29, [sp]
+    add sp, sp, #32
+
+    msr pan, x17
+
+	sub x0, x0, #176
+    mov x1, sp
+    mov x2, #176    
+
+    // memcpy(current->driver_stack, current->kernel_stack, 176)
+	mov x16, lr
+	bl memcpy
+	mov lr, x16
+
+	mov sp, x0
+    isb
+	ret
+SYM_FUNC_END(koi_switch_to_ko_stack)
+
+SYM_FUNC_START(koi_switch_to_kernel_stack)
+	/* 
+     * current sp belongs to driver stack, and the bottom 160 bytes saves registers when exception occurred, 
+     * so we should add 160 to current sp, and store it in task_struct
+     * also, fetch kernel sp from task_struct, copy the bottom 160 bytes from driver stack to kernel stack
+     */
+    mrs x17, pan
+    msr pan, 0x0
+
+    sub sp, sp, #32
+    stp x30, x29, [sp]
+    str x17, [sp, #16]
+
+    mrs x0, sp_el0
+    add x1, sp, #192
+    bl _iee_write_koi_stack
+
+    mrs x0, sp_el0
+    bl _iee_read_koi_kernel_stack
+
+    ldr x17, [sp, #16]
+    ldp x30, x29, [sp]
+    add sp, sp, #32
+    
+    msr pan, x17
+	
+	// x0 = kernel_stack
+	sub x0, x0, #160
+    mov x1, sp
+	// x2 = 160
+	mov x2, #160
+
+	mov x16, lr
+	bl memcpy
+	mov lr, x16
+
+	mov sp, x0
+    isb
+	ret
+SYM_FUNC_END(koi_switch_to_kernel_stack)
+#else 
+/*
+ * This function is used to switch to ko stack in glue code
+ */
+SYM_FUNC_START(koi_do_switch_to_ko_stack)
+    sub sp, sp, #16
+    stp x16, x17, [sp]
+    mrs x17, sp_el0
+    adrp x16, koi_offset
+    ldr x16, [x16, #:lo12:koi_offset]
+    add x17, x17, x16
+    add x16, sp, #16
+    str x16, [x17, #koi_kernel_from_token_offset]
+    ldr x16, [x17, #koi_from_token_offset]
+    mov x17, sp
+    mov sp, x16
+    ldp x16, x17, [x17]
+    isb
+    ret
+SYM_FUNC_END(koi_do_switch_to_ko_stack)
+
+/*
+ * This fucntion is used to switch to kernel stack in glue code
+ */
+SYM_FUNC_START(koi_do_switch_to_kernel_stack)
+    sub sp, sp, #16
+    stp x16, x17, [sp]
+    mrs x17, sp_el0
+    adrp x16, koi_offset
+    ldr x16, [x16, #:lo12:koi_offset]
+    add x17, x17, x16
+    add x16, sp, #16
+    str x16, [x17, #koi_from_token_offset]
+    ldr x16, [x17, #koi_kernel_from_token_offset]
+    mov x17, sp
+    mov sp, x16
+    ldp x16, x17, [x17]
+    isb
+    ret
+SYM_FUNC_END(koi_do_switch_to_kernel_stack)
+
+/*
+ * Before switch to ko's pgtable, we must switch current stack to ko's stack.
+ * We have stored registers to kernel stack, and we need to restore them from ko's stack after switching,
+ * so we need to copy from kernel stack to ko stack
+ * the memory region to copy is [sp, stack_top)
+ * void koi_switch_to_ko_stack(unsigned long stack_top);
+ */
+SYM_FUNC_START(koi_switch_to_ko_stack)
+    // current sp stores in x1
+	add x3, x1, #176
+    adrp x4, koi_offset
+    ldr x4, [x4, #:lo12:koi_offset]
+    add x4, x0, x4
+    // current sp_el0 stores in x0
+	str x3, [x4, #koi_kernel_from_token_offset]
+    ldr x0, [x4, #koi_from_token_offset]
+	sub x0, x0, #176
+    mov x2, #176    
+
+    // memcpy(current->driver_stack, current->kernel_stack, 176)
+	mov x16, lr
+	bl memcpy
+	mov lr, x16
+
+	mov sp, x0
+    isb
+	ret
+SYM_FUNC_END(koi_switch_to_ko_stack)
+
+SYM_FUNC_START(koi_switch_to_kernel_stack)
+	/* 
+     * current sp belongs to driver stack, and the bottom 176 bytes saves registers when exception occurred, 
+     * so we should add 176 to current sp, and store it in task_struct
+     * also, fetch kernel sp from task_struct, copy the bottom 176 bytes from driver stack to kernel stack
+     */
+	mov x1, sp
+	add x3, sp, #160
+	
+	mrs x16, sp_el0
+    adrp x2, koi_offset
+    ldr x2, [x2, #:lo12:koi_offset]
+    add x16, x16, x2
+	str x3, [x16, #koi_from_token_offset]
+	//  sp points to kernel_stack
+	ldr x0, [x16, #koi_kernel_from_token_offset]
+	
+	// x0 = kernel_stack
+	sub x0, x0, #160
+	// x2 = 160
+	mov x2, #160
+	mov x16, lr
+	// memcpy(kernel_stack, driver_stack, 160)
+	bl memcpy
+	mov lr, x16
+	mov sp, x0
+    isb
+	ret
+SYM_FUNC_END(koi_switch_to_kernel_stack)
+#endif
+
+SYM_FUNC_START(koi_switch_to_ko_pgtbl)
+	stp x0, x1, [sp, #16 * 1]
+	stp x2, x3, [sp, #16 * 2]
+	stp x4, x5, [sp, #16 * 3]
+	stp x6, x7, [sp, #16 * 4]
+	stp x8, x9, [sp, #16 * 5]
+	stp x10, x11, [sp, #16 * 6]
+	stp x12, x13, [sp, #16 * 7]
+	stp x14, x15, [sp, #16 * 8]
+	stp x16, x17, [sp, #16 * 9]
+	stp x18, x30, [sp, #16 * 10]
+
+	adrp x0, koi_swapper_ttbr1
+	ldr x0, [x0, #:lo12:koi_swapper_ttbr1]
+	cbz x0, 0f
+	bl koi_do_switch_to_ko_pgtbl
+    // if x0 == 0, don't need to switch pgtable and stack, jump to 0
+    cbz x0, 0f
+    mov x19, x0
+    // if current on task's kernel stack switch to ko stack
+    mrs x0, sp_el0
+    mov x1, sp
+    ldr x2, [x0, TSK_STACK]
+    eor x2, x2, x1
+    and x2, x2, #~(THREAD_SIZE - 1)
+    cbnz x2, 1f
+
+    bl koi_switch_to_ko_stack
+1:
+#ifndef CONFIG_IEE
+    msr ttbr1_el1, x19
+    isb
+    nop
+    nop
+    nop
+#else
+    mov x0, #IEE_SWITCH_TO_KOI
+    mov x1, x19
+    bl  iee_rwx_gate_entry
+#endif
+0:
+	
+	ldp x0, x1, [sp, #16 * 1]
+	ldp x2, x3, [sp, #16 * 2]
+	ldp x4, x5, [sp, #16 * 3]
+	ldp x6, x7, [sp, #16 * 4]
+	ldp x8, x9, [sp, #16 * 5]
+	ldp x10, x11, [sp, #16 * 6]
+	ldp x12, x13, [sp, #16 * 7]
+	ldp x14, x15, [sp, #16 * 8]
+	ldp x16, x17, [sp, #16 * 9]
+	ldp x18, x30, [sp, #16 * 10]
+	ret
+SYM_FUNC_END(koi_switch_to_ko_pgtbl)
+
+.pushsection ".koi.text", "ax"
+SYM_FUNC_START(koi_switch_to_kernel_pgtbl)
+    sub sp, sp, #160
+    stp x0, x1, [sp, #16 * 0]
+	stp x2, x3, [sp, #16 * 1]
+	stp x4, x5, [sp, #16 * 2]
+	stp x6, x7, [sp, #16 * 3]
+	stp x8, x9, [sp, #16 * 4]
+	stp x10, x11, [sp, #16 * 5]
+
+	stp x12, x13, [sp, #16 * 6]
+	stp x14, x15, [sp, #16 * 7]
+	stp x16, x17, [sp, #16 * 8]
+	stp x18, x30, [sp, #16 * 9]
+    // check whether paging init finished
+	adrp x0, koi_swapper_ttbr1
+	ldr x0, [x0, #:lo12:koi_swapper_ttbr1]
+	cbz x0, 0f
+
+	bl koi_do_switch_to_kernel_pgtbl
+	/*
+     * koi_do_switch_to_kernel_pgtbl return 0 indicates
+     * that when exception occurred, the isolated ko is executing under koi pgtbl, 
+     * so we need to switch stack to kernel stack after switch pgtbl back to koi_swapper_ttbr1. 
+	 */
+	cbz x0, 0f 
+#ifndef CONFIG_IEE
+    mrs x0, sp_el0
+    adrp x1, koi_offset
+    ldr x1, [x1, #:lo12:koi_offset]
+    add x0, x0, x1
+    mov x16, sp
+    ldr x17, [x0, koi_stack_base_from_token_offset]
+    eor x17, x17, x16
+    and x17, x17, #~(THREAD_SIZE - 1)
+    cbnz x17, 0f
+#else
+    // save current pan
+    mrs x17, pan
+    // disable pan
+    msr pan, 0x0
+    mrs x0, sp_el0
+    bl _iee_read_koi_stack_base
+    // restore pan
+    msr pan, x17
+
+    mov x16, sp
+    eor x0, x0, x16
+    and x0, x0, #~(THREAD_SIZE - 1)
+    cbnz x0, 0f
+#endif
+	bl koi_switch_to_kernel_stack
+0:
+	
+	ldp x0, x1, [sp, #16 * 0]
+	ldp x2, x3, [sp, #16 * 1]
+	ldp x4, x5, [sp, #16 * 2]
+	ldp x6, x7, [sp, #16 * 3]
+	ldp x8, x9, [sp, #16 * 4]
+	ldp x10, x11, [sp, #16 * 5]
+	ldp x12, x13, [sp, #16 * 6]
+	ldp x14, x15, [sp, #16 * 7]
+	ldp x16, x17, [sp, #16 * 8]
+	ldp x18, x30, [sp, #16 * 9]
+    add sp, sp, #160
+	ret
+SYM_FUNC_END(koi_switch_to_kernel_pgtbl)
+.popsection
+#endif
 
 	.macro kernel_ventry, el, label, regsize = 64
 	.align 7
 .Lventry_start\@:
+
 	.if	\el == 0
 	/*
 	 * This must be the first instruction of the EL0 vector entries. It is
@@ -78,6 +454,15 @@
 .Lskip_tramp_vectors_cleanup\@:
 	.endif
 
+#ifdef CONFIG_KOI
+	.if \el == 1
+    msr tpidrro_el0, x30
+	bl koi_switch_to_kernel_pgtbl
+    mrs x30, tpidrro_el0
+    msr	tpidrro_el0, xzr
+	.endif
+#endif
+
 	sub	sp, sp, #S_FRAME_SIZE
 #ifdef CONFIG_VMAP_STACK
 	/*
@@ -182,6 +567,17 @@ alternative_else_nop_endif
 #endif
 	.endm
 
+#ifdef CONFIG_IEE
+// SP_EL0 check failed.
+SYM_FUNC_START_LOCAL(sp_el0_check_failed)
+	mov	x0, sp
+	mov	x1, #BAD_IEE
+	mrs	x2, esr_el1
+	bl	bad_mode
+	ASM_BUG()
+SYM_FUNC_END(sp_el0_check_failed)
+#endif
+
 	.macro	kernel_entry, el, regsize = 64
 	.if	\regsize == 32
 	mov	w0, w0				// zero upper 32 bits of x0
@@ -207,6 +603,13 @@ alternative_else_nop_endif
 	mrs	x21, sp_el0
 	ldr_this_cpu	tsk, __entry_task, x20
 	msr	sp_el0, tsk
+#ifdef CONFIG_IEE
+	// tsk check.
+	ldr_this_cpu x19, __entry_task, x20
+	mrs x20, sp_el0
+	cmp x19, x20
+	b.ne sp_el0_check_failed
+#endif
 
 	/*
 	 * Ensure MDSCR_EL1.SS is clear, since we can unmask debug exceptions
@@ -223,6 +626,13 @@ alternative_else_nop_endif
 
 	scs_load_current
 	.else
+#ifdef CONFIG_IEE
+	// tsk check.
+	ldr_this_cpu x19, __entry_task, x20
+	mrs x20, sp_el0
+	cmp x19, x20
+	b.ne sp_el0_check_failed
+#endif
 	add	x21, sp, #S_FRAME_SIZE
 	get_current_task tsk
 	.endif /* \el == 0 */
@@ -271,6 +681,17 @@ alternative_else_nop_endif
 alternative_if ARM64_MTE
 	SET_PSTATE_TCO(0)
 alternative_else_nop_endif
+#endif
+
+#ifndef CONFIG_IEE
+#ifdef CONFIG_KOI
+	// set tcr_el1 to choose asid from ttbr1_el1 or ttbr0_el1
+	.if	\el == 0
+	mrs x0, tcr_el1
+	orr x0, x0 ,#0x0000000000400000 
+	msr tcr_el1,x0
+	.endif
+#endif
 #endif
 
 	/*
@@ -284,9 +705,11 @@ alternative_else_nop_endif
 	.endm
 
 	.macro	kernel_exit, el
+	#ifndef CONFIG_IEE
 	.if	\el != 0
 	disable_daif
 	.endif
+	#endif
 
 #ifdef CONFIG_ARM64_PSEUDO_NMI
 	/* Restore pmr */
@@ -335,6 +758,50 @@ alternative_else_nop_endif
 
 	msr	elr_el1, x21			// set up the return data
 	msr	spsr_el1, x22
+
+#ifdef CONFIG_IEE
+
+	.if	\el == 0
+
+	#ifndef CONFIG_UNMAP_KERNEL_AT_EL0
+	// SET hpd1 = 0 start
+	mrs x0, tcr_el1
+	and x0, x0, #0xFFFFFBFFFFFFFFFF
+	and x0, x0, #0xFFFFFFFFFFBFFFFF
+	msr tcr_el1, x0
+	// SET hpd1 = 0 end
+
+	disable_daif
+
+	// Check ELR_EL1
+	mrs x0, elr_el1
+	lsr x0, x0, #48
+	tst x0, #0xffff
+	b.ne 5f
+	#endif
+
+	// write dbg ctrl registers for breakpoint 0 to serve user ptrace.
+    //mrs x5, afsr0_el1
+    //cbz x5, 1145f           // skip if not used by user
+    //ldr_this_cpu x1, iee_si_user_bvr0, x2
+    //ldr_this_cpu x3, iee_si_user_bcr0, x4
+    //msr dbgbvr0_el1, x1
+    //msr dbgbcr0_el1, x3
+1145:
+
+	.endif
+
+#else
+#ifdef CONFIG_KOI
+	.if \el==0
+	mrs x0, tcr_el1
+	and x0, x0, #0xFFFFFFFFFFBFFFFF
+	msr tcr_el1,x0
+	.endif
+#endif
+
+#endif
+
 	ldp	x0, x1, [sp, #16 * 0]
 	ldp	x2, x3, [sp, #16 * 1]
 	ldp	x4, x5, [sp, #16 * 2]
@@ -366,13 +833,30 @@ alternative_else_nop_endif
 	tramp_alias	x30, tramp_exit_compat, x29
 	br	x30
 #endif
+
+#ifdef CONFIG_IEE
+5:
+	// ELR_EL1 check fail
+	mov	x0, sp
+	mov	x1, #BAD_IEE
+	mrs	x2, esr_el1
+	bl	bad_mode
+	ASM_BUG()
+#endif
+
 	.else
 	ldr	lr, [sp, #S_LR]
 	add	sp, sp, #S_FRAME_SIZE		// restore sp
 
 	/* Ensure any device/NC reads complete */
 	alternative_insn nop, "dmb sy", ARM64_WORKAROUND_1508412
-
+#ifdef CONFIG_KOI
+    sub sp, sp, #176
+	stp x30, x19, [sp, #16 * 0]
+	bl koi_switch_to_ko_pgtbl
+	ldp	x30, x19, [sp, #16 * 0]
+	add sp, sp, #176
+#endif
 	eret
 	.endif
 	sb
@@ -651,21 +1135,213 @@ SYM_CODE_START_LOCAL(el1_error_invalid)
 	inv_entry 1, BAD_ERROR
 SYM_CODE_END(el1_error_invalid)
 
+/*
+ * iee exception entry 
+ */
+	.macro	iee_exception_entry, el
+
+    /* skip setting breakpoint 0 if user hasn't change it. */
+    //.if \el == 0
+    //mrs x0, afsr0_el1
+    //cbz x0, 1148f
+    //.endif
+
+	//b 1147f
+    /* enable breakpoint 0 to proctect iee rwx gate */
+    //adrp x1, iee_rwx_gate_entry
+	//add x1, x1, #:lo12:iee_rwx_gate_entry
+#ifdef CONFIG_KOI
+	//add x1, x1, #120
+#else
+    //add x1, x1, #36
+#endif
+1145:
+    //msr dbgbvr0_el1, x1
+    //mov x2, 0x21e7
+1146:
+    //msr dbgbcr0_el1, x2
+	//isb
+
+1147:
+    /* check val of dbg ctrl registers */
+    //mrs x3, dbgbvr0_el1
+    //adrp x1, iee_rwx_gate_entry
+	//add x1, x1, #:lo12:iee_rwx_gate_entry
+#ifdef CONFIG_KOI
+	//add x1, x1, #120
+#else
+    //add x1, x1, #36
+#endif
+    //cmp x1, x3
+    //bne 1145b
+    //mov x2, 0x21e7
+    //mrs x0, dbgbcr0_el1
+    //cmp x2, x0
+    //bne 1146b
+    //enable_dbg
+1148:
+
+	/* el0 set hpds */
+	.if	\el == 0
+
+	#ifndef CONFIG_UNMAP_KERNEL_AT_EL0
+	/* SET hpd1 = 1 start */
+	mrs x0, tcr_el1
+	orr x0, x0, #0x0000040000000000
+	orr x0, x0, #0x0000000000400000
+	msr tcr_el1, x0
+	/* SET hpd1 = 1 end */
+
+	disable_daif
+
+	/* Check TCR_EL1 */
+	mrs x0, tcr_el1
+	tst x0, #0x0000040000000000
+	b.eq 5f
+	tst x0, #0x0000000000400000
+	b.ne 6f
+
+5:
+	/* TCR_EL1 check fail */
+	mov	x0, sp
+	mov	x1, #BAD_IEE
+	mrs	x2, esr_el1
+	bl	bad_mode
+	ASM_BUG()
+
+6:
+	nop
+	#endif
+
+	.else
+#ifdef CONFIG_IEE_INTERRUPTABLE
+	/* el1 save elr_el1 and set pan */
+	/* Check ELR_EL1 */
+	ldr x1, =__iee_code_start
+	cmp x1, x22
+	b.hi 7f
+	ldr x1, =__iee_code_end
+	cmp x1, x22
+	b.lo 7f
+	/* Exception from iee code */
+	/* Switch to kernel stack */
+	mrs x0, sp_el0 /* x0 -> task_struct(VA) */
+	adrp x2, iee_offset
+	ldr x2, [x2, #:lo12:iee_offset]
+	add x1, x0, x2 /* x1 -> task_token(IEE) */
+	// store iee stack
+	mov x3, sp
+	str x3, [x1, #iee_from_token_offset]
+	// load kernel stack
+	ldr x3, [x1, #kernel_from_token_offset]
+	mov sp, x3
+	sub	sp, sp, #S_FRAME_SIZE
+	/* Enable PAN */
+	msr pan, #0x1
+
+7:
+	/* Exception from kernel code */
+	mov x0, #0x0
+	mov x1, #0x0
+	mov x2, #0x0
+	mov x3, #0x0
+#endif
+	.endif
+	.endm
+
+/*
+ * iee exception exit
+ */
+	.macro	iee_exception_exit, el
+	// Disable daif
+	disable_daif
+	
+	.if	\el == 1
+#ifdef CONFIG_IEE_INTERRUPTABLE
+	/* el1 pop elr_el1 and set pan */
+	/* Check ELR_EL1 */
+	ldr x1, =__iee_code_start
+	cmp x1, x22
+	b.hi 9f
+	ldr x1, =__iee_code_end
+	cmp x1, x22
+	b.lo 9f
+	/* Eret iee code */
+	/* Disable PAN */
+	msr pan, #0x0
+	/* Switch to iee stack */
+	add	sp, sp, #S_FRAME_SIZE
+	mrs x0, sp_el0 /* x0 -> task_struct */
+	adrp x2, iee_offset
+	ldr x2, [x2, #:lo12:iee_offset]
+	add x1, x0, x2 /* x1 -> task_token(IEE) */
+	// store kernel stack
+	mov x3, sp
+	str x3, [x1, #kernel_from_token_offset]
+	// load iee stack
+	ldr x2, [x1, #iee_from_token_offset]
+	mov sp, x2
+	/* Load ELR_EL1 from iee stack */
+	ldr	x21, [sp, #S_PC]
+	/* Check the modify of ELR_EL1 */
+	cmp x21, x22
+	b.ne 8f
+	/* ELR_EL1 not modified */
+	b 9f
+
+8:
+	// ELR_EL1 modified
+	mov	x0, sp
+	mov	x1, #BAD_IEE
+	mrs	x2, esr_el1
+	bl	bad_mode
+	ASM_BUG()
+
+9:
+	// Eret kernel code
+	mov x0, #0x0
+	mov x1, #0x0
+	mov x2, #0x0
+	mov x3, #0x0
+#endif
+	.endif
+	.endm
+
 /*
  * EL1 mode handlers.
  */
 	.align	6
 SYM_CODE_START_LOCAL_NOALIGN(el1_sync)
 	kernel_entry 1
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 1
+	#endif
+
 	mov	x0, sp
 	bl	el1_sync_handler
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 1
+	#endif
+
 	kernel_exit 1
 SYM_CODE_END(el1_sync)
 
 	.align	6
 SYM_CODE_START_LOCAL_NOALIGN(el1_irq)
 	kernel_entry 1
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 1
+	#endif
+
 	el1_interrupt_handler handle_arch_irq
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit  1
+	#endif
+
 	kernel_exit 1
 SYM_CODE_END(el1_irq)
 
@@ -675,8 +1351,18 @@ SYM_CODE_END(el1_irq)
 	.align	6
 SYM_CODE_START_LOCAL_NOALIGN(el0_sync)
 	kernel_entry 0
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 0
+	#endif
+
 	mov	x0, sp
 	bl	el0_sync_handler
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 0
+	#endif
+
 	b	ret_to_user
 SYM_CODE_END(el0_sync)
 
@@ -684,8 +1370,18 @@ SYM_CODE_END(el0_sync)
 	.align	6
 SYM_CODE_START_LOCAL_NOALIGN(el0_sync_compat)
 	kernel_entry 0, 32
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 0
+	#endif
+
 	mov	x0, sp
 	bl	el0_sync_compat_handler
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 0
+	#endif
+
 	b	ret_to_user
 SYM_CODE_END(el0_sync_compat)
 
@@ -705,22 +1401,47 @@ SYM_CODE_END(el0_error_compat)
 SYM_CODE_START_LOCAL_NOALIGN(el0_irq)
 	kernel_entry 0
 el0_irq_naked:
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 0
+	#endif
+
 	el0_interrupt_handler handle_arch_irq
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 0
+	#endif
+
 	b	ret_to_user
 SYM_CODE_END(el0_irq)
 
 SYM_CODE_START_LOCAL(el1_error)
 	kernel_entry 1
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 1
+	#endif
+
 	mrs	x1, esr_el1
 	enable_dbg
 	mov	x0, sp
 	bl	do_serror
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 1
+	#endif
+
 	kernel_exit 1
 SYM_CODE_END(el1_error)
 
 SYM_CODE_START_LOCAL(el0_error)
 	kernel_entry 0
 el0_error_naked:
+
+	#ifdef CONFIG_IEE
+	iee_exception_entry 0
+	#endif
+
 	mrs	x25, esr_el1
 	user_exit_irqoff
 	enable_dbg
@@ -728,6 +1449,11 @@ el0_error_naked:
 	mov	x1, x25
 	bl	do_serror
 	enable_da_f
+
+	#ifdef CONFIG_IEE
+	iee_exception_exit 0
+	#endif
+	
 	b	ret_to_user
 SYM_CODE_END(el0_error)
 
@@ -751,6 +1477,8 @@ finish_ret_to_user:
 #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
 	bl	stackleak_erase
 #endif
+
+
 	kernel_exit 0
 
 /*
@@ -1013,6 +1741,13 @@ SYM_FUNC_START(cpu_switch_to)
 	ldr	lr, [x8]
 	mov	sp, x9
 	msr	sp_el0, x1
+#ifdef CONFIG_IEE
+	// tsk check.
+	ldr_this_cpu x8, __entry_task, x9
+	mrs x9, sp_el0
+	cmp x8, x9
+	b.ne sp_el0_check_failed
+#endif
 	ptrauth_keys_install_kernel x1, x8, x9, x10
 	scs_save x0, x8
 	scs_load_current
@@ -1169,6 +1904,13 @@ SYM_CODE_START(__sdei_asm_handler)
 	mrs	x28, sp_el0
 	ldr_this_cpu	dst=x0, sym=__entry_task, tmp=x1
 	msr	sp_el0, x0
+#ifdef CONFIG_IEE
+	// tsk check.
+	ldr_this_cpu x0, __entry_task, x1
+	mrs x1, sp_el0
+	cmp x0, x1
+	b.ne sp_el0_check_failed
+#endif
 
 	/* If we interrupted the kernel point to the previous stack/frame. */
 	and     x0, x3, #0xc
diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
index 91890de6cefa..bd23dac0dd2e 100644
--- a/arch/arm64/kernel/head.S
+++ b/arch/arm64/kernel/head.S
@@ -300,7 +300,11 @@ SYM_FUNC_START_LOCAL(__create_page_tables)
 	subs	x1, x1, #64
 	b.ne	1b
 
+#ifdef CONFIG_IEE
+    mov	x7, SWAPPER_MM_MMUFLAGS_IDMAP
+#else
 	mov	x7, SWAPPER_MM_MMUFLAGS
+#endif
 
 	/*
 	 * Create the identity mapping.
@@ -375,6 +379,10 @@ SYM_FUNC_START_LOCAL(__create_page_tables)
 
 	map_memory x0, x1, x3, x6, x7, x3, x4, x10, x11, x12, x13, x14
 
+#ifdef CONFIG_IEE
+    mov	x7, SWAPPER_MM_MMUFLAGS
+#endif
+
 	/*
 	 * Map the kernel image (starting with PHYS_OFFSET).
 	 */
@@ -880,6 +888,18 @@ SYM_FUNC_START_LOCAL(secondary_startup)
 	br	x8
 SYM_FUNC_END(secondary_startup)
 
+#ifdef CONFIG_IEE
+// SP_EL0 check failed.
+SYM_FUNC_START_LOCAL(sp_el0_check_failed)
+	1:
+	nop
+	nop
+	nop
+	nop
+	b 1f
+SYM_FUNC_END(sp_el0_check_failed)
+#endif
+
 SYM_FUNC_START_LOCAL(__secondary_switched)
 	adr_l	x5, vectors
 	msr	vbar_el1, x5
@@ -892,6 +912,24 @@ SYM_FUNC_START_LOCAL(__secondary_switched)
 	ldr	x2, [x0, #CPU_BOOT_TASK]
 	cbz	x2, __secondary_too_slow
 	msr	sp_el0, x2
+#ifdef CONFIG_IEE
+	// tsk check.
+	adrp x29, __per_cpu_offset
+	mrs x3, sp_el0
+	ldr x30, [x3, #TSK_TI_CPU] /* cpu number */
+1:
+	cmp x30, #0
+	b.eq 2f
+	add x29, x29, #8
+	sub x30, x30, #1
+	b 1b
+2:
+	ldr x30, [x29, #:lo12:__per_cpu_offset] /* cpu offset */
+	adr_l	x29, __entry_task
+	ldr	x29, [x29, x30]
+	cmp x29, x3
+	b.ne sp_el0_check_failed
+#endif
 	scs_load_current
 	mov	x29, #0
 	mov	x30, #0
diff --git a/arch/arm64/kernel/hibernate.c b/arch/arm64/kernel/hibernate.c
index 42003774d261..5f5571c7bf82 100644
--- a/arch/arm64/kernel/hibernate.c
+++ b/arch/arm64/kernel/hibernate.c
@@ -40,6 +40,10 @@
 #include <asm/sysreg.h>
 #include <asm/virt.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 /*
  * Hibernate core relies on this value being 0 on resume, and marks it
  * __nosavedata assuming it will keep the resume kernel's '0' value. This
@@ -187,12 +191,20 @@ static int trans_pgd_map_page(pgd_t *trans_pgd, void *page,
 	pud_t *pudp;
 	pmd_t *pmdp;
 	pte_t *ptep;
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	pgdp = pgd_offset_pgd(trans_pgd, dst_addr);
 	if (pgd_none(READ_ONCE(*pgdp))) {
 		pudp = (void *)get_safe_page(GFP_ATOMIC);
 		if (!pudp)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(pudp));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pudp);
+		#endif
 		pgd_populate(&init_mm, pgdp, pudp);
 	}
 
@@ -201,6 +213,11 @@ static int trans_pgd_map_page(pgd_t *trans_pgd, void *page,
 		pudp = (void *)get_safe_page(GFP_ATOMIC);
 		if (!pudp)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(pudp));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pudp);
+		#endif
 		p4d_populate(&init_mm, p4dp, pudp);
 	}
 
@@ -209,6 +226,11 @@ static int trans_pgd_map_page(pgd_t *trans_pgd, void *page,
 		pmdp = (void *)get_safe_page(GFP_ATOMIC);
 		if (!pmdp)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(pmdp));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pmdp);
+		#endif
 		pud_populate(&init_mm, pudp, pmdp);
 	}
 
@@ -217,6 +239,11 @@ static int trans_pgd_map_page(pgd_t *trans_pgd, void *page,
 		ptep = (void *)get_safe_page(GFP_ATOMIC);
 		if (!ptep)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(ptep));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)ptep);
+		#endif
 		pmd_populate_kernel(&init_mm, pmdp, ptep);
 	}
 
@@ -245,6 +272,9 @@ static int create_safe_exec_page(void *src_start, size_t length,
 	void *page = (void *)get_safe_page(GFP_ATOMIC);
 	pgd_t *trans_pgd;
 	int rc;
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	if (!page)
 		return -ENOMEM;
@@ -255,6 +285,11 @@ static int create_safe_exec_page(void *src_start, size_t length,
 	trans_pgd = (void *)get_safe_page(GFP_ATOMIC);
 	if (!trans_pgd)
 		return -ENOMEM;
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(trans_pgd));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)trans_pgd);
+	#endif
 
 	rc = trans_pgd_map_page(trans_pgd, page, dst_addr,
 				PAGE_KERNEL_EXEC);
@@ -275,7 +310,11 @@ static int create_safe_exec_page(void *src_start, size_t length,
 	 */
 	cpu_set_reserved_ttbr0();
 	local_flush_tlb_all();
+#ifdef CONFIG_IEE
+    iee_rwx_gate_entry(IEE_WRITE_ttbr0_el1, phys_to_ttbr(virt_to_phys(trans_pgd)));
+#else
 	write_sysreg(phys_to_ttbr(virt_to_phys(trans_pgd)), ttbr0_el1);
+#endif
 	isb();
 
 	*phys_dst_addr = virt_to_phys(page);
@@ -490,10 +529,18 @@ static int copy_pte(pmd_t *dst_pmdp, pmd_t *src_pmdp, unsigned long start,
 	pte_t *src_ptep;
 	pte_t *dst_ptep;
 	unsigned long addr = start;
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	dst_ptep = (pte_t *)get_safe_page(GFP_ATOMIC);
 	if (!dst_ptep)
 		return -ENOMEM;
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(dst_ptep));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)dst_ptep);
+	#endif
 	pmd_populate_kernel(&init_mm, dst_pmdp, dst_ptep);
 	dst_ptep = pte_offset_kernel(dst_pmdp, start);
 
@@ -512,11 +559,19 @@ static int copy_pmd(pud_t *dst_pudp, pud_t *src_pudp, unsigned long start,
 	pmd_t *dst_pmdp;
 	unsigned long next;
 	unsigned long addr = start;
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	if (pud_none(READ_ONCE(*dst_pudp))) {
 		dst_pmdp = (pmd_t *)get_safe_page(GFP_ATOMIC);
 		if (!dst_pmdp)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(dst_pmdp));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)dst_pmdp);
+		#endif
 		pud_populate(&init_mm, dst_pudp, dst_pmdp);
 	}
 	dst_pmdp = pmd_offset(dst_pudp, start);
@@ -547,11 +602,19 @@ static int copy_pud(p4d_t *dst_p4dp, p4d_t *src_p4dp, unsigned long start,
 	pud_t *src_pudp;
 	unsigned long next;
 	unsigned long addr = start;
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	if (p4d_none(READ_ONCE(*dst_p4dp))) {
 		dst_pudp = (pud_t *)get_safe_page(GFP_ATOMIC);
 		if (!dst_pudp)
 			return -ENOMEM;
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(dst_pudp));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)dst_pudp);
+		#endif
 		p4d_populate(&init_mm, dst_p4dp, dst_pudp);
 	}
 	dst_pudp = pud_offset(dst_p4dp, start);
@@ -620,6 +683,11 @@ static int trans_pgd_create_copy(pgd_t **dst_pgdp, unsigned long start,
 {
 	int rc;
 	pgd_t *trans_pgd = (pgd_t *)get_safe_page(GFP_ATOMIC);
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr = __phys_to_iee(__pa(trans_pgd));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)trans_pgd);
+	#endif
 
 	if (!trans_pgd) {
 		pr_err("Failed to allocate memory for temporary page tables.\n");
diff --git a/arch/arm64/kernel/hw_breakpoint.c b/arch/arm64/kernel/hw_breakpoint.c
index b152e69c0401..2a2df3087956 100644
--- a/arch/arm64/kernel/hw_breakpoint.c
+++ b/arch/arm64/kernel/hw_breakpoint.c
@@ -26,6 +26,10 @@
 #include <asm/cputype.h>
 #include <asm/system_misc.h>
 
+#ifdef CONFIG_IEE
+#include <asm/iee-si.h>
+#endif
+
 /* Breakpoint currently in use for each BRP. */
 static DEFINE_PER_CPU(struct perf_event *, bp_on_reg[ARM_MAX_BRP]);
 
@@ -102,13 +106,68 @@ int hw_breakpoint_slots(int type)
 	WRITE_WB_REG_CASE(OFF, 14, REG, VAL);	\
 	WRITE_WB_REG_CASE(OFF, 15, REG, VAL)
 
+#ifdef CONFIG_IEE
+
+#define IEE_SI_READ_WB_REG_CASE(OFF, N, REG, VAL)	\
+	case (OFF + N):				\
+		IEE_SI_AARCH64_DBG_READ(N, REG, VAL);	\
+		break
+
+#define IEE_SI_WRITE_WB_REG_CASE(OFF, N, REG, VAL)	\
+	case (OFF + N):				\
+		IEE_SI_AARCH64_DBG_WRITE(N, REG, VAL);	\
+		break
+
+#define IEE_SI_GEN_READ_REG_CASES(OFF, REG, VAL)	\
+	IEE_SI_READ_WB_REG_CASE(OFF, 0, REG, VAL);  \
+	WRITE_WB_REG_CASE(OFF,  1, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  2, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  3, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  4, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  5, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  6, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  7, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  8, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  9, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 10, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 11, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 12, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 13, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 14, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 15, REG, VAL)
+
+#define IEE_SI_GEN_WRITE_REG_CASES(OFF, REG, VAL)	\
+	IEE_SI_WRITE_WB_REG_CASE(OFF, 0, REG, VAL);  \
+	WRITE_WB_REG_CASE(OFF,  1, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  2, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  3, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  4, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  5, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  6, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  7, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  8, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF,  9, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 10, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 11, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 12, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 13, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 14, REG, VAL);	\
+	WRITE_WB_REG_CASE(OFF, 15, REG, VAL)
+
+#endif
+
 static u64 read_wb_reg(int reg, int n)
 {
 	u64 val = 0;
 
 	switch (reg + n) {
+// #ifdef CONFIG_IEE
+//     IEE_SI_GEN_READ_REG_CASES(AARCH64_DBG_REG_BVR, AARCH64_DBG_REG_NAME_BVR, val);
+// 	IEE_SI_GEN_READ_REG_CASES(AARCH64_DBG_REG_BCR, AARCH64_DBG_REG_NAME_BCR, val);
+// #else
 	GEN_READ_WB_REG_CASES(AARCH64_DBG_REG_BVR, AARCH64_DBG_REG_NAME_BVR, val);
 	GEN_READ_WB_REG_CASES(AARCH64_DBG_REG_BCR, AARCH64_DBG_REG_NAME_BCR, val);
+// #endif
 	GEN_READ_WB_REG_CASES(AARCH64_DBG_REG_WVR, AARCH64_DBG_REG_NAME_WVR, val);
 	GEN_READ_WB_REG_CASES(AARCH64_DBG_REG_WCR, AARCH64_DBG_REG_NAME_WCR, val);
 	default:
@@ -122,8 +181,13 @@ NOKPROBE_SYMBOL(read_wb_reg);
 static void write_wb_reg(int reg, int n, u64 val)
 {
 	switch (reg + n) {
+// #ifdef CONFIG_IEE
+//     IEE_SI_GEN_WRITE_REG_CASES(AARCH64_DBG_REG_BVR, AARCH64_DBG_REG_NAME_BVR, val);
+// 	IEE_SI_GEN_WRITE_REG_CASES(AARCH64_DBG_REG_BCR, AARCH64_DBG_REG_NAME_BCR, val);
+// #else
 	GEN_WRITE_WB_REG_CASES(AARCH64_DBG_REG_BVR, AARCH64_DBG_REG_NAME_BVR, val);
 	GEN_WRITE_WB_REG_CASES(AARCH64_DBG_REG_BCR, AARCH64_DBG_REG_NAME_BCR, val);
+// #endif
 	GEN_WRITE_WB_REG_CASES(AARCH64_DBG_REG_WVR, AARCH64_DBG_REG_NAME_WVR, val);
 	GEN_WRITE_WB_REG_CASES(AARCH64_DBG_REG_WCR, AARCH64_DBG_REG_NAME_WCR, val);
 	default:
@@ -171,6 +235,10 @@ static int is_a32_compat_bp(struct perf_event *bp)
 	return tsk && is_a32_compat_thread(task_thread_info(tsk));
 }
 
+#ifdef CONFIG_IEE
+int arch_check_bp_in_kernelspace(struct arch_hw_breakpoint *hw);
+#endif
+
 /**
  * hw_breakpoint_slot_setup - Find and setup a perf slot according to
  *			      operations
@@ -191,6 +259,37 @@ static int hw_breakpoint_slot_setup(struct perf_event **slots, int max_slots,
 {
 	int i;
 	struct perf_event **slot;
+// reserve hw breakpoint 0 for iee rwx gate in kernel sapce.
+// #ifdef CONFIG_IEE
+//     struct arch_hw_breakpoint *info = counter_arch_bp(bp);
+//     if (arch_check_bp_in_kernelspace(info)){
+//         for (i = 1; i < max_slots; ++i) {   // search from hw breakpoint 1
+//             slot = &slots[i];
+//             switch (ops) {
+//             case HW_BREAKPOINT_INSTALL:
+//                 if (!*slot) {
+//                     *slot = bp;
+//                     return i;
+//                 }
+//                 break;
+//             case HW_BREAKPOINT_UNINSTALL:
+//                 if (*slot == bp) {
+//                     *slot = NULL;
+//                     return i;
+//                 }
+//                 break;
+//             case HW_BREAKPOINT_RESTORE:
+//                 if (*slot == bp)
+//                     return i;
+//                 break;
+//             default:
+//                 pr_warn_once("Unhandled hw breakpoint ops %d\n", ops);
+//                 return -EINVAL;
+//             }
+//         }
+//         return -ENOSPC;
+//     }
+// #endif
 
 	for (i = 0; i < max_slots; ++i) {
 		slot = &slots[i];
diff --git a/arch/arm64/kernel/iee/Makefile b/arch/arm64/kernel/iee/Makefile
new file mode 100644
index 000000000000..123c68c5cc4e
--- /dev/null
+++ b/arch/arm64/kernel/iee/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_IEE) += iee.o iee-gate.o iee-func.o
\ No newline at end of file
diff --git a/arch/arm64/kernel/iee/iee-func.c b/arch/arm64/kernel/iee/iee-func.c
new file mode 100644
index 000000000000..8ef0b769af73
--- /dev/null
+++ b/arch/arm64/kernel/iee/iee-func.c
@@ -0,0 +1,189 @@
+#include "asm/pgtable.h"
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <asm/tlb.h>
+#include <asm/tlbflush.h>
+#include <asm/pgalloc.h>
+
+void set_iee_page_valid(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+	pte_t pte = READ_ONCE(*ptep);
+
+    if((addr < (((unsigned long)0xffff << 48) + IEE_OFFSET)) | (addr > (((unsigned long)0xffff8 << 44))))
+		return;
+
+	pte = __pte(pte_val(pte) | 0x1);
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+
+void set_iee_page_invalid(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+	pte_t pte = READ_ONCE(*ptep);
+
+    if((addr < (((unsigned long)0xffff << 48) + IEE_OFFSET)) | (addr > (((unsigned long)0xffff8 << 44))))
+		return;
+
+	pte = __pte(pte_val(pte) & ~0x1);
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+
+void iee_set_logical_mem_ro(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+	pte_t pte = READ_ONCE(*ptep);
+
+    if(addr < ((unsigned long)0xffff << 48))
+		return;
+
+	pte = __pte((pte_val(pte) | PTE_RDONLY) & ~PTE_DBM);
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+
+void iee_set_logical_mem_rw(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+	pte_t pte = READ_ONCE(*ptep);
+
+    if((addr < ((unsigned long)0xffff << 48)) | (addr > ((unsigned long)0xffff4 << 44)))
+		return;
+    
+	pte = __pte(pte_val(pte) | PTE_DBM);
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+
+void iee_set_token_page_valid(void *token, void *new)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, (unsigned long)token);
+
+	p4d_t *p4dp = p4d_offset(pgdp, (unsigned long)token);
+
+	pud_t *pudp = pud_offset(p4dp, (unsigned long)token);
+
+	pmd_t *pmdp = pmd_offset(pudp, (unsigned long)token);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, (unsigned long)token);
+	pte_t pte = READ_ONCE(*ptep);
+	pte = __pte(((pte_val(pte) | 0x1) & ~PTE_ADDR_MASK) | __phys_to_pte_val(__pa(new)));
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range((unsigned long)token, (unsigned long)(token+PAGE_SIZE));
+	isb();
+}
+
+void iee_set_token_page_invalid(void *token)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, (unsigned long)token);
+
+	p4d_t *p4dp = p4d_offset(pgdp, (unsigned long)token);
+
+	pud_t *pudp = pud_offset(p4dp, (unsigned long)token);
+
+	pmd_t *pmdp = pmd_offset(pudp, (unsigned long)token);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, (unsigned long)token);
+	pte_t pte = READ_ONCE(*ptep);
+	pte = __pte(((pte_val(pte) & ~((unsigned long)0x1)) & ~PTE_ADDR_MASK) | __phys_to_pte_val(__pa(token - IEE_OFFSET)));
+	set_pte(ptep, pte);
+	flush_tlb_kernel_range((unsigned long)token, (unsigned long)(token+PAGE_SIZE));
+	isb();
+}
+
+void iee_set_kernel_ppage(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+
+	int i;
+	for(i = 0; i < 4; i++)
+	{
+		pte_t pte = READ_ONCE(*ptep);
+		pte = __pte(pte_val(pte) & ~PTE_USER & ~PTE_NG);
+		iee_set_pte_ppage(ptep, pte);
+		ptep++;
+	}
+	flush_tlb_kernel_range(addr, addr+4*PAGE_SIZE);
+	isb();
+}
+
+void iee_set_kernel_upage(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+	p4d_t p4d = READ_ONCE(*p4dp);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+
+	int i;
+
+    __p4d_populate(p4dp, __p4d_to_phys(p4d), PGD_APT | PUD_TYPE_TABLE);
+	for(i = 0; i < 4; i++)
+	{
+		pte_t pte = READ_ONCE(*ptep);
+		pte = __pte(pte_val(pte) | PTE_USER | PTE_NG);
+		iee_set_pte_upage(ptep, pte);
+		ptep++;
+	}
+	flush_tlb_kernel_range(addr, addr+4*PAGE_SIZE);
+	isb();
+}
\ No newline at end of file
diff --git a/arch/arm64/kernel/iee/iee-gate.S b/arch/arm64/kernel/iee/iee-gate.S
new file mode 100644
index 000000000000..c0633674f413
--- /dev/null
+++ b/arch/arm64/kernel/iee/iee-gate.S
@@ -0,0 +1,229 @@
+#include <asm/asm-offsets.h>
+#include <linux/linkage.h>
+#include <asm/bug.h>
+#include <asm-generic/export.h>
+
+#ifdef CONFIG_IEE
+
+SYM_FUNC_START(iee_rw_gate)
+    /* save daif, close irq  */
+    mrs x13, daif
+	msr daifset, #0x2
+    isb
+    /* save lr */
+    sub	sp, sp, #16
+	stp	x29, x30, [sp]
+	bl iee_protected_rw_gate
+    /* restore lr */
+    ldp	x29, x30, [sp]
+	add	sp, sp, #16
+    /* restore daif */
+	msr daif, x13
+    ret
+SYM_FUNC_END(iee_rw_gate)
+#if defined(CONFIG_CREDP) || defined(CONFIG_KOI)
+EXPORT_SYMBOL(iee_rw_gate)
+#endif
+
+	.pushsection ".iee.text.header", "ax"
+
+SYM_FUNC_START(iee_protected_rw_gate)
+    mrs x9, pan
+	/* disable PAN */
+	msr pan, #0x0
+	/* switch to iee stack */
+	mrs x9, sp_el0  /* x9 -> task_struct */
+    adrp x12, iee_offset
+    ldr x12, [x12, #:lo12:iee_offset]
+    add x11, x9, x12 /* x11 -> task_token(IEE) */
+    // store kernel stack
+    mov x10, sp
+    str x10, [x11, #kernel_from_token_offset]
+    // load iee stack
+    ldr x10, [x11, #iee_from_token_offset]
+    mov sp, x10
+#ifdef CONFIG_IEE_INTERRUPTABLE
+	isb
+	/* restore daif */
+	msr	daif, x13
+	sub	sp, sp, #16
+	stp	x29, x30, [sp]
+#else
+    sub	sp, sp, #16
+    stp	x13, x30, [sp]
+#endif
+	/* call iee func */
+	bl	iee_dispatch
+#ifdef CONFIG_IEE_INTERRUPTABLE
+	ldp	x29, x30, [sp]
+	add	sp, sp, #16
+	/* store and disable daif */
+	mrs	x13, daif
+	msr	daifset, #0x2
+	isb
+#else
+    ldp	x13, x30, [sp]
+    add	sp, sp, #16
+#endif
+	/* switch to kernel stack */
+	mrs x9, sp_el0 /* x9 -> task_struct(VA) */
+    adrp x12, iee_offset
+    ldr x12, [x12, #:lo12:iee_offset]
+    add x11, x9, x12 /* x11 -> task_token(IEE) */
+    // store iee stack
+    mov x10, sp
+    str x10, [x11, #iee_from_token_offset]
+    // load kernel stack
+    ldr x10, [x11, #kernel_from_token_offset]
+    mov sp, x10
+	/* enable PAN */
+	msr pan, #0x1
+	ret
+SYM_FUNC_END(iee_protected_rw_gate)
+
+	.popsection
+
+#include <asm/asm-bug.h>
+#define BAD_IEE		4
+
+    .pushsection ".iee.exec_entry", "ax"
+
+SYM_FUNC_START(iee_rwx_gate_entry)
+    /* Disable irq first. */
+    mrs x15, daif           // use x15 to restore daif
+    msr DAIFSet, #0x2
+
+    msr dbgbvr0_el1, xzr    // clear breakpoint 0 
+    isb
+    
+    msr pan, #0
+#ifdef CONFIG_KOI
+    cmp x0, #7
+    b.lo 3f
+
+    /* Use x13 to mark KOI switch branches. */
+    mov x13, #1
+
+    /* Get ASID from ttbr0 to calculate ASID of TTBR1. */
+    mrs x10, ttbr0_el1
+    ubfx x12, x10, #48, #16
+    /* ASID of ttbr0 must be odd number except 1 when KOI switch. */
+    tbz x12, #0, error
+    cmp x12, #1
+    b.eq error
+    bic x12, x12, #1
+
+    cmp x0, #7
+    b.eq 2f
+    /* Branch of switching to KOI pgd. */
+    /* TODO: verify the incomming pdg addr val. */
+    mov x9, x1
+    bfi x9, x12, #48, #16
+    b 1f
+2:  /* Branch of switching back to kernel. */
+    adrp x9, iee_base_swapper_pg_dir
+    ldr	x9, [x9, #:lo12:iee_base_swapper_pg_dir]
+    bfi x9, x12, #48, #16
+    b 1f
+3:  /* Branch of switching to IEE pgd. */
+    mov x13, #0
+#endif
+// trans va of iee_pg_dir to pa to switch ttbr1    
+    mrs x14, ttbr1_el1          // use x14 to restore ttbr1
+    adrp x9, iee_base_iee_pg_dir
+    ldr	x9, [x9, #:lo12:iee_base_iee_pg_dir]
+    movk x9, #1, lsl 48
+1:
+    msr ttbr1_el1, x9
+    isb
+    /* Reset dbgbvr0_el1 and check its value to prevent reuse attack */
+4:
+    adr x9, 1b
+    msr dbgbvr0_el1, x9     // protect msr ttbr1 inst
+    isb
+    mrs x10, dbgbvr0_el1
+    adr x11, 1b
+    cmp x10, x11
+    b.ne 4b
+#ifdef CONFIG_KOI
+    cbz x13, 5f
+    /* Restore PAN and DAIF to end KOI switch */
+    msr pan, #1
+    msr daif, x15
+    ret
+5:
+#endif
+    /* jump to iee si codes which are only visible to iee pgd. */
+    b iee_rwx_gate_tramp
+error:
+	mov	x0, sp
+	mov	x1, #BAD_IEE
+	mrs	x2, esr_el1
+	bl	bad_mode
+	ASM_BUG()                  
+SYM_FUNC_END(iee_rwx_gate_entry)
+EXPORT_SYMBOL(iee_rwx_gate_entry)
+
+    .popsection
+
+    .pushsection ".iee.si_text", "ax"
+
+SYM_FUNC_START(iee_rwx_gate_tramp)
+    /* use x13 to store current stack */
+    mov x13, sp
+    
+    /* If iee hasn't initialized, skip stack switch. */
+    ldr x11, =iee_init_done;
+    ldr x10, [x11]
+    cbz x10, 1f
+
+    /* Switch to iee stack */
+    mrs x9, sp_el0              // x9 -> task_struct
+    adrp x12, iee_offset
+    ldr x12, [x12, #:lo12:iee_offset]
+    add x11, x9, x12            // x11 -> task_token(IEE)
+    // load iee stack
+    ldr x10, [x11, #iee_from_token_offset]
+    mov sp, x10
+1:  
+    str x13, [sp, #-16]!        // Switch stack end
+
+    /* In context switch, change x14 here to modify ASID of TTBR1 when iee exits */
+    cmp w0, #6                  // for IEE_CONTEXT_SWITCH
+    b.ne 2f
+    ubfx x10, x1, #48, #16      // get ASID field of incomming TTBR0 value.
+    bic x10, x10, #1
+    bfi x14, x10, #48, #16      // insert new ASID to x14
+    
+    /* x15 stores daif and x14 stores previous ttbr1 */
+2:
+    stp x15, x14, [sp, #-32]!
+    stp x29, x30, [sp, #16]
+    bl iee_si_handler           // enter actual handler
+    ldp x29, x30, [sp, #16]
+
+    b iee_rwx_gate_exit         // jump to iee exit
+SYM_FUNC_END(iee_rwx_gate_tramp)
+
+    .popsection
+
+    .pushsection ".iee.exec_exit", "ax"
+
+SYM_FUNC_START(iee_rwx_gate_exit)
+    ldp x15, x14, [sp], #32
+// switch to kernel stack
+    ldr x13, [sp], #16
+    mov sp, x13
+// switch end
+    msr pan, #1
+    /* Switch back to kernel PGD. ASID maybe changed by iee_rwx_gate_tramp. */
+    msr ttbr1_el1, x14
+    isb
+    msr daif, x15
+    isb
+    ret 
+SYM_FUNC_END(iee_rwx_gate_exit)
+
+    .popsection
+
+#endif
diff --git a/arch/arm64/kernel/iee/iee.c b/arch/arm64/kernel/iee/iee.c
new file mode 100644
index 000000000000..85ee8bd1f2e5
--- /dev/null
+++ b/arch/arm64/kernel/iee/iee.c
@@ -0,0 +1,1363 @@
+#include "linux/sched.h"
+#include <stdarg.h>
+#include <asm/pgtable-types.h>
+#include <asm/iee.h>
+#include <asm/iee-si.h>
+#include <asm/sysreg.h>
+#include <linux/pgtable.h>
+#include <linux/cred.h>
+#include <asm/iee-slab.h>
+#include <asm/percpu.h>
+
+#ifdef CONFIG_IEE
+extern struct cred init_cred;
+extern s64			memstart_addr;
+
+void __iee_code _iee_set_swapper_pgd(pgd_t *pgdp, pgd_t pgd);
+void __iee_code _iee_iee_set_tramp_pgd_pre_init(pgd_t *pgdp, pgd_t pgd);
+void __iee_code _iee_set_pte(pte_t *ptep, pte_t pte);
+void __iee_code _iee_set_pmd(pmd_t *pmdp, pmd_t pmd);
+void __iee_code _iee_set_pud(pud_t *pudp, pud_t pud);
+void __iee_code _iee_set_p4d(p4d_t *p4dp, p4d_t p4d);
+void __iee_code _iee_set_bm_pte(pte_t *ptep, pte_t pte);
+pteval_t __iee_code _iee_set_xchg_relaxed(pte_t *ptep, pteval_t pteval);
+pteval_t __iee_code _iee_set_cmpxchg_relaxed(pte_t *ptep, pteval_t old_pteval, pteval_t new_pteval);
+void __iee_code _iee_write_in_byte(void *ptr, __u64 data, int length);
+void __iee_code _iee_set_cred_uid(struct cred *cred, kuid_t uid);
+void __iee_code _iee_set_cred_gid(struct cred *cred, kgid_t gid);
+void __iee_code _iee_copy_cred(struct cred *old, struct cred *new);
+void __iee_code _iee_set_cred_suid(struct cred *cred, kuid_t suid);
+void __iee_code _iee_set_cred_sgid(struct cred *cred, kgid_t sgid);
+void __iee_code _iee_set_cred_euid(struct cred *cred, kuid_t euid);
+void __iee_code _iee_set_cred_egid(struct cred *cred, kgid_t egid);
+void __iee_code _iee_set_cred_fsuid(struct cred *cred, kuid_t fsuid);
+void __iee_code _iee_set_cred_fsgid(struct cred *cred, kgid_t fsgid);
+void __iee_code _iee_set_cred_user(struct cred *cred, struct user_struct *user);
+void __iee_code _iee_set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns);
+void __iee_code _iee_set_cred_group_info(struct cred *cred, struct group_info *group_info);
+void __iee_code _iee_set_cred_securebits(struct cred *cred, unsigned securebits);
+void __iee_code _iee_set_cred_cap_inheritable(struct cred *cred, kernel_cap_t cap_inheritable);
+void __iee_code _iee_set_cred_cap_permitted(struct cred *cred, kernel_cap_t cap_permitted);
+void __iee_code _iee_set_cred_cap_effective(struct cred *cred, kernel_cap_t cap_effective);
+void __iee_code _iee_set_cred_cap_bset(struct cred *cred, kernel_cap_t cap_bset);
+void __iee_code _iee_set_cred_cap_ambient(struct cred *cred, kernel_cap_t cap_ambient);
+void __iee_code _iee_set_cred_jit_keyring(struct cred *cred, unsigned char jit_keyring);
+void __iee_code _iee_set_cred_session_keyring(struct cred *cred, struct key *session_keyring);
+void __iee_code _iee_set_cred_process_keyring(struct cred *cred, struct key *process_keyring);
+void __iee_code _iee_set_cred_thread_keyring(struct cred *cred, struct key *thread_keyring);
+void __iee_code _iee_set_cred_request_key_auth(struct cred *cred, struct key *request_key_auth);
+void __iee_code _iee_set_cred_non_rcu(struct cred *cred, int non_rcu);
+void __iee_code _iee_set_cred_atomic_set_usage(struct cred *cred, int i);
+bool __iee_code _iee_set_cred_atomic_op_usage(struct cred *cred, int flag);
+void __iee_code _iee_set_cred_security(struct cred *cred, void *security);
+void __iee_code _iee_set_cred_rcu(struct cred *cred, struct rcu_head *rcu);
+void __iee_code _iee_memset(void *ptr, int data, size_t n);
+void __iee_code _iee_set_track(struct track *ptr, struct track *data);
+void __iee_code _iee_set_freeptr(void **pptr, void *ptr);
+void __iee_code _iee_set_pte_upage(pte_t *ptep, pte_t pte);
+void __iee_code _iee_set_pte_ppage(pte_t *ptep, pte_t pte);
+void __iee_code _iee_set_token_mm(struct task_struct *tsk, struct mm_struct *mm);
+void __iee_code _iee_set_token_pgd(struct task_struct *tsk, pgd_t *pgd);
+void __iee_code _iee_init_token(struct task_struct *tsk, void *kernel_stack, void *iee_stack);
+void __iee_code _iee_free_token(struct task_struct *tsk);
+unsigned long __iee_code _iee_read_token_stack(struct task_struct *tsk);
+void __iee_code _iee_write_entry_task(struct task_struct *tsk);
+#ifdef CONFIG_KOI
+unsigned long __iee_code _iee_read_koi_stack(struct task_struct *tsk);
+void __iee_code _iee_write_koi_stack(struct task_struct *tsk, unsigned long koi_stack); 
+unsigned long __iee_code _iee_read_token_ttbr1(struct task_struct *tsk);
+void __iee_code _iee_write_token_ttbr1(struct task_struct *tsk, unsigned long current_ttbr1);
+unsigned long __iee_code _iee_read_koi_kernel_stack(struct task_struct *tsk);
+void __iee_code _iee_write_koi_kernel_stack(struct task_struct *tsk, unsigned long kernel_stack);
+unsigned long __iee_code _iee_read_koi_stack_base(struct task_struct *tsk);
+void __iee_code _iee_write_koi_stack_base(struct task_struct *tsk, unsigned long koi_stack_base);
+#endif
+
+/* wrapper functions */
+void __iee_code iee_wrapper_write_in_byte(va_list args) {
+    void *ptr = va_arg(args, void *);
+    __u64 data = va_arg(args, __u64);
+    int length = va_arg(args, int);
+    _iee_write_in_byte(ptr, data, length);
+}
+
+void __iee_code iee_wrapper_set_pte(va_list args) {
+    pte_t *ptep = va_arg(args, pte_t *);
+    pte_t pte = va_arg(args, pte_t);
+    _iee_set_pte(ptep, pte);
+}
+
+void __iee_code iee_wrapper_set_pmd(va_list args) {
+    pmd_t *pmdp = va_arg(args, pmd_t *);
+    pmd_t pmd = va_arg(args, pmd_t);
+    _iee_set_pmd(pmdp, pmd);
+}
+
+void __iee_code iee_wrapper_set_p4d(va_list args) {
+    p4d_t *p4dp = va_arg(args, p4d_t *);
+    p4d_t p4d = va_arg(args, p4d_t);
+    _iee_set_p4d(p4dp, p4d);
+}
+
+void __iee_code iee_wrapper_set_pud(va_list args) {
+    pud_t *pudp = va_arg(args, pud_t *);
+    pud_t pud = va_arg(args, pud_t);
+    _iee_set_pud(pudp, pud);
+}
+
+void __iee_code iee_wrapper_set_bm_pte(va_list args) {
+    pte_t *ptep = va_arg(args, pte_t *);
+    pte_t pte = va_arg(args, pte_t);
+    _iee_set_bm_pte(ptep, pte);
+}
+
+void __iee_code iee_wrapper_set_swapper_pgd(va_list args) {
+    pgd_t *pgdp = va_arg(args, pgd_t *);
+    pgd_t pgd = va_arg(args, pgd_t);
+    _iee_set_swapper_pgd(pgdp, pgd);
+}
+
+void __iee_code iee_wrapper_set_tramp_pgd(va_list args) {
+    pgd_t *pgdp = va_arg(args, pgd_t *);
+    pgd_t pgd = va_arg(args, pgd_t);
+    _iee_iee_set_tramp_pgd_pre_init(pgdp, pgd);
+}
+
+pteval_t __iee_code iee_wrapper_set_xchg(va_list args) {
+	pteval_t ret;
+    pte_t *ptep = va_arg(args, pte_t *);
+    pteval_t pteval = va_arg(args, pteval_t);
+    ret = _iee_set_xchg_relaxed(ptep, pteval);
+	return (u64)ret;
+}
+
+pteval_t __iee_code iee_wrapper_set_cmpxchg(va_list args) {
+	pteval_t ret;
+    pte_t *ptep = va_arg(args, pte_t *);
+    pteval_t old_pteval = va_arg(args, pteval_t);
+    pteval_t new_pteval = va_arg(args, pteval_t);
+    ret = _iee_set_cmpxchg_relaxed(ptep, old_pteval, new_pteval);
+	return (u64)ret;
+}
+
+void __iee_code iee_wrapper_set_cred_uid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kuid_t uid = va_arg(args, kuid_t);
+    _iee_set_cred_uid(cred, uid);
+}
+
+void __iee_code iee_wrapper_set_cred_gid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kgid_t gid = va_arg(args, kgid_t);
+    _iee_set_cred_gid(cred, gid);
+}
+
+void __iee_code iee_wrapper_copy_cred(va_list args) {
+    struct cred *old = va_arg(args, struct cred *);
+    struct cred *new = va_arg(args, struct cred *);
+    _iee_copy_cred(old, new);
+}
+
+void __iee_code iee_wrapper_set_cred_suid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kuid_t suid = va_arg(args, kuid_t);
+    _iee_set_cred_suid(cred, suid);
+}
+
+void __iee_code iee_wrapper_set_cred_sgid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kgid_t sgid = va_arg(args, kgid_t);
+    _iee_set_cred_sgid(cred, sgid);
+}
+
+void __iee_code iee_wrapper_set_cred_euid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kuid_t euid = va_arg(args, kuid_t);
+    _iee_set_cred_euid(cred, euid);
+}
+
+void __iee_code iee_wrapper_set_cred_egid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kgid_t egid = va_arg(args, kgid_t);
+    _iee_set_cred_egid(cred, egid);
+}
+
+void __iee_code iee_wrapper_set_cred_fsuid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kuid_t fsuid = va_arg(args, kuid_t);
+    _iee_set_cred_fsuid(cred, fsuid);
+}
+
+void __iee_code iee_wrapper_set_cred_fsgid(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kgid_t fsgid = va_arg(args, kgid_t);
+    _iee_set_cred_fsgid(cred, fsgid);
+}
+
+void __iee_code iee_wrapper_set_cred_user(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct user_struct *user = va_arg(args, struct user_struct *);
+    _iee_set_cred_user(cred, user);
+}
+
+void __iee_code iee_wrapper_set_cred_user_ns(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct user_namespace *user_ns = va_arg(args, struct user_namespace *);
+    _iee_set_cred_user_ns(cred, user_ns);
+}
+
+void __iee_code iee_wrapper_set_cred_group_info(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct group_info *group_info = va_arg(args, struct group_info *);
+    _iee_set_cred_group_info(cred, group_info);
+}
+
+void __iee_code iee_wrapper_set_cred_securebits(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    unsigned securebits = va_arg(args, unsigned);
+    _iee_set_cred_securebits(cred, securebits);
+}
+
+void __iee_code iee_wrapper_set_cred_cap_inheritable(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kernel_cap_t cap_inheritable = va_arg(args, kernel_cap_t);
+    _iee_set_cred_cap_inheritable(cred, cap_inheritable);
+}
+
+void __iee_code iee_wrapper_set_cred_cap_permitted(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kernel_cap_t cap_permitted = va_arg(args, kernel_cap_t);
+    _iee_set_cred_cap_permitted(cred, cap_permitted);
+}
+
+void __iee_code iee_wrapper_set_cred_cap_effective(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kernel_cap_t cap_effective = va_arg(args, kernel_cap_t);
+    _iee_set_cred_cap_effective(cred, cap_effective);
+}
+
+void __iee_code iee_wrapper_set_cred_cap_bset(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kernel_cap_t cap_bset = va_arg(args, kernel_cap_t);
+    _iee_set_cred_cap_bset(cred, cap_bset);
+}
+
+void __iee_code iee_wrapper_set_cred_cap_ambient(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    kernel_cap_t cap_ambient = va_arg(args, kernel_cap_t);
+    _iee_set_cred_cap_ambient(cred, cap_ambient);
+}
+
+void __iee_code iee_wrapper_set_cred_jit_keyring(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    unsigned long jit_keyring = va_arg(args, unsigned long);
+    _iee_set_cred_jit_keyring(cred, (unsigned char)jit_keyring);
+}
+
+void __iee_code iee_wrapper_set_cred_session_keyring(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct key *session_keyring = va_arg(args, struct key *);
+    _iee_set_cred_session_keyring(cred, session_keyring);
+}
+
+void __iee_code iee_wrapper_set_cred_process_keyring(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct key *process_keyring = va_arg(args, struct key *);
+    _iee_set_cred_process_keyring(cred, process_keyring);
+}
+
+void __iee_code iee_wrapper_set_cred_thread_keyring(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct key *thread_keyring = va_arg(args, struct key *);
+    _iee_set_cred_thread_keyring(cred, thread_keyring);
+}
+
+void __iee_code iee_wrapper_set_cred_request_key_auth(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct key *request_key_auth = va_arg(args, struct key *);
+    _iee_set_cred_request_key_auth(cred, request_key_auth);
+}
+
+void __iee_code iee_wrapper_set_cred_non_rcu(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    int non_rcu = va_arg(args, int);
+    _iee_set_cred_non_rcu(cred, non_rcu);
+}
+
+void __iee_code iee_wrapper_set_cred_atomic_set_usage(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    int i = va_arg(args, int);
+    _iee_set_cred_atomic_set_usage(cred, i);
+}
+
+u64 __iee_code iee_wrapper_set_cred_atomic_op_usage(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    int flag = va_arg(args, int);
+    return (u64)_iee_set_cred_atomic_op_usage(cred, flag);
+}
+
+void __iee_code iee_wrapper_set_cred_security(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    void *security = va_arg(args, void *);
+    _iee_set_cred_security(cred, security);
+}
+
+void __iee_code iee_wrapper_set_cred_rcu(va_list args) {
+    struct cred *cred = va_arg(args, struct cred *);
+    struct rcu_head *rcu = va_arg(args, struct rcu_head *);
+    _iee_set_cred_rcu(cred, rcu);
+}
+
+void __iee_code iee_wrapper_memset(va_list args) {
+    void *ptr = va_arg(args, void *);
+    int data = va_arg(args, int);
+    size_t n = va_arg(args, size_t);
+    _iee_memset(ptr, data, n);
+}
+
+void __iee_code iee_wrapper_set_track(va_list args) {
+    struct track *ptr = va_arg(args, struct track *);
+    struct track *data = va_arg(args, struct track *);
+    _iee_set_track(ptr, data);
+}
+
+void __iee_code iee_wrapper_set_freeptr(va_list args) {
+    void **pptr = va_arg(args, void **);
+    void *ptr = va_arg(args, void *);
+    _iee_set_freeptr(pptr, ptr);
+}
+
+void __iee_code iee_wrapper_set_pte_upage(va_list args) {
+	pte_t *ptep = va_arg(args, pte_t *);
+    pte_t pte = va_arg(args, pte_t);
+    _iee_set_pte_upage(ptep, pte);
+}
+
+void __iee_code iee_wrapper_set_pte_ppage(va_list args) {
+	pte_t *ptep = va_arg(args, pte_t *);
+    pte_t pte = va_arg(args, pte_t);
+    _iee_set_pte_ppage(ptep, pte);
+}
+
+void __iee_code iee_wrapper_set_token_mm(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	struct mm_struct *mm = va_arg(args, struct mm_struct *);
+	_iee_set_token_mm(tsk, mm);
+}
+
+void __iee_code iee_wrapper_set_token_pgd(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	pgd_t *pgd = va_arg(args, pgd_t *);
+	_iee_set_token_pgd(tsk, pgd);
+}
+
+void __iee_code iee_wrapper_init_token(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	void *kernel_stack = va_arg(args, void *);
+	void *iee_stack = va_arg(args, void *);
+	_iee_init_token(tsk, kernel_stack, iee_stack);
+}
+
+void __iee_code iee_wrapper_free_token(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	_iee_free_token(tsk);
+}
+
+u64 __iee_code iee_wrapper_read_token_stack(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	return (u64)_iee_read_token_stack(tsk);
+}
+
+void __iee_code iee_wrapper_write_entry_task(va_list args) {
+	struct task_struct *tsk = va_arg(args, struct task_struct *);
+	_iee_write_entry_task(tsk);
+}
+
+#ifdef CONFIG_KOI
+u64 __iee_code iee_wrapper_read_koi_stack(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    return (u64)_iee_read_koi_stack(tsk);
+}
+
+void __iee_code iee_wrapper_write_koi_stack(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    unsigned long koi_stack = va_arg(args, unsigned long);
+    _iee_write_koi_stack(tsk, koi_stack);
+}
+
+u64 __iee_code iee_wrapper_read_token_ttbr1(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    return (u64)_iee_read_token_ttbr1(tsk);
+}
+
+void __iee_code iee_wrapper_write_token_ttbr1(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    unsigned long current_ttbr1 = va_arg(args, unsigned long);
+    _iee_write_token_ttbr1(tsk, current_ttbr1);
+}
+
+u64 __iee_code iee_wrapper_read_koi_kernel_stack(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    return (u64)_iee_read_koi_kernel_stack(tsk);
+}
+
+void __iee_code iee_wrapper_write_koi_kernel_stack(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    unsigned long kernel_stack = va_arg(args, unsigned long);
+    _iee_write_koi_kernel_stack(tsk, kernel_stack);
+}
+
+u64 __iee_code iee_wrapper_read_koi_stack_base(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    return (u64)_iee_read_koi_stack_base(tsk);
+}
+
+void __iee_code iee_wrapper_write_koi_stack_base(va_list args) {
+    struct task_struct *tsk = va_arg(args, struct task_struct *);
+    unsigned long koi_stack_base = va_arg(args, unsigned long);
+    _iee_write_koi_stack_base(tsk, koi_stack_base);
+}
+#endif
+// Define the function pointer type for wrapper functions.
+// Each function pointer conforms to a standardized calling convention
+// using a variable argument list (va_list) as its parameter. 
+// This allows dynamic invocation of different functions with various arguments.
+typedef void (*iee_wrapper_func)(va_list args);
+iee_wrapper_func iee_wrappers[] = {
+    iee_wrapper_write_in_byte,
+    iee_wrapper_set_pte,
+    iee_wrapper_set_pmd,
+    iee_wrapper_set_p4d,
+    iee_wrapper_set_pud,
+    iee_wrapper_set_bm_pte,
+    iee_wrapper_set_swapper_pgd,
+    iee_wrapper_set_tramp_pgd,
+    (iee_wrapper_func)iee_wrapper_set_xchg,
+    (iee_wrapper_func)iee_wrapper_set_cmpxchg,
+    iee_wrapper_set_cred_uid,
+    iee_wrapper_set_cred_gid,
+    iee_wrapper_copy_cred,
+    iee_wrapper_set_cred_suid,
+    iee_wrapper_set_cred_sgid,
+    iee_wrapper_set_cred_euid,
+    iee_wrapper_set_cred_egid,
+    iee_wrapper_set_cred_fsuid,
+    iee_wrapper_set_cred_fsgid,
+    iee_wrapper_set_cred_user,
+    iee_wrapper_set_cred_user_ns,
+    iee_wrapper_set_cred_group_info,
+    iee_wrapper_set_cred_securebits,
+    iee_wrapper_set_cred_cap_inheritable,
+    iee_wrapper_set_cred_cap_permitted,
+    iee_wrapper_set_cred_cap_effective,
+    iee_wrapper_set_cred_cap_bset,
+    iee_wrapper_set_cred_cap_ambient,
+    iee_wrapper_set_cred_jit_keyring,
+    iee_wrapper_set_cred_session_keyring,
+    iee_wrapper_set_cred_process_keyring,
+    iee_wrapper_set_cred_thread_keyring,
+    iee_wrapper_set_cred_request_key_auth,
+    iee_wrapper_set_cred_non_rcu,
+    iee_wrapper_set_cred_atomic_set_usage,
+    (iee_wrapper_func)iee_wrapper_set_cred_atomic_op_usage,
+    iee_wrapper_set_cred_security,
+    iee_wrapper_set_cred_rcu,
+    iee_wrapper_memset,
+    iee_wrapper_set_track,
+    iee_wrapper_set_freeptr,
+	iee_wrapper_set_pte_upage,
+	iee_wrapper_set_pte_ppage,
+	iee_wrapper_set_token_mm,
+	iee_wrapper_set_token_pgd,
+	iee_wrapper_init_token,
+	iee_wrapper_free_token,
+	(iee_wrapper_func)iee_wrapper_read_token_stack,
+	iee_wrapper_write_entry_task,
+#ifdef CONFIG_KOI
+    (iee_wrapper_func)iee_wrapper_read_koi_stack,
+    iee_wrapper_write_koi_stack,
+    (iee_wrapper_func)iee_wrapper_read_token_ttbr1,
+    iee_wrapper_write_token_ttbr1,
+    (iee_wrapper_func)iee_wrapper_read_koi_kernel_stack,
+    iee_wrapper_write_koi_kernel_stack,
+    (iee_wrapper_func)iee_wrapper_read_koi_stack_base,
+    iee_wrapper_write_koi_stack_base
+#endif
+};
+
+u64 __iee_code iee_dispatch(int flag, ...){
+	va_list pArgs;
+
+    va_start(pArgs, flag);
+
+	switch(flag)
+	{
+		case IEE_OP_SET_CMPXCHG:
+		{
+			pteval_t ret = iee_wrapper_set_cmpxchg(pArgs);
+			va_end(pArgs);
+			return (u64)ret;
+		}
+		case IEE_OP_SET_XCHG:
+		{
+			pteval_t ret = iee_wrapper_set_xchg(pArgs);
+			va_end(pArgs);
+			return (u64)ret;
+		}
+		case IEE_OP_SET_CRED_ATOP_USAGE:
+		{
+			u64 ret = iee_wrapper_set_cred_atomic_op_usage(pArgs);
+			va_end(pArgs);
+			return ret;
+		}
+		case IEE_READ_TOKEN_STACK:
+		{
+			u64 ret = iee_wrapper_read_token_stack(pArgs);
+			va_end(pArgs);
+			return ret;
+		}
+#ifdef CONFIG_KOI
+        case IEE_READ_KOI_STACK:
+        {
+            u64 ret = iee_wrapper_read_koi_stack(pArgs);
+            va_end(pArgs);
+            return ret;
+        }
+        case IEE_READ_TOKEN_TTBR1:
+        {
+            u64 ret = iee_wrapper_read_token_ttbr1(pArgs);
+            va_end(pArgs);
+            return ret;
+        }
+        case IEE_READ_KOI_KERNEL_STACK:
+        {
+            u64 ret = iee_wrapper_read_koi_kernel_stack(pArgs);
+            va_end(pArgs);
+            return ret;
+        }
+        case IEE_READ_KOI_STACK_BASE:
+        {
+            u64 ret = iee_wrapper_read_koi_stack_base(pArgs);
+            va_end(pArgs);
+            return ret;
+        }
+#endif
+		default:
+		{
+        #ifndef CONFIG_KOI
+			if((flag < IEE_WRITE_IN_BYTE) | (flag > IEE_WRITE_ENTRY_TASK))
+				panic("Invalid iee flag.\n");
+        #else 
+            if((flag < IEE_WRITE_IN_BYTE) | (flag > IEE_WRITE_KOI_STACK_BASE))
+				panic("Invalid iee flag.\n");
+        #endif
+			iee_wrappers[flag](pArgs);
+			break;
+		}
+	}
+
+	va_end(pArgs);
+	return 0;
+}
+
+
+#ifdef CONFIG_KOI
+unsigned long __iee_code _iee_read_koi_stack(struct task_struct *tsk) 
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    return (unsigned long)token->koi_stack;
+}
+
+void __iee_code _iee_write_koi_stack(struct task_struct *tsk, unsigned long koi_stack) 
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    token->koi_stack = koi_stack;
+}
+
+unsigned long __iee_code _iee_read_token_ttbr1(struct task_struct *tsk)
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    return token->current_ttbr1;
+}
+
+void __iee_code _iee_write_token_ttbr1(struct task_struct *tsk, unsigned long current_ttbr1)
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    token->current_ttbr1 = current_ttbr1;
+}
+
+unsigned long __iee_code _iee_read_koi_kernel_stack(struct task_struct *tsk)
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    return token->koi_kernel_stack;
+}
+
+void __iee_code _iee_write_koi_kernel_stack(struct task_struct *tsk, unsigned long kernel_stack) 
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    token->koi_kernel_stack = kernel_stack;
+}
+
+unsigned long __iee_code _iee_read_koi_stack_base(struct task_struct *tsk) 
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    return (unsigned long)token->koi_stack_base;
+}
+
+void __iee_code _iee_write_koi_stack_base(struct task_struct *tsk, unsigned long koi_stack_base) 
+{
+    struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+    token->koi_stack_base = koi_stack_base;
+}
+#endif
+
+// Protect the __entry_task.
+__attribute__((aligned(PAGE_SIZE))) DECLARE_PER_CPU(struct task_struct *[PAGE_SIZE/sizeof(struct task_struct *)], __entry_task);
+void __iee_code _iee_write_entry_task(struct task_struct *tsk)
+{
+	// Add check of tsk.
+	struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+
+	unsigned long flags;
+	unsigned long res;
+	struct task_struct **entry_addr;
+	local_irq_save(flags);
+	asm volatile("at s1e1r, %0"::"r"(token));
+	isb();
+	res = read_sysreg(par_el1);
+	local_irq_restore(flags);
+
+	// If it is logical map, that means it is not a token.
+	if(__phys_to_iee(res & PTE_ADDR_MASK) == (((unsigned long)token) & PTE_ADDR_MASK))
+		panic("Trying to forge a token.\n");
+
+	if(!token->valid)
+		panic("Trying to write a wrong task into __entry_task.\n");
+	entry_addr = (struct task_struct **)__phys_to_iee(__pa(SHIFT_PERCPU_PTR(__entry_task,__kern_my_cpu_offset())));
+	*entry_addr = tsk;
+}
+
+unsigned long __iee_code _iee_read_token_stack(struct task_struct *tsk)
+{
+	struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+	return (unsigned long)token->iee_stack;
+}
+
+void __iee_code _iee_free_token(struct task_struct *tsk)
+{
+	_iee_memset(tsk, 0, sizeof(struct task_token));
+}
+
+#ifdef CONFIG_KOI
+extern unsigned long koi_swapper_ttbr1;
+#endif
+void __iee_code _iee_init_token(struct task_struct *tsk, void *kernel_stack, void *iee_stack)
+{
+	struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+	token->kernel_stack = kernel_stack;
+	token->iee_stack = iee_stack;
+	token->valid = true;
+#ifdef CONFIG_KOI
+    token->koi_kernel_stack = NULL;
+    token->koi_stack = NULL;
+    token->koi_stack_base = NULL;
+    token->current_ttbr1 = 0;
+#endif
+}
+
+void __iee_code _iee_set_token_mm(struct task_struct *tsk, struct mm_struct *mm)
+{
+	struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+	token->mm = mm;
+}
+
+void __iee_code _iee_set_token_pgd(struct task_struct *tsk, pgd_t *pgd)
+{
+	struct task_token *token = (struct task_token *)__phys_to_iee(__pa(tsk));
+	token->pgd = pgd;
+}
+
+void __iee_code _iee_set_freeptr(void **pptr, void *ptr)
+{
+	pptr = (void **)__phys_to_iee(__pa(pptr));
+	*pptr = ptr;
+}
+
+#pragma GCC push_options
+#pragma GCC optimize("O0")
+void __iee_code _iee_memset(void *ptr, int data, size_t n)
+{
+	char *_ptr = (char *)__phys_to_iee(__pa(ptr));
+
+	while (n--)
+		*_ptr++ = data;
+}
+
+void __iee_code _iee_memcpy(void *dst, void *src, size_t n)
+{
+	char *_dst = (char *)__phys_to_iee(__pa(dst));
+	char *_src = (char *)src;
+
+	while(n--)
+		*_dst++ = *_src++;
+}
+#pragma GCC pop_options
+
+void __iee_code _iee_set_track(struct track *ptr, struct track *data)
+{
+	_iee_memcpy(ptr, data, sizeof(struct track));
+}
+
+void __iee_code _iee_set_cred_rcu(struct cred *cred, struct rcu_head *rcu)
+{
+	if(cred == &init_cred)
+		cred = (struct cred *)__phys_to_iee(__pa_symbol(cred));
+	else
+		cred = (struct cred *)__phys_to_iee(__pa(cred));
+	#ifdef CONFIG_CREDP
+	*((struct rcu_head **)(&(cred->rcu.func))) = rcu;
+	#endif
+}
+
+void __iee_code _iee_set_cred_security(struct cred *cred, void *security)
+{
+	if(cred == &init_cred)
+		cred = (struct cred *)__phys_to_iee(__pa_symbol(cred));
+	else
+		cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->security = security;
+}
+
+bool __iee_code _iee_set_cred_atomic_op_usage(struct cred *cred, int flag)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	switch (flag)
+	{
+	case AT_INC: {
+		atomic_inc(&cred->usage);
+		return 0;
+	}
+	case AT_INC_NOT_ZERO: {
+		return atomic_inc_not_zero(&cred->usage);
+	}
+	case AT_DEC_AND_TEST: {
+		return atomic_dec_and_test(&cred->usage);
+	}
+	}
+	return 0;
+}
+
+void __iee_code _iee_set_cred_atomic_set_usage(struct cred *cred, int i)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	atomic_set(&cred->usage,i);
+}
+
+void __iee_code _iee_set_cred_non_rcu(struct cred *cred, int non_rcu)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->non_rcu = non_rcu;
+}
+
+void __iee_code _iee_set_cred_session_keyring(struct cred *cred, struct key *session_keyring)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->session_keyring = session_keyring;
+}
+
+void __iee_code _iee_set_cred_process_keyring(struct cred *cred, struct key *process_keyring)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->process_keyring = process_keyring;
+}
+
+void __iee_code _iee_set_cred_thread_keyring(struct cred *cred, struct key *thread_keyring)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->thread_keyring = thread_keyring;
+}
+
+void __iee_code _iee_set_cred_request_key_auth(struct cred *cred, struct key *request_key_auth)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->request_key_auth = request_key_auth;
+}
+
+void __iee_code _iee_set_cred_jit_keyring(struct cred *cred, unsigned char jit_keyring)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->jit_keyring = jit_keyring;
+}
+
+void __iee_code _iee_set_cred_cap_inheritable(struct cred *cred, kernel_cap_t cap_inheritable)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->cap_inheritable = cap_inheritable;
+}
+
+void __iee_code _iee_set_cred_cap_permitted(struct cred *cred, kernel_cap_t cap_permitted)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->cap_permitted = cap_permitted;
+}
+
+void __iee_code _iee_set_cred_cap_effective(struct cred *cred, kernel_cap_t cap_effective)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->cap_effective = cap_effective;
+}
+
+void __iee_code _iee_set_cred_cap_bset(struct cred *cred, kernel_cap_t cap_bset)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->cap_bset = cap_bset;
+}
+
+void __iee_code _iee_set_cred_cap_ambient(struct cred *cred, kernel_cap_t cap_ambient)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->cap_ambient = cap_ambient;
+}
+
+void __iee_code _iee_set_cred_securebits(struct cred *cred, unsigned securebits)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->securebits = securebits;
+}
+
+void __iee_code _iee_set_cred_group_info(struct cred *cred, struct group_info *group_info)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->group_info = group_info;
+}
+
+void __iee_code _iee_set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->user_ns = user_ns;
+}
+
+void __iee_code _iee_set_cred_user(struct cred *cred, struct user_struct *user)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->user = user;
+}
+
+void __iee_code _iee_set_cred_fsgid(struct cred *cred, kgid_t fsgid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->fsgid = fsgid;
+}
+
+void __iee_code _iee_set_cred_fsuid(struct cred *cred, kuid_t fsuid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->fsuid = fsuid;
+}
+
+void __iee_code _iee_set_cred_egid(struct cred *cred, kgid_t egid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->egid = egid;
+}
+
+void __iee_code _iee_set_cred_euid(struct cred *cred, kuid_t euid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->euid = euid;
+}
+
+void __iee_code _iee_set_cred_sgid(struct cred *cred, kgid_t sgid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->sgid = sgid;
+}
+
+void __iee_code _iee_set_cred_suid(struct cred *cred, kuid_t suid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->suid = suid;
+}
+
+void __iee_code _iee_copy_cred(struct cred *old, struct cred *new)
+{
+	#ifdef CONFIG_CREDP
+	struct rcu_head *rcu = (struct rcu_head *)(new->rcu.func);
+	struct cred *_new = (struct cred *)__phys_to_iee(__pa(new));
+	_iee_memcpy(new,old,sizeof(struct cred));
+	*(struct rcu_head **)(&(_new->rcu.func)) = rcu;
+	*(struct rcu_head *)(_new->rcu.func) = *(struct rcu_head *)(old->rcu.func);
+	#endif
+}
+
+void __iee_code _iee_set_cred_gid(struct cred *cred, kgid_t gid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->gid = gid;
+}
+
+void __iee_code _iee_set_cred_uid(struct cred *cred, kuid_t uid)
+{
+	cred = (struct cred *)__phys_to_iee(__pa(cred));
+	cred->uid = uid;
+}
+
+void __iee_code _iee_write_in_byte(void *ptr, __u64 data, int length)
+{
+	ptr = (void *)__phys_to_iee(__pa(ptr));
+	switch(length) {
+		case 8: {
+			*(__u64 *)ptr = data;
+			break;
+		}
+		case 4: {
+			*(__u32 *)ptr = (__u32)data;
+			break;
+		}
+		case 2: {
+			*(__u16 *)ptr = (__u16)data;
+			break;
+		}
+		case 1: {
+			*(__u8 *)ptr = (__u8)data;
+			break;
+		}
+	}
+}
+
+pteval_t __iee_code _iee_set_xchg_relaxed(pte_t *ptep, pteval_t pteval)
+{
+	pteval_t ret = xchg_relaxed((pteval_t *)(__phys_to_iee(__pa(ptep))), pteval);
+	return ret;
+}
+
+pteval_t __iee_code _iee_set_cmpxchg_relaxed(pte_t *ptep, pteval_t old_pteval, pteval_t new_pteval)
+{
+	pteval_t pteval = cmpxchg_relaxed((pteval_t *)(__phys_to_iee(__pa(ptep))), old_pteval, new_pteval);
+	return pteval;
+}
+
+/* Check if addr is allocated in IEE page */
+static inline bool check_addr_in_iee_valid(unsigned long addr) 
+{
+	unsigned long flags;
+	unsigned long res;
+	local_irq_save(flags);
+	asm volatile("at s1e1r, %0"::"r"(addr));
+	isb();
+	res = read_sysreg(par_el1);
+	local_irq_restore(flags);
+
+	// If it is not logical map, that means it is a token.
+	if(__phys_to_iee(res & PTE_ADDR_MASK) != addr)
+		return false;
+
+	return !(res & 0x1);
+}
+
+void __iee_code _iee_iee_set_tramp_pgd_pre_init(pgd_t *pgdp, pgd_t pgd)
+{
+	WRITE_ONCE(*((pgd_t *)(__phys_to_iee(__pa_symbol(pgdp)))), pgd);
+}
+
+void __iee_code _iee_set_swapper_pgd(pgd_t *pgdp, pgd_t pgd)
+{
+	if(!(pgd_val(pgd) & PMD_SECT_VALID))
+	{
+		WRITE_ONCE(*((pgd_t *)(__phys_to_iee(__pa_symbol(pgdp)))), pgd);
+		return;
+	}
+
+	if ((pgd_val(pgd) & PMD_TABLE_BIT) && !check_addr_in_iee_valid(__phys_to_iee(__pgd_to_phys(pgd))))
+		panic("You can't use non-iee-pgtable\n");
+
+	if((pgdp >= pgd_offset_pgd((pgd_t *)swapper_pg_dir, (unsigned long)0xffff4 << 44)) && (pgdp < pgd_offset_pgd((pgd_t *)swapper_pg_dir, (unsigned long)0xffff8 << 44)) && !(pgd_val(pgd) & PGD_APT))
+		panic("Set IEE pgd U page.\n");
+
+	if((pgd_val(pgd) & (PGD_PXN | PGD_UXN)))
+		panic("Set kernel pgd executable.\n");
+
+	WRITE_ONCE(*((pgd_t *)(__phys_to_iee(__pa_symbol(pgdp)))), pgd);
+}
+
+void __iee_code _iee_set_p4d(p4d_t *p4dp, p4d_t p4d)
+{
+	if(!(p4d_val(p4d) & PMD_SECT_VALID))
+	{
+		WRITE_ONCE(*((p4d_t *)(__phys_to_iee(__pa(p4dp)))), p4d);
+		return;
+	}
+
+	if ((p4d_val(p4d) & PMD_TABLE_BIT) && !check_addr_in_iee_valid(__phys_to_iee(__p4d_to_phys(p4d))))
+		panic("You can't use non-iee-pgtable\n");
+
+	WRITE_ONCE(*((p4d_t *)(__phys_to_iee(__pa(p4dp)))), p4d);
+}
+
+void __iee_code _iee_set_pud(pud_t *pudp, pud_t pud)
+{
+	if(!(pud_val(pud) & PMD_SECT_VALID))
+	{
+		WRITE_ONCE(*((pud_t *)(__phys_to_iee(__pa(pudp)))), pud);
+		return;
+	}
+
+	if ((pud_val(pud) & PMD_TABLE_BIT) && !check_addr_in_iee_valid(__phys_to_iee(__pud_to_phys(pud))))
+		panic("You can't use non-iee-pgtable\n");
+
+	WRITE_ONCE(*((pud_t *)(__phys_to_iee(__pa(pudp)))), pud);
+}
+
+// Return true if the modify does not break DEP.
+static inline bool check_pmd_dep(char *addr, pmd_t pmd)
+{
+	// DEP for kernel code and readonly data
+	// _text: .text start addr, __init_begin: .rodata end addr
+	if (addr >= _text && addr < _etext)
+	{
+		if ((PTE_WRITE & pmd_val(pmd)) || // DBM == 1 --> writable
+			!(PTE_RDONLY & pmd_val(pmd))) // DBM == 0 && AP[2] = 0 --> writable
+		{
+			panic("Can't make kernel's text/readonly page as writable!\n"
+					   "addr = 0x%16llx, pmd_val = 0x%16llx",
+				  (u64)addr, pmd_val(pmd));
+		}
+	}
+	return true;
+}
+
+// Return true if the pmd table is a part of kernel page table.
+// TODO : Optimize to get lower overhead.
+static inline bool is_kernel_pmd_table(pmd_t *pmdp, pmd_t pmd)
+{
+	int i = 0,j = 0;
+	for(i = 0; i < PAGE_SIZE/sizeof(pgd_t); i++)
+	{
+		pgd_t *pgdp = (pgd_t *)swapper_pg_dir + i;
+		if((pgd_val(*pgdp) & PMD_SECT_VALID) && (pgd_val(*pgdp) & PMD_TABLE_BIT))
+		{
+			for(j = 0; j < PAGE_SIZE/sizeof(pud_t); j++)
+			{
+				pud_t *pudp = (pud_t *)__va(__pgd_to_phys(*pgdp)) + i;
+				if((pud_val(*pudp) & PMD_SECT_VALID) && (pud_val(*pudp) & PMD_TABLE_BIT))
+				{
+					pmd_t *current_pmdp = __va(__pud_to_phys(*pudp));
+					if((unsigned long)current_pmdp == ((unsigned long)pmdp & PAGE_MASK))
+						return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+// Return true if it is mapped to a physical range containing IEE page.
+// TODO : Optimize to get lower overhead.
+static inline bool check_addr_range_in_iee_valid(pmd_t pmd)
+{
+	int i = 0;
+	unsigned long addr = __phys_to_iee(__pmd_to_phys(pmd));
+	for(i = 0; i < PAGE_SIZE/sizeof(pmd_t); i++)
+	{
+		if(check_addr_in_iee_valid(addr + PAGE_SIZE * i))
+			return true;
+	}
+	return false;
+}
+
+void __iee_code _iee_set_pmd(pmd_t *pmdp, pmd_t pmd)
+{
+	char * addr = (char *)__phys_to_kimg(__pmd_to_phys(pmd));
+
+	if(!(pmd_val(pmd) & PMD_SECT_VALID))
+	{
+		WRITE_ONCE(*((pmd_t *)(__phys_to_iee(__pa(pmdp)))), pmd);
+		return;
+	}
+
+	// Check if the pte table is legally allocated.
+	if ((pmd_val(pmd) & PMD_TABLE_BIT) && !check_addr_in_iee_valid(__phys_to_iee(__pmd_to_phys(pmd))))
+		panic("You can't use non-iee-pgtable\n");
+
+	// Avoid mapping a huge pmd as U page.
+	if(!(pmd_val(pmd) & PMD_TABLE_BIT) && (pmd_val(pmd) & PMD_SECT_USER) && is_kernel_pmd_table(pmdp, pmd))
+		panic("Set a block descriptor in kernel space U page.\n");
+
+	// Avoid mapping a huge pmd to IEE physical page.
+	if(!(pmd_val(pmd) & PMD_TABLE_BIT) && check_addr_range_in_iee_valid(pmd))
+		panic("Mapping IEE physical page to a huge pmd.\n");
+
+	if(!check_pmd_dep(addr, pmd))
+		return;
+
+	WRITE_ONCE(*((pmd_t *)(__phys_to_iee(__pa(pmdp)))), pmd);
+}
+
+// Return true if the pte table is a part of kernel page table.
+// TODO : Optimize to get lower overhead.
+static inline bool is_kernel_pte_table(pte_t *ptep, pte_t pte)
+{
+	return false;
+}
+
+// Return true if it does not change the privilage or add new U page in kernel.
+static inline bool check_privilage_safe(pte_t *ptep, pte_t pte)
+{
+	if(!(pte_val(pte) & PTE_VALID))
+		return true;
+	
+	if((pte_val(*ptep) & PTE_VALID))
+	{
+		if((pte_val(*ptep) & PTE_USER) != (pte_val(pte) & PTE_USER))
+			panic("Incorrectly change privilage.\n");
+	}
+	else
+	{
+		if((pte_val(pte) & PTE_USER) && is_kernel_pte_table(ptep, pte))
+			panic("Add new U page in kernel space.\n");
+	}
+	return true;
+}
+
+// TODO : When adding a new executable page, check it for DEP.
+static inline bool safely_adding_new_exec_page(pte_t *ptep, pte_t pte)
+{
+	return true;
+}
+
+// Return true if it is only changing prot of a pte.
+static inline bool is_changing_pte_prot(pte_t *ptep, pte_t pte)
+{
+	if(((pte_val(*ptep) ^ pte_val(pte)) & PTE_ADDR_MASK) == 0)
+		return true;
+	else
+		return false;
+}
+
+// Return true if the modify does not break DEP.
+static inline bool check_pte_dep(char *addr, pte_t pte)
+{
+	// DEP for kernel code and readonly data
+	// _text: .text start addr, __init_begin: .rodata end addr
+	if (addr >= _text && addr < _etext)
+	{
+		if ((PTE_WRITE & pte_val(pte)) // DBM == 1 --> writable
+			|| !(PTE_RDONLY & pte_val(pte))) // DBM == 0 && AP[2] = 0 --> writable
+		{
+			panic("Can't make kernel's text/readonly page as writable!\n"
+					   "addr = 0x%16llx, pte_val = 0x%16llx",
+				  (u64)addr, pte_val(pte));
+		}
+	}
+	return true;
+}
+
+void __iee_code _iee_set_pte(pte_t *ptep, pte_t pte)
+{
+	char * addr = (char *)__phys_to_kimg(__pte_to_phys(pte));
+
+	if(!(pte_val(pte) & PTE_VALID))
+	{
+		WRITE_ONCE(*((pte_t *)(__phys_to_iee(__pa(ptep)))), pte);
+		return;
+	}
+
+	// Avoid modify privilage unsafely.
+	if(!check_privilage_safe(ptep, pte))
+		panic("You are modify privilage unsafely.\n");
+
+	// Avoid mapping a new executable page.
+	if(!safely_adding_new_exec_page(ptep, pte))
+		panic("You are adding a new executable page unsafely.\n");
+
+	// Avoid mapping a new VA to IEE PA.
+	if(!is_changing_pte_prot(ptep, pte) && 
+	   check_addr_in_iee_valid(__phys_to_iee(__pte_to_phys(pte))))
+		panic("You are remmaping IEE page to other VA.\n");
+
+	// Avoid mapping a writable VA to kernel code PA.
+	if(!check_pte_dep(addr, pte))
+		return;
+
+	WRITE_ONCE(*((pte_t *)(__phys_to_iee(__pa(ptep)))), pte);
+}
+
+// Return true if it only sets U page and modify NG.
+static inline bool is_setting_upage(pte_t *ptep, pte_t pte)
+{
+	if(((pte_val(*ptep) ^ pte_val(pte)) & ~(PTE_USER | PTE_NG)) != 0)
+		panic("Incorrectly setting U page.\n");
+	if((pte_val(pte) & PTE_USER) != PTE_USER)
+		panic("Using error interface to set P page.\n");
+	return true;
+}
+
+void __iee_code _iee_set_pte_upage(pte_t *ptep, pte_t pte)
+{
+	// Check if it only change the prot.
+	if(!is_setting_upage(ptep,pte))
+		panic("Incorrectly setting U page.\n");
+
+	WRITE_ONCE(*((pte_t *)(__phys_to_iee(__pa(ptep)))), pte);
+}
+
+// Return true if it only sets P page and modify NG.
+static inline bool is_setting_ppage(pte_t *ptep, pte_t pte)
+{
+	if(((pte_val(*ptep) ^ pte_val(pte)) & ~(PTE_USER | PTE_NG)) != 0)
+		panic("Incorrectly setting P page.\n");
+	if((pte_val(pte) & PTE_USER) != 0)
+		panic("Using error interface to set U page.\n");
+	return true;
+}
+
+void __iee_code _iee_set_pte_ppage(pte_t *ptep, pte_t pte)
+{
+	// Check if it only change the prot.
+	if(!is_setting_ppage(ptep,pte))
+		panic("Incorrectly setting P page.\n");
+
+	WRITE_ONCE(*((pte_t *)(__phys_to_iee(__pa(ptep)))), pte);
+}
+
+void __iee_code _iee_set_bm_pte(pte_t *ptep, pte_t pte)
+{
+	WRITE_ONCE(*((pte_t *)(__phys_to_iee(__pa_symbol(ptep)))), pte);
+}
+
+/* Data in iee_si_base is visible to all pgd while iee_si_data is private. */
+unsigned long iee_base_swapper_pg_dir __iee_si_base;
+unsigned long iee_base_iee_pg_dir __iee_si_base;
+unsigned long iee_base_idmap_pg_dir __iee_si_data;
+unsigned long iee_base_reserved_pg_dir __iee_si_data;
+unsigned long iee_base__bp_harden_el1_vectors __iee_si_data;
+bool iee_init_done __iee_si_data;
+
+DEFINE_PER_CPU(unsigned long, iee_si_user_bvr0);
+DEFINE_PER_CPU(unsigned long, iee_si_user_bcr0);
+EXPORT_SYMBOL(iee_si_user_bvr0);
+EXPORT_SYMBOL(iee_si_user_bcr0);
+
+// unsigned long iee_base_kimage_voffset __iee_si_base;
+// unsigned long iee_base_memstart_addr __iee_si_data;
+// /* replace __pa macro inside IEE rwx gate. */
+// #define __iee_si_is_lm_address(addr)	(((u64)(addr) ^ PAGE_OFFSET) < (PAGE_END - PAGE_OFFSET))
+// #define __iee_si_lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + iee_base_memstart_addr)
+// #define __iee_si_kimg_to_phys(addr)	((addr) - iee_base_kimage_voffset)
+
+// #define __iee_si_virt_to_phys(x) ({					
+//     __iee_si_is_lm_address(x) ? __iee_si_lm_to_phys(x) : __iee_si_kimg_to_phys(x);	
+// })
+// #define __iee_si_pa(x)			__iee_si_virt_to_phys((unsigned long)(x))
+
+static u64 __iee_si_code inline iee_si_mask(unsigned long mask, unsigned long new_val, unsigned long old_val)
+{
+    return (new_val & mask) | (old_val & ~mask);
+}
+/* 
+ * handler function for requests of executing sensitive instrutions. 
+ */
+u64 __iee_si_code iee_si_handler(int flag, ...)
+{
+    va_list pArgs;
+    u64 old_val, new_val;
+
+	// BUG_ON(flag > IEE_WRITE_MDSCR);
+    va_start(pArgs, flag);
+    switch (flag) {
+		case IEE_SI_TEST:
+			break;
+        case IEE_WRITE_SCTLR: {
+            old_val = read_sysreg(sctlr_el1);
+            new_val = va_arg(pArgs, u64);
+            new_val = iee_si_mask(IEE_SCTLR_MASK, new_val, old_val);
+            write_sysreg(new_val, sctlr_el1);
+            break;
+        }
+        case IEE_WRITE_TTBR0: 
+        case IEE_CONTEXT_SWITCH: {
+            u64 new_asid, new_phys, old_phys, token_phys;
+            struct task_struct *tsk;
+	        struct task_token *token;
+            new_val = va_arg(pArgs, u64);
+            new_phys = (new_val & PAGE_MASK) & ~TTBR_ASID_MASK;
+            new_asid = new_val >> 48;
+
+            // Check ASID first
+            if (new_asid!=0 && (new_asid == 1 || new_asid % 2 ==0))
+                panic("IEE SI warning: TTBR0 ASID invalid: %llx:%llx", new_asid, new_val);
+            // TO DO: operations to protect idmap_pg_dir
+            if (new_phys == iee_base_idmap_pg_dir)
+            {
+				printk("IEE SI: switch to idmap_pg_dir.");
+            }
+
+            /* Skip verification if iee hasn't been initialized. */
+            if (iee_init_done){
+                // Verify current sp_el0 with iee token info
+                asm volatile("mrs %x0, sp_el0":"=r"(tsk));
+                token = (struct task_token *)__phys_to_iee(__pa(tsk));
+                
+                /* 
+                 * token->pgd != NULL means it is a user task, then we need to check whether current ttbr0 is correct.
+                 */ 
+                if (token->pgd){
+                    old_val = read_sysreg(ttbr0_el1);
+                    // When TTBR0 is reserved_pg_dir then no checking is available.
+                    if (old_val != iee_base_reserved_pg_dir){
+						old_phys = (old_val & PAGE_MASK) & ~TTBR_ASID_MASK;
+                        token_phys = __pa(token->pgd);
+                        if (old_phys != token_phys)
+                            panic("IEE SI warning: Pgd set error. old ttbr0:%lx, token ttbr0:%lx, token pgd:%lx", 
+                                (unsigned long)old_phys, (unsigned long)token_phys, (unsigned long)(token->pgd));
+                    }
+                }
+            }
+            // all checks are done.
+            write_sysreg(new_val, ttbr0_el1);
+            break;
+        }
+        case IEE_WRITE_VBAR: {
+            u64 el1_vector;
+            new_val = va_arg(pArgs, u64);
+            el1_vector = iee_base__bp_harden_el1_vectors;
+            if(new_val == el1_vector || new_val == el1_vector+SZ_2K || 
+                    new_val == el1_vector+SZ_2K*2 || new_val == el1_vector+SZ_2K*3)
+                write_sysreg(new_val, vbar_el1);
+            break;
+        }
+        case IEE_WRITE_TCR: {
+            old_val = read_sysreg(tcr_el1);
+            new_val = va_arg(pArgs, u64);
+            new_val = iee_si_mask(IEE_TCR_MASK, new_val, old_val);
+            write_sysreg(new_val, tcr_el1);
+            break;
+        }
+        case IEE_WRITE_MDSCR: {
+            old_val = read_sysreg(mdscr_el1);
+            new_val = va_arg(pArgs, u64);
+            new_val = iee_si_mask(IEE_MDSCR_MASK, new_val, old_val);
+            write_sysreg(new_val, mdscr_el1);
+            break;
+        }
+        /* To protect IEE rwx gate by DBG Breakpoint 0 */
+        case IEE_WRITE_AFSR0: {
+            u64 user_bcr0;
+            /* iee_si_user_bcr0 = 0 means user proc doesn't use breakpoint 0 */
+            user_bcr0 = this_cpu_read(iee_si_user_bcr0);
+            if (user_bcr0 == 0)
+                write_sysreg(1, afsr0_el1);
+            else
+                write_sysreg(0, afsr0_el1);
+            break;
+        }
+    }
+    va_end(pArgs);
+	return 0;
+}
+/* 
+ * TODO: scan a page to check whether it contains sensitive instructions 
+ * return 1 when finding sensitive inst, 0 on safe page.
+ */
+int iee_si_scan_page(unsigned long addr);
+#endif
\ No newline at end of file
diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 60456a62da11..6323369f1f4f 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -26,7 +26,9 @@
 DEFINE_PER_CPU(struct nmi_ctx, nmi_contexts);
 
 DEFINE_PER_CPU(unsigned long *, irq_stack_ptr);
-
+#ifdef CONFIG_KOI
+EXPORT_SYMBOL(irq_stack_ptr);
+#endif
 #ifdef CONFIG_VMAP_STACK
 static void init_irq_stacks(void)
 {
diff --git a/arch/arm64/kernel/koi/Makefile b/arch/arm64/kernel/koi/Makefile
new file mode 100644
index 000000000000..9be8710b714a
--- /dev/null
+++ b/arch/arm64/kernel/koi/Makefile
@@ -0,0 +1 @@
+obj-y += koi.o
\ No newline at end of file
diff --git a/arch/arm64/kernel/koi/koi.c b/arch/arm64/kernel/koi/koi.c
new file mode 100644
index 000000000000..716ba16ab358
--- /dev/null
+++ b/arch/arm64/kernel/koi/koi.c
@@ -0,0 +1,1327 @@
+#include "asm/koi.h"
+#include "linux/compiler_attributes.h"
+#include "linux/compiler_types.h"
+#include "asm/barrier.h"
+#include "asm-generic/bug.h"
+#include "asm-generic/errno-base.h"
+#include "asm-generic/memory_model.h"
+#include "asm-generic/pgtable-nop4d.h"
+#include "asm-generic/rwonce.h"
+#include "asm/pgalloc.h"
+#include "asm/memory.h"
+#include "linux/bitfield.h"
+#include "linux/compiler.h"
+#include "linux/types.h"
+#include "linux/spinlock.h"
+#include "linux/spinlock_types.h"
+#include "linux/kernel.h"
+#include "linux/rculist.h"
+#include "linux/rcupdate.h"
+#include "linux/list.h"
+#include "asm/current.h"
+#include "linux/compiler_types.h"
+#include "asm-generic/barrier.h"
+#include "asm-generic/rwonce.h"
+#include "asm-generic/pgalloc.h"
+#include "asm/cpufeature.h"
+#include "asm/kvm_hyp.h"
+#include "asm/mmu.h"
+#include "asm/mmu_context.h"
+#include "asm/page-def.h"
+#include "asm/pgalloc.h"
+#include "asm/pgtable-hwdef.h"
+#include "asm/pgtable-types.h"
+#include "asm/pgtable.h"
+#include "asm/string.h"
+#include "asm/sysreg.h"
+#include "linux/bitfield.h"
+#include "linux/compiler.h"
+#include "linux/export.h"
+#include "linux/gfp.h"
+#include "linux/huge_mm.h"
+#include "linux/kallsyms.h"
+#include "linux/kconfig.h"
+#include "linux/kern_levels.h"
+#include "linux/kernel.h"
+#include "linux/list.h"
+#include "linux/lockdep.h"
+#include "linux/mm.h"
+#include "linux/mm_types.h"
+#include "linux/pgtable.h"
+#include "linux/printk.h"
+#include "linux/rculist.h"
+#include "linux/rcupdate.h"
+#include "linux/rmap.h"
+#include "linux/sched.h"
+#include "linux/stddef.h"
+#include "linux/string.h"
+#include "linux/swap.h"
+#include "linux/swapops.h"
+#include "linux/types.h"
+#include "linux/slab.h"
+#include "linux/string.h"
+#include "linux/hashtable.h"
+
+#define __koi_code __section(".koi.text")
+#define __koi_data __section(".data..koi")
+
+extern unsigned long __koi_code_start[];
+extern unsigned long __koi_code_end[];
+extern unsigned long __koi_data_start[];
+extern unsigned long __koi_data_end[];
+#ifdef CONFIG_IEE
+extern unsigned long __iee_si_base_start[];
+extern unsigned long __iee_exec_entry_start[];
+extern unsigned long __iee_exec_entry_end[];
+#endif
+
+__koi_data unsigned long koi_swapper_ttbr1 = 0;
+EXPORT_SYMBOL(koi_swapper_ttbr1);
+#define KOI_SWAPPER_MASK 0x0000fffffffffff0
+
+__attribute__((aligned(PAGE_SIZE)))
+DEFINE_PER_CPU(unsigned long[PAGE_SIZE / sizeof(unsigned long)],
+	       koi_irq_current_ttbr1);
+EXPORT_SYMBOL(koi_irq_current_ttbr1);
+
+extern void koi_switch_to_ko_stack(unsigned long stack_top);
+extern void init_ko_mm(struct mm_struct *ko_mm, pgd_t *pgdp);
+extern void koi_check_and_switch_context(struct mm_struct *mm);
+extern int koi_add_page_mapping(unsigned long dst, unsigned long src);
+extern unsigned long _iee_read_token_ttbr1(struct task_struct *tsk);
+/**
+*struct koi_mem_list - maintain a linked list of free memory in the kernel
+*@addr: stating address of this memory
+*@size: the size of the memory
+*@list: the head of the koi_mem_list
+*@rcu: for rcu 
+*/
+struct koi_mem_list {
+	unsigned long addr;
+	unsigned long size;
+	struct list_head list;
+	struct rcu_head rcu;
+};
+//mapping parameter pointer to copy
+struct koi_addr_map {
+	unsigned long buffer_addr;
+	unsigned long orig_addr;
+	int offset;
+	struct hlist_node node;
+	struct rcu_head rcu;
+};
+
+DEFINE_HASHTABLE(koi_mem_htbl, HASH_TABLE_BIT);
+EXPORT_SYMBOL(koi_mem_htbl);
+DEFINE_SPINLOCK(koi_mem_htbl_spin_lock);
+EXPORT_SYMBOL(koi_mem_htbl_spin_lock);
+
+EXPORT_SYMBOL(koi_do_switch_to_ko_stack);
+EXPORT_SYMBOL(koi_do_switch_to_kernel_stack);
+
+extern unsigned long long iee_rw_gate(int flag, ...);
+
+/**
+* koi_ttbr_ctor - return ttbr1 for the given driver module
+*/
+unsigned long koi_ttbr_ctor(struct module *mod)
+{
+	struct koi_mem_hash_node *ko;
+	struct mm_struct *ko_mm;
+	unsigned long ttbr1;
+	unsigned long asid;
+	int bkt;
+	rcu_read_lock();
+	hash_for_each_rcu (koi_mem_htbl, bkt, ko, node) {
+		if (ko->mod == mod) {
+			ko_mm = ko->ko_mm;
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (!ko_mm) {
+		printk(KERN_ERR "cannot found module %s in koi_mem_htbl",
+		       mod->name);
+		return 0;
+	}
+    asm volatile("mrs %0, ttbr0_el1\n":"=r"(asid):);
+    asid &= TTBR_ASID_MASK;
+    ttbr1 = ko->ko_ttbr1 | asid;
+	// koi_check_and_switch_context(ko_mm);
+	// asid = ASID(ko_mm);
+	// ttbr1 = ko->ko_ttbr1 | FIELD_PREP(TTBR_ASID_MASK, asid);
+	return ttbr1;
+}
+EXPORT_SYMBOL(koi_ttbr_ctor);
+//release the hash node
+static __maybe_unused void koi_mem_hash_node_free(struct rcu_head *rcu)
+{
+	struct koi_mem_hash_node *node =
+		container_of(rcu, struct koi_mem_hash_node, rcu);
+	kfree(node);
+}
+//release free memory linked list nodes
+static void koi_mem_node_free(struct rcu_head *rcu)
+{
+	struct koi_mem_list *mem_node =
+		container_of(rcu, struct koi_mem_list, rcu);
+	kfree(mem_node);
+}
+//release the node in koi_addr_map
+static void koi_addr_map_node_free(struct rcu_head *rcu)
+{
+	struct koi_addr_map *addr_map_node =
+		container_of(rcu, struct koi_addr_map, rcu);
+	kfree(addr_map_node);
+}
+
+#ifndef CONFIG_IEE
+/*
+ * This function is used to switch to ko's pgtable.
+ */
+__koi_code noinline unsigned long koi_do_switch_to_ko_pgtbl(void)
+{
+	struct koi_mem_hash_node *ko;
+	// struct mm_struct *ko_mm;
+	unsigned long addr;
+	unsigned long ttbr1, asid;
+	unsigned long *ptr;
+	struct task_token *token_addr =
+		(struct task_token *)((unsigned long)current +
+				      (unsigned long)koi_offset);
+	int bkt;
+	asm volatile(" mrs %0, elr_el1\n" : "=r"(addr));
+	ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1, __kern_my_cpu_offset());
+	rcu_read_lock();
+	hash_for_each_rcu (koi_mem_htbl, bkt, ko, node) {
+		if (ko->mod->init_layout.base != NULL) {
+			if (addr >= (unsigned long)ko->mod->init_layout.base &&
+			    addr < (unsigned long)(ko->mod->init_layout.base +
+						   ko->mod->init_layout.size)) {
+				if (token_addr->current_ttbr1 == ko->ko_ttbr1 ||
+				    *ptr == ko->ko_ttbr1) {
+					// ko_mm = ko->ko_mm;
+					// koi_check_and_switch_context(ko_mm);
+                    // asid = ASID(ko_mm);
+				    // ttbr1 = ko->ko_ttbr1;
+				    // ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
+                    asm volatile("mrs %0, ttbr0_el1\n":"=r"(asid):);
+                    asid &= TTBR_ASID_MASK;
+					ttbr1 = ko->ko_ttbr1 | asid;
+                    rcu_read_unlock();
+					return ttbr1;
+				}
+                rcu_read_unlock();
+				return 0;
+			}
+		}
+		if (addr >= (unsigned long)ko->mod->core_layout.base &&
+		    addr < (unsigned long)ko->mod->core_layout.base +
+				    ko->mod->core_layout.size) {
+			if (token_addr->current_ttbr1 == ko->ko_ttbr1 ||
+			    *ptr == ko->ko_ttbr1) {
+				// ko_mm = ko->ko_mm;
+				// koi_check_and_switch_context(ko_mm);
+				// asid = ASID(ko_mm);
+				// ttbr1 = ko->ko_ttbr1;
+				// ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
+                asm volatile("mrs %0, ttbr0_el1\n":"=r"(asid):);
+                asid &= TTBR_ASID_MASK;
+                ttbr1 = ko->ko_ttbr1 | asid;
+                rcu_read_unlock();
+				return ttbr1;
+			}
+            rcu_read_unlock();
+			return 0;
+		}
+	}
+	rcu_read_unlock();
+	return 0;
+}
+/**
+* koi_do_switch_to_kernel_pgtbl - switch to kernel pagetable
+*/
+__koi_code noinline int koi_do_switch_to_kernel_pgtbl(void)
+{
+	unsigned long curr_ttbr1, asid;
+	// if (!cpu_online(smp_processor_id()))
+	//     return 0;
+	asm volatile("	mrs %0, ttbr1_el1\n" : "=r"(curr_ttbr1));
+	if ((curr_ttbr1 & KOI_SWAPPER_MASK) ==
+	    (koi_swapper_ttbr1 & KOI_SWAPPER_MASK)) {
+		return 0;
+	}
+	if (((curr_ttbr1 & TTBR_ASID_MASK) >> 48) <= 1) {
+		return 0;
+	}
+    asm volatile("mrs %0, ttbr0_el1\n":"=r"(asid):);
+    asid &= ~USER_ASID_FLAG;
+    asid &= TTBR_ASID_MASK;
+	write_sysreg(koi_swapper_ttbr1 | asid, ttbr1_el1);
+	isb();
+	asm volatile(ALTERNATIVE("nop; nop; nop", "ic iallu; dsb nsh; isb",
+				 ARM64_WORKAROUND_CAVIUM_27456));
+	return 1;
+}
+#else
+__koi_code noinline unsigned long koi_do_switch_to_ko_pgtbl(void)
+{
+	struct koi_mem_hash_node *ko;
+	struct mm_struct *ko_mm;
+	unsigned long addr, ttbr1, asid, pan_flag, current_ttbr1;
+    unsigned long *ptr;
+	int bkt;
+	asm volatile("mrs %0, pan\n"
+		     "msr pan, 0x0\n"
+		     : "=r"(pan_flag)
+		     :);
+	current_ttbr1 = _iee_read_token_ttbr1(current);
+	asm volatile("msr pan, %0\n" : : "r"(pan_flag));
+    ptr = SHIFT_PERCPU_PTR(koi_irq_current_ttbr1, __kern_my_cpu_offset());
+	if (current_ttbr1 == 0 && *ptr == 0)
+		return 0;
+	asm volatile(" mrs %0, elr_el1\n" : "=r"(addr));
+	rcu_read_lock();
+	hash_for_each_rcu (koi_mem_htbl, bkt, ko, node) {
+		if (ko->mod->init_layout.base != NULL) {
+			if (addr >= (unsigned long)ko->mod->init_layout.base &&
+			    addr < (unsigned long)(ko->mod->init_layout.base +
+						   ko->mod->init_layout.size)) {
+				rcu_read_unlock();
+				if (current_ttbr1 == ko->ko_ttbr1 || *ptr == ko->ko_ttbr1) {
+					// ko_mm = ko->ko_mm;
+					// koi_check_and_switch_context(ko_mm);
+					// asid = ASID(ko_mm);
+					// ttbr1 = ko->ko_ttbr1;
+					// ttbr1 |= FIELD_PREP(TTBR_ASID_MASK,
+					// 		    asid);
+					return ko->ko_ttbr1;
+				}
+				return 0;
+			}
+		}
+		if (addr >= (unsigned long)ko->mod->core_layout.base &&
+		    addr < (unsigned long)ko->mod->core_layout.base +
+				    ko->mod->core_layout.size) {
+			rcu_read_unlock();
+			if (current_ttbr1 == ko->ko_ttbr1 || *ptr == ko->ko_ttbr1) {
+				// ko_mm = ko->ko_mm;
+				// koi_check_and_switch_context(ko_mm);
+				// asid = ASID(ko_mm);
+				// ttbr1 = ko->ko_ttbr1;
+				// ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
+				return ko->ko_ttbr1;
+			}
+			return 0;
+		}
+	}
+	rcu_read_unlock();
+	return 0;
+}
+
+__koi_code noinline int koi_do_switch_to_kernel_pgtbl(void)
+{
+	unsigned long curr_ttbr1;
+	// if (!cpu_online(smp_processor_id()))
+	//     return 0;
+	asm volatile("	mrs %0, ttbr1_el1\n" : "=r"(curr_ttbr1));
+	if ((curr_ttbr1 & KOI_SWAPPER_MASK) ==
+	    (koi_swapper_ttbr1 & KOI_SWAPPER_MASK)) {
+		return 0;
+	}
+	if (((curr_ttbr1 & TTBR_ASID_MASK) >> 48) <= 1) {
+		return 0;
+	}
+	iee_rwx_gate_entry(IEE_SWITCH_TO_KERNEL);
+	return 1;
+}
+#endif
+/**
+* koi_save_ttbr - save ttbr of each driver module
+* @mod: driver module
+* @pgdp:pointer to driver module top page table,pgd
+*/
+static void koi_save_ttbr(struct module *mod, pgd_t *pgdp,
+			  struct koi_mem_hash_node *node)
+{
+	phys_addr_t ttbr1 = phys_to_ttbr(virt_to_phys(pgdp));
+	if (system_supports_cnp())
+		ttbr1 |= TTBR_CNP_BIT;
+	node->ko_ttbr1 = ttbr1;
+}
+/**
+*kio_normal_page - to obtain the pointer of the corresponding struct page structure
+*from a given page table entry(pte) 
+*/
+struct page *koi_normal_page(pte_t pte)
+{
+	unsigned long pfn = pte_pfn(pte);
+
+	if (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {
+		if (likely(!pte_special(pte)))
+			goto check_pfn;
+		if (is_zero_pfn(pfn)) {
+			printk(KERN_ERR "zero pfn found! pte=0x%16lx\n", pte);
+			return NULL;
+		}
+		if (pte_devmap(pte)) {
+			printk(KERN_ERR "pte for dev found! pte=0x%16lx\n",
+			       pte);
+			return NULL;
+		}
+		return NULL;
+	}
+
+check_pfn:
+	return pfn_to_page(pfn);
+}
+
+/**
+ * Copy one pte. Returns 0 if succeeded, or -EAGAIN if one preallocated page 
+ * is required to copy this pte.
+*/
+static inline int koi_copy_present_pte(pte_t *dst_pte, pte_t *src_pte,
+				       unsigned long addr,
+				       struct page **prealloc)
+{
+	pte_t pte = *src_pte;
+	struct page *page;
+
+	page = koi_normal_page(pte);
+	if (!page) {
+		printk(KERN_ERR "pte_page unavailable. Impossible.....\n");
+		return -1;
+	}
+
+	set_pte(dst_pte, pte);
+	return 0;
+}
+/**
+* copy huge pmd from kernel space to driver space. 
+*/
+static int koi_copy_huge_pmd(struct mm_struct *ko_mm, pmd_t *dst_pmd,
+			     pmd_t *src_pmd, unsigned long addr)
+{
+	spinlock_t *src_ptl;
+	pmd_t pmd;
+	int ret = -ENOMEM;
+
+	src_ptl = pmd_lockptr(&init_mm, src_pmd);
+	spin_lock_bh(src_ptl);
+
+	ret = -EAGAIN;
+	pmd = *src_pmd;
+
+	set_pte((pte_t *)dst_pmd, pmd_pte(pmd));
+	ret = 0;
+	spin_unlock_bh(src_ptl);
+	return ret;
+}
+
+int __koi_pte_alloc(struct mm_struct *mm, pmd_t *pmd)
+{
+	spinlock_t *ptl;
+	pgtable_t new = pte_alloc_one(mm);
+	if (!new)
+		return -ENOMEM;
+
+	/*
+	 * Ensure all pte setup (eg. pte page lock and page clearing) are
+	 * visible before the pte is made visible to other CPUs by being
+	 * put into page tables.
+	 *
+	 * The other side of the story is the pointer chasing in the page
+	 * table walking code (when walking the page table without locking;
+	 * ie. most of the time). Fortunately, these data accesses consist
+	 * of a chain of data-dependent loads, meaning most CPUs (alpha
+	 * being the notable exception) will already guarantee loads are
+	 * seen in-order. See the alpha page table accessors for the
+	 * smp_rmb() barriers in page table walking code.
+	 */
+	smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
+
+	ptl = pmd_lockptr(mm, pmd);
+    spin_lock_bh(ptl);
+	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
+		#ifdef CONFIG_PTP
+		pte_t *pte = (pte_t *)page_address(new);
+		unsigned long iee_addr = __phys_to_iee(__pa(pte));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pte);
+		#endif
+        mm_inc_nr_ptes(mm);
+		pmd_populate(mm, pmd, new);
+		new = NULL;
+	}
+	spin_unlock_bh(ptl);
+	if (new)
+		pte_free(mm, new);
+	return 0;
+}
+
+#define koi_pte_alloc(mm, pmd) (unlikely(pmd_none(*(pmd))) && __koi_pte_alloc(mm, pmd))
+
+#define koi_pte_offset_map_lock(mm, pmd, address, ptlp)	\
+({							\
+	spinlock_t *__ptl = pte_lockptr(mm, pmd);	\
+	pte_t *__pte = pte_offset_map(pmd, address);	\
+	*(ptlp) = __ptl;				\
+	spin_lock_bh(__ptl);				\
+	__pte;						\
+})
+
+#define koi_pte_alloc_map_lock(mm, pmd, address, ptlp)	\
+	(koi_pte_alloc(mm, pmd) ?			\
+		 NULL : koi_pte_offset_map_lock(mm, pmd, address, ptlp))
+
+/**
+*koi_copy_pte_range - copy pte from kernel space to driver space
+*/
+static int koi_copy_pte_range(struct mm_struct *ko_mm, pmd_t *dst_pmd,
+			      pmd_t *src_pmd, unsigned long addr,
+			      unsigned long end)
+{
+	pte_t *src_pte, *dst_pte;
+	spinlock_t *src_ptl, *dst_ptl;
+	int ret = 0;
+	struct page *prealloc = NULL;
+again:
+	dst_pte = koi_pte_alloc_map_lock(ko_mm, dst_pmd, addr, &dst_ptl);
+	if (!dst_pte) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	src_pte = pte_offset_map(src_pmd, addr);
+	src_ptl = pte_lockptr(&init_mm, src_pmd);
+	spin_lock_bh(src_ptl);
+	arch_enter_lazy_mmu_mode();
+
+	do {
+		if (pte_none(*src_pte))
+			continue;
+		if (unlikely(!pte_present(*src_pte))) {
+			continue;
+		}
+		/* koi_copy_present_pte() will clear `*prealloc` if consumed */
+		ret = koi_copy_present_pte(dst_pte, src_pte, addr, &prealloc);
+		if (unlikely(ret == -EAGAIN))
+			break;
+		if (unlikely(prealloc)) {
+			put_page(prealloc);
+			prealloc = NULL;
+		}
+	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
+	arch_leave_lazy_mmu_mode();
+	spin_unlock_bh(src_ptl);
+    spin_unlock_bh(dst_ptl);
+
+	if (ret) {
+		WARN_ON_ONCE(ret != -EAGAIN);
+		ret = 0;
+	}
+	if (addr != end)
+		goto again;
+out:
+	if (unlikely(prealloc))
+		put_page(prealloc);
+	return ret;
+}
+
+int __koi_pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
+{
+	spinlock_t *ptl;
+	pmd_t *new = pmd_alloc_one(mm, address);
+	if (!new)
+		return -ENOMEM;
+
+	smp_wmb(); /* See comment in __pte_alloc */
+
+	ptl = pud_lockptr(mm, pud);
+    spin_lock_bh(ptl);
+	if (!pud_present(*pud)) {
+        #ifdef CONFIG_PTP
+		unsigned long iee_addr = __phys_to_iee(__pa(new));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)new);
+		#endif
+		mm_inc_nr_pmds(mm);
+		pud_populate(mm, pud, new);
+	} else	/* Another has populated it */
+		pmd_free(mm, new);
+	spin_unlock_bh(ptl);
+	return 0;
+}
+
+static inline pmd_t *koi_pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
+{
+	return (unlikely(pud_none(*pud)) && __koi_pmd_alloc(mm, pud, address))?
+		NULL: pmd_offset(pud, address);
+}
+
+/**
+*kio_copy_pmd_range - copy pmd from kernel to driver space
+*/
+static inline int koi_copy_pmd_range(struct mm_struct *ko_mm, pud_t *dst_pud,
+				     pud_t *src_pud, unsigned long addr,
+				     unsigned long end)
+{
+	pmd_t *src_pmd, *dst_pmd;
+	unsigned long next;
+	int err;
+
+	dst_pmd = koi_pmd_alloc(ko_mm, dst_pud, addr);
+	if (!dst_pmd) {
+		return -ENOMEM;
+	}
+	src_pmd = pmd_offset(src_pud, addr);
+	do {
+		next = pmd_addr_end(addr, end);
+		// CONFIG_TRANSPARENT_HUGEPAGE is enabled, so we must add copy_huge_pmd
+		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd) ||
+		    (pmd_devmap(*src_pmd))) {
+			err = koi_copy_huge_pmd(ko_mm, dst_pmd, src_pmd, addr);
+			if (err == -ENOMEM)
+				return -ENOMEM;
+			if (!err)
+				continue;
+		}
+		if (pmd_none_or_clear_bad(src_pmd)) {
+			continue;
+		}
+		if (koi_copy_pte_range(ko_mm, dst_pmd, src_pmd, addr, next))
+			return -ENOMEM;
+	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
+	return 0;
+}
+
+int __koi_pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)
+{
+	pud_t *new = pud_alloc_one(mm, address);
+	if (!new)
+		return -ENOMEM;
+
+	smp_wmb(); /* See comment in __pte_alloc */
+
+	spin_lock_bh(&mm->page_table_lock);
+	if (!p4d_present(*p4d)) {
+        #ifdef CONFIG_PTP
+		unsigned long iee_addr = __phys_to_iee(__pa(new));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)new);
+		#endif
+		mm_inc_nr_puds(mm);
+		p4d_populate(mm, p4d, new);
+	} else	/* Another has populated it */
+		pud_free(mm, new);
+	spin_unlock_bh(&mm->page_table_lock);
+	return 0;
+}
+
+static inline pud_t *koi_pud_alloc(struct mm_struct *mm, p4d_t *p4d,
+		unsigned long address)
+{
+	return (unlikely(p4d_none(*p4d)) && __koi_pud_alloc(mm, p4d, address)) ?
+		NULL : pud_offset(p4d, address);
+}
+
+/**
+*koi_copy_pud_range - copy pud from kernel to driver
+*/
+static inline int koi_copy_pud_range(struct mm_struct *ko_mm, p4d_t *dst_p4d,
+				     p4d_t *src_p4d, unsigned long addr,
+				     unsigned long end)
+{
+	pud_t *src_pud, *dst_pud;
+	unsigned long next;
+	dst_pud = koi_pud_alloc(ko_mm, dst_p4d, addr);
+	if (!dst_pud)
+		return -ENOMEM;
+	src_pud = pud_offset(src_p4d, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
+			continue;
+			/* fall through */
+		}
+		if (pud_none_or_clear_bad(src_pud))
+			continue;
+		if (koi_copy_pmd_range(ko_mm, dst_pud, src_pud, addr, next))
+			return -ENOMEM;
+	} while (dst_pud++, src_pud++, addr = next, addr != end);
+	return 0;
+}
+
+/**
+* koi_copy_p4d_range - map the kernel pagetable to the driver space level by level
+* @ko_mm: the mm_struct of driver module
+* @dst_pgd: destination pgd 
+* @src_pgd: source pgd
+* @addr: the start of address
+* @end:  the end of  address
+*/
+static inline int koi_copy_p4d_range(struct mm_struct *ko_mm, pgd_t *dst_pgd,
+				     pgd_t *src_pgd, unsigned long addr,
+				     unsigned long end)
+{
+	p4d_t *src_p4d, *dst_p4d;
+	unsigned long next;
+	dst_p4d = p4d_alloc(ko_mm, dst_pgd, addr);
+	if (!dst_p4d)
+		return -ENOMEM;
+	src_p4d = p4d_offset(src_pgd, addr);
+	do {
+		next = p4d_addr_end(addr, end);
+		if (p4d_none_or_clear_bad(src_p4d))
+			continue;
+		if (koi_copy_pud_range(ko_mm, dst_p4d, src_p4d, addr, next)) {
+			return -ENOMEM;
+		}
+	} while (dst_p4d++, src_p4d++, addr = next, addr != end);
+	return 0;
+}
+
+/**
+*int koi_copy_pagetable - map the address range from "addr" to "end" to the driver pagetable
+*@ko_mm: the mm_struct of the driver module
+*@koi_pg_dir: koi_pg_dir, related to the driver module, the entry for driver pagetable
+*@addr: the starting address of mapping zone
+*@end:  the end address of mapping zone
+*/
+int koi_copy_pagetable(struct mm_struct *ko_mm, pgd_t *koi_pg_dir,
+		       unsigned long addr, unsigned long end)
+{
+	int ret = 0;
+	unsigned long next;
+
+	pgd_t *src_pgd, *dst_pgd;
+
+	src_pgd = pgd_offset_pgd(swapper_pg_dir, addr);
+	dst_pgd = pgd_offset_pgd(koi_pg_dir, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none_or_clear_bad(src_pgd))
+			continue;
+		if (unlikely(koi_copy_p4d_range(ko_mm, dst_pgd, src_pgd, addr,
+						next))) {
+			ret = -ENOMEM;
+			break;
+		}
+	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
+
+	return ret;
+}
+
+void koi_set_rdonly(unsigned long addr, pgd_t *pgdir)
+{
+	p4d_t *p4dp;
+	pud_t *pudp;
+	pmd_t *pmdp;
+	pte_t *ptep;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+	if (pgd_none(*pgdp) || pgd_bad(*pgdp)) {
+		return;
+	}
+
+	p4dp = p4d_offset(pgdp, addr);
+	if (p4d_none(*p4dp) || p4d_bad(*p4dp)) {
+		return;
+	}
+
+	pudp = pud_offset(p4dp, addr);
+	if (pud_none(*pudp) || pud_bad(*pudp)) {
+		return;
+	}
+	pmdp = pmd_offset(pudp, addr);
+	if (pmd_none(*pmdp) || pmd_bad(*pmdp)) {
+		return;
+	}
+
+	ptep = pte_offset_kernel(pmdp, addr);
+	if (pte_none(*ptep)) {
+		printk(KERN_ERR "ptep 0x%llx not available\n", ptep);
+		return;
+	}
+	set_pte(ptep, __pte(pte_val(*ptep) | PTE_RDONLY));
+	printk(KERN_ERR "set_readonly successfully\n");
+	return;
+}
+
+/**
+* koi_create_pagetable - create pagetable for driver
+* @mod: driver module
+* 1.create a new koi_mem_hash_node  new_node
+* 2.create page table return the pgd address, init the new_node->pgdp
+* 3.create and init the new_node->ko_mm
+* 4.map swapper_ttbr1 to the newly created pagetable
+* 5.map the interrupt vector table to the newly created pagetable
+* 6.map the init_layout of the module
+* 7.map the core_layout of the module
+* 8.map switch_to_kernel_pgtable into driver view
+* 9.map share memory
+*/
+void koi_create_pagetable(struct module *mod)
+{
+	int ret = 0, cpu;
+	unsigned long vbar, addr, ttbr1;
+	pgd_t *pgdp;
+	unsigned long *ptr;
+	struct koi_mem_list *new_mem_node;
+	struct koi_mem_hash_node *new_node =
+		kzalloc(sizeof(struct koi_mem_hash_node), GFP_KERNEL);
+	if (!new_node) {
+		printk(KERN_ERR "NULL new_node\n");
+		return;
+	};
+	if (koi_swapper_ttbr1 == 0) {
+		pgdp = lm_alias(swapper_pg_dir);
+		ttbr1 = phys_to_ttbr(virt_to_phys(pgdp));
+		if (system_supports_cnp() &&
+		    !WARN_ON(pgdp != lm_alias(swapper_pg_dir)))
+			ttbr1 |= TTBR_CNP_BIT;
+#ifdef CONFIG_IEE
+		ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, 1);
+#endif
+		koi_swapper_ttbr1 = ttbr1;
+		// __WRITE_ONCE(koi_swapper_ttbr1, ttbr1);
+		// koi_set_rdonly(&koi_swapper_ttbr1, swapper_pg_dir);
+	}
+	new_node->pgdp = koi_pgd_alloc();
+	new_node->ko_mm =
+		kzalloc(sizeof(struct mm_struct) +
+				sizeof(unsigned long) * BITS_TO_LONGS(NR_CPUS),
+			GFP_KERNEL);
+	init_ko_mm(new_node->ko_mm, new_node->pgdp);
+	new_node->mod = mod;
+	koi_save_ttbr(mod, new_node->pgdp, new_node);
+	printk(KERN_ERR "copying koi_data, start=0x%16llx, end=0x%16llx\n",
+	       (unsigned long)__koi_data_start, (unsigned long)__koi_data_end);
+	// copy koi_swapper_ttbr1, which records page dir base for kernel view
+	koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+			   (unsigned long)__koi_data_start,
+			   (unsigned long)__koi_data_end);
+	asm volatile("mrs %0, VBAR_EL1\n" : "=r"(vbar) :);
+
+	// copy interrupt vectors
+	koi_copy_pagetable(new_node->ko_mm, new_node->pgdp, vbar & PAGE_MASK,
+			   (vbar + PAGE_SIZE) & PAGE_MASK);
+
+	// copy module init_layout, which contains init data and text in driver
+	ret = koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+				 (unsigned long)mod->init_layout.base,
+				 (unsigned long)mod->init_layout.base +
+					 mod->init_layout.size);
+	if (ret != 0)
+		printk(KERN_ERR
+		       "\033[33mError occur when copying init_layout, Eno:%d\033[0m\n",
+		       ret);
+
+	// copy module core_layout, which contains non-init data and text in driver
+	ret = koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+				 (unsigned long)mod->core_layout.base,
+				 (unsigned long)mod->core_layout.base +
+					 mod->core_layout.size);
+	if (ret != 0)
+		printk(KERN_ERR
+		       "\033[33mError occur when copying core_layout, Eno: %d\033[0m\n",
+		       ret);
+
+	// mapping switch_to_kernel_pgtable into driver view, which is used to switch to kernel view when entering INT
+	koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+			   (unsigned long)__koi_code_start,
+			   (unsigned long)__koi_code_end);
+
+	for_each_possible_cpu (cpu) {
+		ptr = per_cpu(irq_stack_ptr, cpu);
+		printk(KERN_ERR
+		       "\033[33mirq_stack_ptr on cpu %d addr=0x%16llx, end=0x%16llx\033[0m\n",
+		       cpu, (unsigned long)ptr,
+		       (unsigned long)ptr + IRQ_STACK_SIZE);
+		koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+				   (unsigned long)ptr,
+				   (unsigned long)ptr + IRQ_STACK_SIZE);
+	}
+
+	for_each_possible_cpu (cpu) {
+		ptr = per_cpu(koi_irq_current_ttbr1, cpu);
+		printk(KERN_ERR
+		       "\033[33mirq_current_ptr on cpu %d addr=0x%16llx, end=0x%16llx\033[0m\n",
+		       cpu, (unsigned long)ptr, (unsigned long)ptr + PAGE_SIZE);
+		koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+				   (unsigned long)ptr,
+				   (unsigned long)ptr + PAGE_SIZE);
+	}
+
+#ifdef CONFIG_IEE
+	// mapping iee_rwx_gate_entry and iee_si_base to ko's pagetable
+	koi_copy_pagetable(new_node->ko_mm, new_node->pgdp,
+			   (unsigned long)__iee_si_base_start,
+			   (unsigned long)__iee_exec_entry_end);
+#endif
+
+	// alloc 16KB memory for new ko, and add it into hashtable
+	addr = (unsigned long)kmalloc(THREAD_SIZE, GFP_KERNEL);
+	if ((void *)addr == NULL) {
+		printk(KERN_ERR "alloc buffer error\n");
+	}
+	koi_copy_pagetable(new_node->ko_mm, new_node->pgdp, addr,
+			   addr + THREAD_SIZE);
+
+	new_mem_node = kmalloc(sizeof(struct koi_mem_list), GFP_KERNEL);
+	if (new_mem_node == NULL) {
+		printk(KERN_ERR "alloc new_mem_node error\n");
+	}
+	new_mem_node->addr = addr;
+	new_mem_node->size = THREAD_SIZE;
+
+	new_node->mem_list_head =
+		(struct list_head)LIST_HEAD_INIT(new_node->mem_list_head);
+	hash_init(new_node->addr_htbl);
+	spin_lock_init(&new_node->addr_htbl_spin_lock);
+	spin_lock_init(&new_node->spin_lock);
+
+	spin_lock(&new_node->spin_lock);
+	list_add_rcu(&new_mem_node->list, &new_node->mem_list_head);
+	spin_unlock(&new_node->spin_lock);
+
+	spin_lock(&koi_mem_htbl_spin_lock);
+	hash_add_rcu(koi_mem_htbl, &new_node->node,
+		     (unsigned long)new_node->mod);
+	spin_unlock(&koi_mem_htbl_spin_lock);
+}
+/**
+* koi_mem_alloc 
+*@mod: driver module
+*@orig_addr: the starting address of the parameter in kernel
+*@size: the size of the parameter
+*/
+unsigned long koi_mem_alloc(struct module *mod, unsigned long orig_addr,
+			    unsigned long size)
+{
+	struct koi_mem_hash_node *target = NULL;
+	struct koi_mem_list *mem_node;
+	struct koi_addr_map *new_addr_node;
+	unsigned long addr = 0, flags;
+	struct koi_mem_list *new_mem_node;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return 0;
+	}
+	spin_lock_irqsave(&target->spin_lock, flags);
+	list_for_each_entry_rcu (mem_node, &target->mem_list_head, list) {
+		if (mem_node->size >= size) {
+			addr = mem_node->addr;
+			mem_node->size -= size;
+			if (mem_node->size == 0) {
+				list_del_rcu(&mem_node->list);
+			} else {
+				new_mem_node =
+					kmalloc(sizeof(struct koi_mem_list),
+						GFP_ATOMIC);
+				new_mem_node->addr = addr + size;
+				new_mem_node->size = mem_node->size;
+				list_replace_rcu(&mem_node->list,
+						 &new_mem_node->list);
+			}
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		}
+	}
+	spin_unlock_irqrestore(&target->spin_lock, flags);
+	if (!addr) {
+		addr = (unsigned long)kmalloc(THREAD_SIZE, GFP_KERNEL);
+		if ((void *)addr == NULL) {
+			return 0;
+		}
+		koi_copy_pagetable(target->ko_mm, target->pgdp, addr,
+				   addr + THREAD_SIZE);
+		mem_node = kmalloc(sizeof(struct koi_mem_list), GFP_KERNEL);
+		if (!mem_node) {
+			printk(KERN_ERR "NULL mem_node\n");
+		}
+		if (size > THREAD_SIZE) {
+			return 0;
+		}
+		mem_node->addr = addr + size;
+		mem_node->size = THREAD_SIZE - size;
+		spin_lock_irqsave(&target->spin_lock, flags);
+		list_add_tail_rcu(&mem_node->list, &target->mem_list_head);
+		spin_unlock_irqrestore(&target->spin_lock, flags);
+	}
+
+	new_addr_node = kzalloc(sizeof(struct koi_addr_map), GFP_KERNEL);
+	new_addr_node->buffer_addr = addr;
+	new_addr_node->orig_addr = orig_addr;
+	spin_lock_irqsave(&target->addr_htbl_spin_lock, flags);
+	hash_add_rcu(target->addr_htbl, &new_addr_node->node,
+		     new_addr_node->buffer_addr);
+	spin_unlock_irqrestore(&target->addr_htbl_spin_lock, flags);
+	return addr;
+}
+EXPORT_SYMBOL(koi_mem_alloc);
+// find the parameter pointer corresponding to the copy
+noinline void *koi_mem_lookup(struct module *mod, unsigned long addr)
+{
+	struct koi_mem_hash_node *target = NULL;
+	struct koi_addr_map *addr_map_node;
+	unsigned long orig_addr = addr;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return NULL;
+	}
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu (target->addr_htbl, addr_map_node, node,
+				    orig_addr) {
+		if (addr_map_node->buffer_addr == addr) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (addr_map_node) {
+		return (void *)(addr_map_node->orig_addr);
+	} else {
+		return NULL;
+	}
+}
+EXPORT_SYMBOL(koi_mem_lookup);
+/**
+* kio_mem_free - recycle a copy of the copied parameters and synchronize the parameters
+* @mod: driver module
+* @addr: the starting addr of parameter
+* @size: the size of the parameter
+* @is_const: const pointers or not
+* @count: contry the number of parameters
+*/
+noinline void koi_mem_free(struct module *mod, unsigned long addr,
+			   unsigned long size, bool is_const, int count, ...)
+{
+	struct koi_mem_hash_node *target = NULL;
+	struct koi_mem_list *mem_node;
+	struct list_head *pos = NULL;
+	struct koi_addr_map *addr_map_node;
+	unsigned long orig_size = size;
+	unsigned long orig_addr = addr;
+	va_list valist;
+	int i;
+	unsigned int offset;
+	unsigned long flags;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return;
+	}
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu (target->addr_htbl, addr_map_node, node,
+				    orig_addr) {
+		if (addr_map_node->buffer_addr == orig_addr) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	va_start(valist, count);
+	for (i = 0; i < count; i++) {
+		offset = va_arg(valist, int);
+		*(unsigned long *)(addr_map_node->buffer_addr + offset) =
+			*(unsigned long *)(addr_map_node->orig_addr + offset);
+	}
+	va_end(valist);
+	memcpy((void *)addr_map_node->orig_addr,
+	       (void *)addr_map_node->buffer_addr, orig_size);
+
+	spin_lock_irqsave(&target->addr_htbl_spin_lock, flags);
+	hlist_del_init_rcu(&addr_map_node->node);
+	call_rcu(&addr_map_node->rcu, koi_addr_map_node_free);
+	spin_unlock_irqrestore(&target->addr_htbl_spin_lock, flags);
+
+	spin_lock_irqsave(&target->spin_lock, flags);
+	list_for_each_entry_rcu (mem_node, &target->mem_list_head, list) {
+		if (mem_node->addr + mem_node->size == addr) {
+			pos = mem_node->list.prev;
+			addr = mem_node->addr;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size == mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size < mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			break;
+		}
+	}
+	mem_node = kzalloc(sizeof(struct koi_mem_list), GFP_ATOMIC);
+	mem_node->addr = addr;
+	mem_node->size = size;
+	if (pos)
+		list_add_rcu(&mem_node->list, pos);
+	else
+		list_add_tail_rcu(&mem_node->list, &target->mem_list_head);
+	spin_unlock_irqrestore(&target->spin_lock, flags);
+}
+EXPORT_SYMBOL(koi_mem_free);
+/**
+* koi_mem_free_callback - used to recycle the copy of parameter.
+*@addr: the address of the parameter
+*@(*func)(void*): callback func, used to release the copy of the parameter pointer
+*/
+noinline void koi_mem_free_callback(struct module *mod, unsigned long addr,
+				    unsigned long size, void (*func)(void *))
+{
+	struct koi_mem_hash_node *target = NULL;
+	struct koi_mem_list *mem_node;
+	struct list_head *pos = NULL;
+	struct koi_addr_map *addr_map_node;
+	unsigned long flags;
+	unsigned long orig_size = size;
+	unsigned long orig_addr = addr;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk("mem node for module: %s not found\n", mod->name);
+		return;
+	}
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu (target->addr_htbl, addr_map_node, node,
+				    orig_addr) {
+		if (addr_map_node->buffer_addr == orig_addr) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (addr_map_node != NULL) {
+		memcpy((void *)addr_map_node->orig_addr,
+		       (void *)addr_map_node->buffer_addr, orig_size);
+		func((void *)addr_map_node->orig_addr);
+	} else {
+		printk("Cannot find addr_map_node in addr_htbl, maybe addr is in kernel space!!\n");
+		func((void *)orig_addr);
+	}
+
+	spin_lock_irqsave(&target->addr_htbl_spin_lock, flags);
+	if (addr_map_node != NULL) {
+		hlist_del_init_rcu(&addr_map_node->node);
+		call_rcu(&addr_map_node->rcu, koi_addr_map_node_free);
+	}
+	spin_unlock_irqrestore(&target->addr_htbl_spin_lock, flags);
+	spin_lock_irqsave(&target->spin_lock, flags);
+	list_for_each_entry_rcu (mem_node, &target->mem_list_head, list) {
+		if (mem_node->addr + mem_node->size == addr) {
+			pos = mem_node->list.prev;
+			addr = mem_node->addr;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size == mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size < mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			break;
+		}
+	}
+	mem_node = kzalloc(sizeof(struct koi_mem_list), GFP_ATOMIC);
+	mem_node->addr = addr;
+	mem_node->size = size;
+	if (pos)
+		list_add_rcu(&mem_node->list, pos);
+	else
+		list_add_tail_rcu(&mem_node->list, &target->mem_list_head);
+	spin_unlock_irqrestore(&target->spin_lock, flags);
+}
+EXPORT_SYMBOL(koi_mem_free_callback);
+
+void koi_map_mem(struct module *mod, unsigned long addr, unsigned long size)
+{
+	struct koi_mem_hash_node *target = NULL;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod)
+			break;
+	}
+	rcu_read_unlock();
+
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return;
+	}
+	koi_copy_pagetable(target->ko_mm, target->pgdp, addr & PAGE_MASK,
+			   (addr + size + PAGE_SIZE) & PAGE_MASK);
+}
+EXPORT_SYMBOL(koi_map_mem);
+/**
+* koi_mem_free_to_user - function 'copy_to_user' in driver space
+*/
+void koi_mem_free_to_user(struct module *mod, unsigned long addr,
+			  unsigned long size)
+{
+	struct koi_mem_hash_node *target = NULL;
+	struct koi_mem_list *mem_node;
+	struct list_head *pos = NULL;
+	struct koi_addr_map *addr_map_node;
+	unsigned long flags;
+	unsigned long orig_size = size;
+	unsigned long orig_addr = addr;
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return;
+	}
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu (target->addr_htbl, addr_map_node, node,
+				    orig_addr) {
+		if (addr_map_node->buffer_addr == orig_addr) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (copy_to_user((void *)addr_map_node->orig_addr,
+			 (void *)addr_map_node->buffer_addr, orig_size)) {
+		return;
+	}
+
+	spin_lock_irqsave(&target->addr_htbl_spin_lock, flags);
+	hlist_del_init_rcu(&addr_map_node->node);
+	call_rcu(&addr_map_node->rcu, koi_addr_map_node_free);
+	spin_unlock_irqrestore(&target->addr_htbl_spin_lock, flags);
+	spin_lock_irqsave(&target->spin_lock, flags);
+	list_for_each_entry_rcu (mem_node, &target->mem_list_head, list) {
+		if (mem_node->addr + mem_node->size == addr) {
+			pos = mem_node->list.prev;
+			addr = mem_node->addr;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size == mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			size += mem_node->size;
+			list_del_rcu(&mem_node->list);
+			call_rcu(&mem_node->rcu, koi_mem_node_free);
+		} else if (addr + size < mem_node->addr) {
+			if (!pos)
+				pos = mem_node->list.prev;
+			break;
+		}
+	}
+	mem_node = kzalloc(sizeof(struct koi_mem_list), GFP_ATOMIC);
+	mem_node->addr = addr;
+	mem_node->size = size;
+	if (pos)
+		list_add_rcu(&mem_node->list, pos);
+	else
+		list_add_tail_rcu(&mem_node->list, &target->mem_list_head);
+	spin_unlock_irqrestore(&target->spin_lock, flags);
+}
+EXPORT_SYMBOL(koi_mem_free_to_user);
+// map the driver stack to kernel
+void koi_map_kostack(struct module *mod)
+{
+	struct koi_mem_hash_node *target = NULL;
+	void *koi_stack;
+	unsigned long cur_sp;
+	asm volatile("mov %0, sp\n" : "=r"(cur_sp) :);
+	if (on_irq_stack(cur_sp, NULL)) {
+		return;
+	}
+#ifndef CONFIG_IEE
+	unsigned long res, alloc_token;
+	struct task_token *token_addr =
+		(struct task_token *)((unsigned long)current +
+				      (unsigned long)koi_offset);
+	if (token_addr->koi_stack_base != NULL)
+		return;
+#else
+	koi_stack = iee_rw_gate(IEE_READ_KOI_STACK, current);
+	if (koi_stack != NULL)
+		return;
+#endif
+	koi_stack =
+		(void *)__get_free_pages(THREADINFO_GFP & ~__GFP_ACCOUNT, 3);
+	free_pages(koi_stack + 4 * PAGE_SIZE, 2);
+	printk(KERN_ERR "alloc dstack start=0x%16llx, end=0x%16llx\n",
+	       koi_stack, koi_stack + THREAD_SIZE);
+#ifndef CONFIG_IEE
+	token_addr->koi_stack =
+		(struct pt_regs *)(THREAD_SIZE + (unsigned long)koi_stack) - 1;
+	token_addr->koi_stack_base = koi_stack;
+#else
+	iee_rw_gate(
+		IEE_WRITE_KOI_STACK, current,
+		(unsigned long)((struct pt_regs *)(THREAD_SIZE +
+						   (unsigned long)koi_stack) -
+				1));
+	iee_rw_gate(IEE_WRITE_KOI_STACK_BASE, current,
+		    (unsigned long)koi_stack);
+#endif
+	rcu_read_lock();
+	hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+				    (unsigned long)mod) {
+		if (target->mod == mod) {
+			break;
+		}
+	}
+	rcu_read_unlock();
+	if (target == NULL) {
+		printk(KERN_ERR "mem node for module: %s not found\n",
+		       mod->name);
+		return;
+	}
+	koi_copy_pagetable(target->ko_mm, target->pgdp,
+			   (unsigned long)koi_stack,
+			   (unsigned long)koi_stack + THREAD_SIZE);
+	printk(KERN_ERR "create ko stack: 0x%16llx\n",
+	       (unsigned long)koi_stack);
+}
+EXPORT_SYMBOL(koi_map_kostack);
\ No newline at end of file
diff --git a/arch/arm64/kernel/mte.c b/arch/arm64/kernel/mte.c
index 4a069f85bd91..4631dc788dd3 100644
--- a/arch/arm64/kernel/mte.c
+++ b/arch/arm64/kernel/mte.c
@@ -78,7 +78,11 @@ int memcmp_pages(struct page *page1, struct page *page2)
 static void update_sctlr_el1_tcf0(u64 tcf0)
 {
 	/* ISB required for the kernel uaccess routines */
+#ifdef CONFIG_IEE
+    sysreg_clear_set_iee_si(sctlr_el1, SCTLR_EL1_TCF0_MASK, tcf0);
+#else
 	sysreg_clear_set(sctlr_el1, SCTLR_EL1_TCF0_MASK, tcf0);
+#endif
 	isb();
 }
 
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 14300c9e06d5..8154ca09eec4 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -85,7 +85,11 @@ static void noinstr __cpu_do_idle_irqprio(void)
 	unsigned long daif_bits;
 
 	daif_bits = read_sysreg(daif);
+#ifdef CONFIG_IEE
+    iee_si_write_daif(daif_bits | PSR_I_BIT);
+#else
 	write_sysreg(daif_bits | PSR_I_BIT, daif);
+#endif
 
 	/*
 	 * Unmask PMR before going idle to make sure interrupts can
@@ -97,7 +101,11 @@ static void noinstr __cpu_do_idle_irqprio(void)
 	__cpu_do_idle();
 
 	gic_write_pmr(pmr);
+#ifdef CONFIG_IEE
+    iee_si_write_daif(daif_bits);
+#else
 	write_sysreg(daif_bits, daif);
+#endif
 }
 
 /*
@@ -477,7 +485,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	}
 	p->thread.cpu_context.pc = (unsigned long)ret_from_fork;
 	p->thread.cpu_context.sp = (unsigned long)childregs;
-
+    
 	ptrace_hw_copy_thread(p);
 
 	return 0;
@@ -534,11 +542,24 @@ static void ssbs_thread_switch(struct task_struct *next)
  * This is *only* for exception entry from EL0, and is not valid until we
  * __switch_to() a user task.
  */
+#ifdef CONFIG_IEE
+// Put __entry_task in a isolated page to protect it.
+__attribute__((aligned(PAGE_SIZE))) DEFINE_PER_CPU(struct task_struct *[PAGE_SIZE/sizeof(struct task_struct *)], __entry_task);
+extern void iee_write_entry_task(struct task_struct *tsk);
+#else
 DEFINE_PER_CPU(struct task_struct *, __entry_task);
+#endif
 
 static void entry_task_switch(struct task_struct *next)
 {
+	#ifdef CONFIG_IEE
+	if(next == &init_task)
+		iee_write_entry_task((struct task_struct *)__va(__pa_symbol(next)));
+	else
+		iee_write_entry_task(next);
+	#else
 	__this_cpu_write(__entry_task, next);
+	#endif
 }
 
 /*
diff --git a/arch/arm64/kernel/proton-pack.c b/arch/arm64/kernel/proton-pack.c
index c569889b0322..f89d68459a0a 100644
--- a/arch/arm64/kernel/proton-pack.c
+++ b/arch/arm64/kernel/proton-pack.c
@@ -576,7 +576,11 @@ static enum mitigation_state spectre_v4_enable_hw_mitigation(void)
 		return state;
 
 	if (spectre_v4_mitigations_off()) {
+#ifdef CONFIG_IEE
+        sysreg_clear_set_iee_si(sctlr_el1, 0, SCTLR_ELx_DSSBS);
+#else
 		sysreg_clear_set(sctlr_el1, 0, SCTLR_ELx_DSSBS);
+#endif
 		set_pstate_ssbs(1);
 		return SPECTRE_VULNERABLE;
 	}
@@ -992,7 +996,11 @@ static void this_cpu_set_vectors(enum arm64_bp_harden_el1_vectors slot)
 	if (arm64_kernel_unmapped_at_el0())
 		return;
 
+#ifdef CONFIG_IEE
+    iee_rwx_gate_entry(IEE_WRITE_vbar_el1, v);
+#else
 	write_sysreg(v, vbar_el1);
+#endif
 	isb();
 }
 
diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
index c687866612d9..7e102bbb9245 100644
--- a/arch/arm64/kernel/setup.c
+++ b/arch/arm64/kernel/setup.c
@@ -33,6 +33,11 @@
 #include <linux/mm.h>
 #include <linux/pin_mem.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#include <asm/iee-si.h>
+#endif
+
 #include <asm/acpi.h>
 #include <asm/fixmap.h>
 #include <asm/cpu.h>
@@ -358,12 +363,33 @@ u64 cpu_logical_map(int cpu)
 	return __cpu_logical_map[cpu];
 }
 
+#ifdef CONFIG_IEE
+/* used for secure modification of vbar*/
+extern char __bp_harden_el1_vectors[];
+/* prepare iee rwx gate for senario of ttbr1=init_pg_dir */
+static void __init iee_si_init_pgd_early(void)
+{
+    /* prepare data used for iee rwx gate. */
+	iee_base_swapper_pg_dir = phys_to_ttbr(__pa_symbol(swapper_pg_dir));
+    iee_base_iee_pg_dir = phys_to_ttbr(__pa_symbol(iee_pg_dir));
+    iee_base_idmap_pg_dir = phys_to_ttbr(__pa_symbol(idmap_pg_dir));
+    iee_base_reserved_pg_dir = phys_to_ttbr(__pa_symbol(reserved_pg_dir));
+	iee_base__bp_harden_el1_vectors = (unsigned long)__bp_harden_el1_vectors;
+    // copy init_pg_dir totoally to simplify ttbr1 switch in iee rwx gate.
+    memcpy(iee_pg_dir, init_pg_dir, PAGE_SIZE);
+}
+#endif
+
 void __init __no_sanitize_address setup_arch(char **cmdline_p)
 {
 	init_mm.start_code = (unsigned long) _text;
 	init_mm.end_code   = (unsigned long) _etext;
 	init_mm.end_data   = (unsigned long) _edata;
 	init_mm.brk	   = (unsigned long) _end;
+	#ifdef CONFIG_IEE
+	init_new_context(&init_task, &init_mm);
+	atomic64_set(&init_mm.context.id, (1UL << get_cpu_asid_bits()) | 0x2);
+	#endif
 
 	*cmdline_p = boot_command_line;
 
@@ -393,6 +419,14 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	 */
 	local_daif_restore(DAIF_PROCCTX_NOIRQ);
 
+#ifdef CONFIG_IEE
+    /* 
+     * Map iee si codes to init_pg_dir to run the following
+     * cpu_uninstall_idmap() which writes ttbr0.
+     */
+    iee_si_init_pgd_early();
+#endif
+
 	/*
 	 * TTBR0 is only used for the identity mapping at this stage. Make it
 	 * point to zero page to avoid speculatively fetching new entries.
diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index e52f83668de5..a5870a97a8f7 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -93,13 +93,21 @@ static void cortex_a76_erratum_1463225_svc_handler(void)
 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
 	reg = read_sysreg(mdscr_el1);
 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+// #ifdef CONFIG_IEE
+//     iee_rwx_gate_entry(IEE_WRITE_mdscr_el1, val);
+// #else
 	write_sysreg(val, mdscr_el1);
+// #endif
 	asm volatile("msr daifclr, #8");
 	isb();
 
 	/* We will have taken a single-step exception by this point */
 
+// #ifdef CONFIG_IEE
+//     iee_rwx_gate_entry(IEE_WRITE_mdscr_el1, reg);
+// #else
 	write_sysreg(reg, mdscr_el1);
+// #endif
 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
 }
 #else
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 15c9c90639f1..21dae40b2808 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -51,7 +51,8 @@ static const char *handler[]= {
 	"Synchronous Abort",
 	"IRQ",
 	"FIQ",
-	"Error"
+	"Error",
+	"IEE"
 };
 
 int show_unhandled_signals = 0;
diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
index 71f4b5f24d15..b0fd07f29c51 100644
--- a/arch/arm64/kernel/vmlinux.lds.S
+++ b/arch/arm64/kernel/vmlinux.lds.S
@@ -85,6 +85,56 @@ jiffies = jiffies_64;
 #define TRAMP_TEXT
 #endif
 
+#ifdef CONFIG_IEE
+#define IEE_TEXT \
+	. = ALIGN(PAGE_SIZE);				\
+	__iee_code_start = .;       \
+	*(.iee.text.header)         \
+	*(.iee.text)				        \
+	. = ALIGN(PAGE_SIZE);				\
+	__iee_code_end = .;
+#else
+#define IEE_TEXT
+#endif
+
+#ifdef CONFIG_IEE
+#define IEE_SI_TEXT \
+    . = ALIGN(PAGE_SIZE); 		\
+    __iee_si_base_start = .;     \
+    *(.iee.si_base)             \
+    . = ALIGN(PAGE_SIZE);       \
+    __iee_exec_entry_start = .;   	\
+	*(.iee.exec_entry)        		\
+	. = ALIGN(PAGE_SIZE);           \
+    __iee_exec_entry_end = .;       \
+    __iee_si_text_start = .; 		\
+	*(.iee.si_text)				    \
+    . = ALIGN(PAGE_SIZE);           \
+    __iee_si_data_start = .;       	\
+    *(.iee.si_data)				    \
+    . = ALIGN(PAGE_SIZE);       	\
+	__iee_si_data_end = .;       	\
+    . += PAGE_SIZE - (20);       \
+    __iee_exec_exit = .;        \
+    __iee_exec_exit_pg = . + (20);  \
+    *(.iee.exec_exit)           \
+    . = ALIGN(PAGE_SIZE);       \
+    __iee_exec_exit_end = .;
+#else 
+#define IEE_SI_TEXT
+#endif
+
+#ifdef CONFIG_KOI
+#define KOI_TEXT \
+	. = ALIGN(PAGE_SIZE);				\
+	__koi_code_start = .;       \
+	*(.koi.text)				        \
+	. = ALIGN(PAGE_SIZE);				\
+	__koi_code_end = .;
+#else 
+#define KOI_TEXT
+#endif
+
 /*
  * The size of the PE/COFF section that covers the kernel image, which
  * runs from _stext to _edata, must be a round multiple of the PE/COFF
@@ -127,6 +177,7 @@ SECTIONS
 			SOFTIRQENTRY_TEXT
 			ENTRY_TEXT
 			TEXT_TEXT
+			IEE_TEXT
 			SCHED_TEXT
 			CPUIDLE_TEXT
 			LOCK_TEXT
@@ -135,6 +186,8 @@ SECTIONS
 			IDMAP_TEXT
 			HIBERNATE_TEXT
 			TRAMP_TEXT
+			IEE_SI_TEXT
+            KOI_TEXT
 			*(.fixup)
 			*(.gnu.warning)
 		. = ALIGN(16);
@@ -164,6 +217,11 @@ SECTIONS
 	. += PAGE_SIZE;
 #endif
 
+#ifdef CONFIG_IEE
+    iee_pg_dir = .;
+	. += PAGE_SIZE;
+#endif
+
 	reserved_pg_dir = .;
 	. += PAGE_SIZE;
 
@@ -261,6 +319,18 @@ SECTIONS
 	. += INIT_DIR_SIZE;
 	init_pg_end = .;
 
+	#ifdef CONFIG_IEE
+	. = ALIGN(PAGE_SIZE*8);
+	init_iee_stack_begin = .;
+	. += PAGE_SIZE*4;
+	init_iee_stack_end = .;
+
+    . = ALIGN(PAGE_SIZE);
+	init_iee_si_stack_begin = .;
+	. += PAGE_SIZE*4;
+	init_iee_si_stack_end = .;
+	#endif
+
 	. = ALIGN(SEGMENT_ALIGN);
 	__pecoff_data_size = ABSOLUTE(. - __initdata_begin);
 	_end = .;
diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index 001737a8f309..d1786fc732e7 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -17,6 +17,10 @@
 #include <asm/smp.h>
 #include <asm/tlbflush.h>
 
+#ifdef CONFIG_IEE
+#include <asm/iee-si.h>
+#endif
+
 static u32 asid_bits;
 static DEFINE_RAW_SPINLOCK(cpu_asid_lock);
 
@@ -39,7 +43,11 @@ static unsigned long *pinned_asid_map;
 #define idx2asid(idx)		asid2idx(idx)
 
 /* Get the ASIDBits supported by the current CPU */
+#ifdef CONFIG_IEE
+u32 get_cpu_asid_bits(void)
+#else
 static u32 get_cpu_asid_bits(void)
+#endif
 {
 	u32 asid;
 	int fld = cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64MMFR0_EL1),
@@ -212,6 +220,38 @@ static u64 new_context(struct mm_struct *mm)
 	return idx2asid(asid) | generation;
 }
 
+#ifdef CONFIG_KOI
+/*
+ * This function is used to check and allocate ASID for ko's pgd
+ * The mm MUST point to the isolated kos' mm_struct, other behaviours are undefined.
+ */
+void koi_check_and_switch_context(struct mm_struct *mm) {
+	u64 asid = atomic64_read(&mm->context.id);
+	u64 old_active_asid;
+	unsigned long flags;
+	unsigned int cpu;
+
+	old_active_asid = atomic64_read(this_cpu_ptr(&active_asids));
+	if (old_active_asid && asid_gen_match(asid) && atomic64_cmpxchg_relaxed(this_cpu_ptr(&active_asids), old_active_asid, asid)) {
+		return;
+	}
+	
+	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
+	asid = atomic64_read(&mm->context.id);
+	if (!asid_gen_match(asid)) {
+		asid = new_context(mm);
+		atomic64_set(&mm->context.id, asid);
+	}
+
+	cpu = smp_processor_id();
+	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending))
+		local_flush_tlb_all();
+
+	atomic64_set(this_cpu_ptr(&active_asids), asid);
+	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
+}
+#endif
+
 void check_and_switch_context(struct mm_struct *mm)
 {
 	unsigned long flags;
@@ -348,7 +388,9 @@ asmlinkage void post_ttbr_update_workaround(void)
 
 void cpu_do_switch_mm(phys_addr_t pgd_phys, struct mm_struct *mm)
 {
+	#ifndef CONFIG_IEE
 	unsigned long ttbr1 = read_sysreg(ttbr1_el1);
+	#endif
 	unsigned long asid = ASID(mm);
 	unsigned long ttbr0 = phys_to_ttbr(pgd_phys);
 
@@ -360,14 +402,30 @@ void cpu_do_switch_mm(phys_addr_t pgd_phys, struct mm_struct *mm)
 	if (IS_ENABLED(CONFIG_ARM64_SW_TTBR0_PAN))
 		ttbr0 |= FIELD_PREP(TTBR_ASID_MASK, asid);
 
-	/* Set ASID in TTBR1 since TCR.A1 is set */
+	#ifdef CONFIG_IEE
+	ttbr0 |= FIELD_PREP(TTBR_ASID_MASK, asid+1);
+    // ttbr1 &= ~TTBR_ASID_MASK;
+	// ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
+    iee_rwx_gate_entry(IEE_CONTEXT_SWITCH, ttbr0);
+	// TODO : IEE-SI
+    // TODO : if defined CONFIG_IEE and defined CONFIG_KOI
+	#else
+	/* Set ASID in TTBR0 since TCR.A1 is set 0*/
+
+	#ifdef CONFIG_KOI
+	ttbr0 |= FIELD_PREP(TTBR_ASID_MASK, asid+1);
+    ttbr1 &= ~TTBR_ASID_MASK;
+	ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
+	#else
 	ttbr1 &= ~TTBR_ASID_MASK;
 	ttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);
-
+	#endif
 	write_sysreg(ttbr1, ttbr1_el1);
 	isb();
 	write_sysreg(ttbr0, ttbr0_el1);
 	isb();
+    #endif
+
 	post_ttbr_update_workaround();
 }
 
@@ -375,11 +433,20 @@ static int asids_update_limit(void)
 {
 	unsigned long num_available_asids = NUM_USER_ASIDS;
 
-	if (arm64_kernel_unmapped_at_el0()) {
-		num_available_asids /= 2;
-		if (pinned_asid_map)
-			set_kpti_asid_bits(pinned_asid_map);
-	}
+	#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+        num_available_asids /= 2;
+		if (pinned_asid_map) {
+            unsigned int len = BITS_TO_LONGS(NUM_USER_ASIDS) * sizeof(unsigned long);
+    	    memset(pinned_asid_map, 0xaa, len);
+        }
+    #else
+        if (arm64_kernel_unmapped_at_el0()) {
+		    num_available_asids /= 2;
+		    if (pinned_asid_map)
+			    set_kpti_asid_bits(pinned_asid_map);
+	    }
+    #endif
+
 	/*
 	 * Expect allocation after rollover to fail if we don't have at least
 	 * one more ASID than CPUs. ASID #0 is reserved for init_mm.
@@ -400,6 +467,10 @@ arch_initcall(asids_update_limit);
 
 static int asids_init(void)
 {
+	#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+	unsigned int len;
+	#endif
+
 	asid_bits = get_cpu_asid_bits();
 	atomic64_set(&asid_generation, ASID_FIRST_VERSION);
 	asid_map = kcalloc(BITS_TO_LONGS(NUM_USER_ASIDS), sizeof(*asid_map),
@@ -412,6 +483,10 @@ static int asids_init(void)
 				  sizeof(*pinned_asid_map), GFP_KERNEL);
 	nr_pinned_asids = 0;
 
+    #if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+        len = BITS_TO_LONGS(NUM_USER_ASIDS) * sizeof(unsigned long);
+    	memset(asid_map, 0xaa, len);
+    #else 
 	/*
 	 * We cannot call set_reserved_asid_bits() here because CPU
 	 * caps are not finalized yet, so it is safer to assume KPTI
@@ -419,6 +494,8 @@ static int asids_init(void)
 	 */
 	if (IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0))
 		set_kpti_asid_bits(asid_map);
+    #endif
+
 	return 0;
 }
 early_initcall(asids_init);
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9bd10909861a..39dd64603b12 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -218,7 +218,11 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 		pteval ^= PTE_RDONLY;
 		pteval |= pte_val(entry);
 		pteval ^= PTE_RDONLY;
+		#ifdef CONFIG_PTP
+		pteval = iee_set_cmpxchg_relaxed(ptep, old_pteval, pteval);
+		#else
 		pteval = cmpxchg_relaxed(&pte_val(*ptep), old_pteval, pteval);
+		#endif
 	} while (pteval != old_pteval);
 
 	/* Invalidate a stale read-only entry */
diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index be67a9c42628..db11a62ef80b 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -57,8 +57,21 @@
  * that cannot be mistaken for a real physical address.
  */
 s64 memstart_addr __ro_after_init = -1;
+#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+s64 memstart_addr_init __ro_after_init = -1;
+s64 memrand __ro_after_init = -1;
+EXPORT_SYMBOL(memrand);
+#endif
+#ifdef CONFIG_KOI
+s64 koi_offset __ro_after_init = -1;
+EXPORT_SYMBOL(koi_offset);
+#endif
+#ifdef CONFIG_IEE
+s64 iee_offset __ro_after_init = -1;
+#endif
 EXPORT_SYMBOL(memstart_addr);
 
+
 /*
  * If the corresponding config options are enabled, we create both ZONE_DMA
  * and ZONE_DMA32. By default ZONE_DMA covers the 32-bit addressable memory
@@ -413,19 +426,34 @@ early_param("memmap", parse_memmap_opt);
 
 void __init arm64_memblock_init(void)
 {
+	#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+	const s64 linear_region_size = BIT(vabits_actual - 2);
+	#else
 	const s64 linear_region_size = BIT(vabits_actual - 1);
+	#endif
 
 	/* Handle linux,usable-memory-range property */
 	fdt_enforce_memory_region();
 
 	/* Remove memory above our supported physical address size */
+	#ifdef CONFIG_IEE
+	// If config iee, phys size can not be above 0x400000000000
+	if(__pa_symbol(_end) > 1ULL << 46)
+		panic("Image on too high phys mem.\n");
+	else
+		memblock_remove(1ULL << 46, ULLONG_MAX);
+	#else
 	memblock_remove(1ULL << PHYS_MASK_SHIFT, ULLONG_MAX);
+	#endif
 
 	/*
 	 * Select a suitable value for the base of physical memory.
 	 */
 	memstart_addr = round_down(memblock_start_of_DRAM(),
 				   ARM64_MEMSTART_ALIGN);
+	#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+	memstart_addr_init = memstart_addr;
+	#endif
 
 	/*
 	 * Remove the memory that we will not be able to cover with the
@@ -506,6 +534,18 @@ void __init arm64_memblock_init(void)
 					 ((range * memstart_offset_seed) >> 16);
 		}
 	}
+	
+    #ifdef CONFIG_KOI
+	koi_offset = memstart_addr - memstart_addr_init + KOI_OFFSET;
+    #endif
+	#ifdef CONFIG_IEE
+	iee_offset = memstart_addr - memstart_addr_init + ((unsigned long)0x4 << 44);
+	#endif
+    //printk(KERN_ERR "koi_offset: 0x%16llx\n", koi_offset);
+
+    printk(KERN_ERR
+	       "\033[33mmemstart_addr=0x%16llx, memstart_addr_init=0x%16llx, memrand=0x%16llx\033[0m\n",
+	       memstart_addr, memstart_addr_init, memrand);
 
 	/*
 	 * Register the kernel text, kernel data, initrd, and initial
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index 804d5197c1a3..257ebab2e125 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -6,6 +6,7 @@
  * Copyright (C) 2012 ARM Ltd.
  */
 
+#include "asm/pgtable.h"
 #include <linux/cache.h>
 #include <linux/export.h>
 #include <linux/kernel.h>
@@ -38,6 +39,11 @@
 #include <asm/ptdump.h>
 #include <asm/tlbflush.h>
 #include <asm/pgalloc.h>
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#include <asm/iee.h>
+#include <asm/iee-si.h>
+#endif
 
 #define NO_BLOCK_MAPPINGS	BIT(0)
 #define NO_CONT_MAPPINGS	BIT(1)
@@ -65,8 +71,281 @@ static pud_t bm_pud[PTRS_PER_PUD] __page_aligned_bss __maybe_unused;
 static DEFINE_SPINLOCK(swapper_pgdir_lock);
 static DEFINE_MUTEX(fixmap_lock);
 
+#ifdef CONFIG_IEE
+extern struct cred init_cred;
+void *bm_pte_addr = (void *)bm_pte;
+
+extern unsigned long init_iee_stack_begin[];
+extern unsigned long init_iee_stack_end[];
+extern unsigned long __iee_si_base_start[];
+extern unsigned long __iee_exec_entry_start[];
+extern unsigned long __iee_si_data_start[];
+extern unsigned long __iee_si_data_end[];
+extern unsigned long __iee_exec_exit_pg[];
+extern unsigned long __iee_exec_exit_end[];
+
+#ifdef CONFIG_PTP
+/* Funcs to set pgtable before iee initialized. */
+static void iee_set_swapper_pgd_pre_init(pgd_t *pgdp, pgd_t pgd)
+{
+	pgd_t *fixmap_pgdp;
+
+	spin_lock(&swapper_pgdir_lock);
+	fixmap_pgdp = pgd_set_fixmap_init(__pa_symbol(pgdp));
+	WRITE_ONCE(*fixmap_pgdp, pgd);
+	/*
+	 * We need dsb(ishst) here to ensure the page-table-walker sees
+	 * our new entry before set_p?d() returns. The fixmap's
+	 * flush_tlb_kernel_range() via clear_fixmap() does this for us.
+	 */
+	pgd_clear_fixmap_init();
+	spin_unlock(&swapper_pgdir_lock);
+}
+
+static inline void iee_set_p4d_pre_init(p4d_t *p4dp, p4d_t p4d)
+{
+	if (in_swapper_pgdir(p4dp)) {
+		iee_set_swapper_pgd_pre_init((pgd_t *)p4dp, __pgd(p4d_val(p4d)));
+		return;
+	}
+
+	WRITE_ONCE(*p4dp, p4d);
+	dsb(ishst);
+	isb();
+}
+
+static inline void iee_set_pud_pre_init(pud_t *pudp, pud_t pud)
+{
+#ifdef __PAGETABLE_PUD_FOLDED
+	if (in_swapper_pgdir(pudp)) {
+		iee_set_swapper_pgd_pre_init((pgd_t *)pudp, __pgd(pud_val(pud)));
+		return;
+	}
+#endif /* __PAGETABLE_PUD_FOLDED */
+#ifdef CONFIG_KOI
+    pudval_t val = pud_val(pud);
+    if (pud_valid(pud) && !(val & PUD_TABLE_BIT)) {
+        // There is no PUD_SEC_NG, so we use PMD_SECT_NG instead.
+        pud = __pud(val | PMD_SECT_NG);
+    }
+#endif
+	WRITE_ONCE(*pudp, pud);
+
+	if (pud_valid(pud)) {
+		dsb(ishst);
+		isb();
+	}
+}
+
+static inline void iee_set_pmd_pre_init(pmd_t *pmdp, pmd_t pmd)
+{
+#ifdef __PAGETABLE_PMD_FOLDED
+	if (in_swapper_pgdir(pmdp)) {
+		iee_set_swapper_pgd_pre_init((pgd_t *)pmdp, __pgd(pmd_val(pmd)));
+		return;
+	}
+#endif /* __PAGETABLE_PMD_FOLDED */
+#ifdef CONFIG_KOI
+    pmdval_t val = pmd_val(pmd);
+    if (pmd_valid(pmd) && !(val & PMD_TABLE_BIT)) {
+        pmd = __pmd(val | PMD_SECT_NG);
+    }
+#endif
+	WRITE_ONCE(*pmdp, pmd);
+
+	if (pmd_valid(pmd)) {
+		dsb(ishst);
+		isb();
+	}
+}
+
+static inline void __iee_p4d_populate_pre_init(p4d_t *p4dp, phys_addr_t pudp, p4dval_t prot)
+{
+	iee_set_p4d_pre_init(p4dp, __p4d(__phys_to_p4d_val(pudp) | prot));
+}
+
+static inline void __iee_pud_populate_pre_init(pud_t *pudp, phys_addr_t pmdp, pudval_t prot)
+{
+	iee_set_pud_pre_init(pudp, __pud(__phys_to_pud_val(pmdp) | prot));
+}
+
+static inline void __iee_pmd_populate_pre_init(pmd_t *pmdp, phys_addr_t ptep,
+				  pmdval_t prot)
+{
+	iee_set_pmd_pre_init(pmdp, __pmd(__phys_to_pmd_val(ptep) | prot));
+}
+
+#define set_pgd_init(pgdptr, pgdval) iee_set_p4d_pre_init((p4d_t *)(pgdptr), (p4d_t) { pgdval })
+
+/* Funcs to set fixmap before iee initialized. */
+static bool pgattr_change_is_safe(u64 old, u64 new);
+static int iee_pud_set_huge_fixmap(pud_t *pudp, phys_addr_t phys, pgprot_t prot)
+{
+	pud_t new_pud = pfn_pud(__phys_to_pfn(phys), mk_pud_sect_prot(prot));
+
+	/* Only allow permission changes for now */
+	if (!pgattr_change_is_safe(READ_ONCE(pud_val(*pudp)),
+				   pud_val(new_pud)))
+		return 0;
+
+	VM_BUG_ON(phys & ~PUD_MASK);
+	iee_set_fixmap_pud_pre_init(pudp, new_pud);
+	return 1;
+}
+
+static int iee_pmd_set_huge_fixmap(pmd_t *pmdp, phys_addr_t phys, pgprot_t prot)
+{
+	pmd_t new_pmd = pfn_pmd(__phys_to_pfn(phys), mk_pmd_sect_prot(prot));
+
+	/* Only allow permission changes for now */
+	if (!pgattr_change_is_safe(READ_ONCE(pmd_val(*pmdp)),
+				   pmd_val(new_pmd)))
+		return 0;
+
+	VM_BUG_ON(phys & ~PMD_MASK);
+	iee_set_fixmap_pmd_pre_init(pmdp, new_pmd);
+	return 1;
+}
+
+static inline void __iee_pmd_populate_fixmap(pmd_t *pmdp, phys_addr_t ptep,
+				  pmdval_t prot)
+{
+	iee_set_fixmap_pmd_pre_init(pmdp, __pmd(__phys_to_pmd_val(ptep) | prot));
+}
+
+static inline void __iee_pud_populate_fixmap(pud_t *pudp, phys_addr_t pmdp, pudval_t prot)
+{
+	iee_set_fixmap_pud_pre_init(pudp, __pud(__phys_to_pud_val(pmdp) | prot));
+}
+#endif
+
+static inline void iee_set_pte_pre_init(pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_KOI
+    if (!pte_none(pte)) {
+		pte = __pte(pte_val(pte) | PTE_NG);
+    }
+#endif    
+	WRITE_ONCE(*ptep, pte);
+
+	/*
+	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
+	 * or update_mmu_cache() have the necessary barriers.
+	 */
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
+}
+
+static void __init iee_set_token_page_valid_pre_init(void *token, void *new)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, (unsigned long)token);
+
+	p4d_t *p4dp = p4d_offset(pgdp, (unsigned long)token);
+
+	pud_t *pudp = pud_offset(p4dp, (unsigned long)token);
+
+	pmd_t *pmdp = pmd_offset(pudp, (unsigned long)token);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, (unsigned long)token);
+	pte_t pte = READ_ONCE(*ptep);
+	pte = __pte(((pte_val(pte) | 0x1) & ~PTE_ADDR_MASK) | __phys_to_pte_val(__pa(new)));
+	iee_set_pte_pre_init(ptep, pte);
+	flush_tlb_kernel_range((unsigned long)token, (unsigned long)(token+PAGE_SIZE));
+	isb();
+}
+#endif
+
+#if defined(CONFIG_KOI) && !defined(CONFIG_IEE)
+int koi_add_page_mapping(unsigned long dst, unsigned long src)
+{
+    pgd_t *src_pgdp, *dst_pgdp;
+    p4d_t *src_p4dp, *dst_p4dp;
+    pud_t *src_pudp, *dst_pudp;
+    pmd_t *src_pmdp, *dst_pmdp;
+    pte_t *src_ptep, *dst_ptep;
+
+    src_pgdp = pgd_offset_pgd(swapper_pg_dir, src);
+	dst_pgdp = pgd_offset_pgd(swapper_pg_dir, dst);
+    
+    src_p4dp = p4d_offset(src_pgdp, src);
+	dst_p4dp = p4d_alloc(&init_mm, dst_pgdp, dst);
+    if (!dst_p4dp) {
+        return -ENOMEM;
+    }
+    src_pudp = pud_offset(src_p4dp, src);
+	dst_pudp = pud_alloc(&init_mm, dst_p4dp, dst);
+    if (!dst_pudp) {
+        return -ENOMEM;
+    }
+    if (pud_val(*src_pudp) & PMD_TABLE_BIT) {
+        src_pmdp = pmd_offset(src_pudp, src);
+        dst_pmdp = pmd_alloc(&init_mm, dst_pudp, dst);
+        if (!dst_pmdp) {
+            return -ENOMEM;
+        }
+        if (pmd_val(*src_pmdp) & PMD_TABLE_BIT) {
+            src_ptep = pte_offset_kernel(src_pmdp, src);
+            dst_ptep = pte_alloc_map(&init_mm, dst_pmdp, dst);
+            set_pte(dst_ptep, *src_ptep);
+        } else {
+            set_pte((pte_t *)dst_pmdp, pmd_pte(*src_pmdp));
+        }
+    } else {
+        set_pte((pte_t *)dst_pudp, pud_pte(*src_pudp));
+    }
+    
+
+	flush_tlb_kernel_range(dst, dst+PAGE_SIZE);
+	isb();
+    return 0;
+}
+
+void koi_remove_page_mapping(unsigned long addr) {
+    pgd_t *src_pgdp;
+    p4d_t *src_p4dp;
+    pud_t *src_pudp;
+    pmd_t *src_pmdp;
+    pte_t *src_ptep;
+
+    src_pgdp = pgd_offset_pgd(swapper_pg_dir, addr);
+    if (pgd_none(*src_pgdp) || pgd_bad(*src_pgdp))
+        return;
+    src_p4dp = p4d_offset(src_pgdp, addr);
+    if (p4d_none(*src_p4dp) || p4d_bad(*src_p4dp))
+        return;
+    src_pudp = pud_offset(src_p4dp, addr);
+    if (pud_none(*src_pudp))
+        return;
+    if (pud_val(*src_pudp) & PMD_TABLE_BIT) {
+        src_pmdp = pmd_offset(src_pudp, addr);
+        if (pmd_none(*src_pmdp))
+            return;
+        if (pmd_val(*src_pmdp) & PMD_TABLE_BIT) {
+            src_ptep = pte_offset_kernel(src_pmdp, addr);
+            if(!pte_none(*src_ptep))
+                pte_clear(&init_mm, addr, src_ptep);
+        } else {
+            pmd_clear(src_pmdp);
+        }
+    } else {
+        pud_clear(src_pudp);
+    }
+    
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+#endif
+
 void set_swapper_pgd(pgd_t *pgdp, pgd_t pgd)
 {
+	#ifdef CONFIG_PTP
+	spin_lock(&swapper_pgdir_lock);
+	iee_rw_gate(IEE_OP_SET_SWAPPER_PGD, pgdp, pgd);
+	spin_unlock(&swapper_pgdir_lock);
+	#else
 	pgd_t *fixmap_pgdp;
 
 	spin_lock(&swapper_pgdir_lock);
@@ -79,6 +358,7 @@ void set_swapper_pgd(pgd_t *pgdp, pgd_t pgd)
 	 */
 	pgd_clear_fixmap();
 	spin_unlock(&swapper_pgdir_lock);
+	#endif
 }
 
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
@@ -92,6 +372,34 @@ pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 }
 EXPORT_SYMBOL(phys_mem_access_prot);
 
+#ifdef CONFIG_PTP
+static phys_addr_t __init early_pgtable_alloc(int shift)
+{
+	phys_addr_t phys;
+	void *ptr;
+
+	phys = memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (!phys)
+		panic("Failed to allocate page table page\n");
+
+	/*
+	 * The FIX_{PGD,PUD,PMD} slots may be in active use, but the FIX_PTE
+	 * slot will be free, so we can (ab)use the FIX_PTE slot to initialise
+	 * any level of table.
+	 */
+	ptr = pte_set_fixmap_init(phys);
+
+	memset(ptr, 0, PAGE_SIZE);
+
+	/*
+	 * Implicit barriers also ensure the zeroed page is visible to the page
+	 * table walker
+	 */
+	pte_clear_fixmap_init();
+
+	return phys;
+}
+#else
 static phys_addr_t __init early_pgtable_alloc(int shift)
 {
 	phys_addr_t phys;
@@ -118,6 +426,7 @@ static phys_addr_t __init early_pgtable_alloc(int shift)
 
 	return phys;
 }
+#endif
 
 static bool pgattr_change_is_safe(u64 old, u64 new)
 {
@@ -166,7 +475,11 @@ static void init_pte(pmd_t *pmdp, unsigned long addr, unsigned long end,
 	do {
 		pte_t old_pte = READ_ONCE(*ptep);
 
+		#ifdef CONFIG_PTP
+		iee_set_fixmap_pte_pre_init(ptep, pfn_pte(__phys_to_pfn(phys), prot));
+		#else
 		set_pte(ptep, pfn_pte(__phys_to_pfn(phys), prot));
+		#endif
 
 		/*
 		 * After the PTE entry has been populated once, we
@@ -195,7 +508,11 @@ static void alloc_init_cont_pte(pmd_t *pmdp, unsigned long addr,
 		phys_addr_t pte_phys;
 		BUG_ON(!pgtable_alloc);
 		pte_phys = pgtable_alloc(PAGE_SHIFT);
+		#ifdef CONFIG_PTP
+		__iee_pmd_populate_fixmap(pmdp, pte_phys, PMD_TYPE_TABLE);
+		#else
 		__pmd_populate(pmdp, pte_phys, PMD_TYPE_TABLE);
+		#endif
 		pmd = READ_ONCE(*pmdp);
 	}
 	BUG_ON(pmd_bad(pmd));
@@ -232,7 +549,11 @@ static void init_pmd(pud_t *pudp, unsigned long addr, unsigned long end,
 		/* try section mapping first */
 		if (((addr | next | phys) & ~SECTION_MASK) == 0 &&
 		    (flags & NO_BLOCK_MAPPINGS) == 0) {
+			#ifdef CONFIG_PTP
+			iee_pmd_set_huge_fixmap(pmdp, phys, prot);
+			#else
 			pmd_set_huge(pmdp, phys, prot);
+			#endif
 
 			/*
 			 * After the PMD entry has been populated once, we
@@ -269,7 +590,11 @@ static void alloc_init_cont_pmd(pud_t *pudp, unsigned long addr,
 		phys_addr_t pmd_phys;
 		BUG_ON(!pgtable_alloc);
 		pmd_phys = pgtable_alloc(PMD_SHIFT);
+		#ifdef CONFIG_PTP
+		__iee_pud_populate_fixmap(pudp, pmd_phys, PUD_TYPE_TABLE);
+		#else
 		__pud_populate(pudp, pmd_phys, PUD_TYPE_TABLE);
+		#endif
 		pud = READ_ONCE(*pudp);
 	}
 	BUG_ON(pud_bad(pud));
@@ -338,7 +663,11 @@ static void alloc_init_pud(pgd_t *pgdp, unsigned long addr, unsigned long end,
 		 */
 		if (use_1G_block(addr, next, phys) &&
 		    (flags & NO_BLOCK_MAPPINGS) == 0) {
+			#ifdef CONFIG_PTP
+			iee_pud_set_huge_fixmap(pudp, phys, prot);
+			#else
 			pud_set_huge(pudp, phys, prot);
+			#endif
 
 			/*
 			 * After the PUD entry has been populated once, we
@@ -391,112 +720,378 @@ static void __create_pgd_mapping(pgd_t *pgdir, phys_addr_t phys,
 	} while (pgdp++, addr = next, addr != end);
 }
 
-static phys_addr_t __pgd_pgtable_alloc(int shift)
+#ifdef CONFIG_PTP
+static int __init iee_pud_set_huge_pre_init(pud_t *pudp, phys_addr_t phys, pgprot_t prot)
 {
-	void *ptr = (void *)__get_free_page(GFP_PGTABLE_KERNEL);
-	BUG_ON(!ptr);
+	pud_t new_pud = pfn_pud(__phys_to_pfn(phys), mk_pud_sect_prot(prot));
 
-	/* Ensure the zeroed page is visible to the page table walker */
-	dsb(ishst);
-	return __pa(ptr);
+	/* Only allow permission changes for now */
+	if (!pgattr_change_is_safe(READ_ONCE(pud_val(*pudp)),
+				   pud_val(new_pud)))
+		return 0;
+
+	VM_BUG_ON(phys & ~PUD_MASK);
+	iee_set_pud_pre_init(pudp, new_pud);
+	return 1;
 }
 
-static phys_addr_t pgd_pgtable_alloc(int shift)
+static int __init iee_pmd_set_huge_pre_init(pmd_t *pmdp, phys_addr_t phys, pgprot_t prot)
 {
-	phys_addr_t pa = __pgd_pgtable_alloc(shift);
+	pmd_t new_pmd = pfn_pmd(__phys_to_pfn(phys), mk_pmd_sect_prot(prot));
 
-	/*
-	 * Call proper page table ctor in case later we need to
-	 * call core mm functions like apply_to_page_range() on
-	 * this pre-allocated page table.
-	 *
-	 * We don't select ARCH_ENABLE_SPLIT_PMD_PTLOCK if pmd is
-	 * folded, and if so pgtable_pmd_page_ctor() becomes nop.
-	 */
-	if (shift == PAGE_SHIFT)
-		BUG_ON(!pgtable_pte_page_ctor(phys_to_page(pa)));
-	else if (shift == PMD_SHIFT)
-		BUG_ON(!pgtable_pmd_page_ctor(phys_to_page(pa)));
+	/* Only allow permission changes for now */
+	if (!pgattr_change_is_safe(READ_ONCE(pmd_val(*pmdp)),
+				   pmd_val(new_pmd)))
+		return 0;
 
-	return pa;
+	VM_BUG_ON(phys & ~PMD_MASK);
+	iee_set_pmd_pre_init(pmdp, new_pmd);
+	return 1;
 }
 
-/*
- * This function can only be used to modify existing table entries,
- * without allocating new levels of table. Note that this permits the
- * creation of new section or page entries.
- */
-static void __init create_mapping_noalloc(phys_addr_t phys, unsigned long virt,
-				  phys_addr_t size, pgprot_t prot)
+static __init void iee_init_pte_pre_init(pmd_t *pmdp, unsigned long addr, unsigned long end,
+		     phys_addr_t phys, pgprot_t prot)
 {
-	if (virt < PAGE_OFFSET) {
-		pr_warn("BUG: not creating mapping for %pa at 0x%016lx - outside kernel range\n",
-			&phys, virt);
-		return;
-	}
-	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot, NULL,
-			     NO_CONT_MAPPINGS);
-}
+	pte_t *ptep;
 
-void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
-			       unsigned long virt, phys_addr_t size,
-			       pgprot_t prot, bool page_mappings_only)
-{
-	int flags = 0;
+	ptep = pte_set_fixmap_offset_init(pmdp, addr);
+	do {
+		pte_t old_pte = READ_ONCE(*ptep);
 
-	BUG_ON(mm == &init_mm);
+		iee_set_pte_pre_init(ptep, pfn_pte(__phys_to_pfn(phys), prot));
 
-	if (page_mappings_only)
-		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
+		/*
+		 * After the PTE entry has been populated once, we
+		 * only allow updates to the permission attributes.
+		 */
+		BUG_ON(!pgattr_change_is_safe(pte_val(old_pte),
+					      READ_ONCE(pte_val(*ptep))));
 
-	__create_pgd_mapping(mm->pgd, phys, virt, size, prot,
-			     pgd_pgtable_alloc, flags);
+		phys += PAGE_SIZE;
+	} while (ptep++, addr += PAGE_SIZE, addr != end);
+
+	pte_clear_fixmap_init();
 }
 
-static void update_mapping_prot(phys_addr_t phys, unsigned long virt,
-				phys_addr_t size, pgprot_t prot)
+static __init void iee_alloc_init_cont_pte_pre_init(pmd_t *pmdp, unsigned long addr,
+				unsigned long end, phys_addr_t phys,
+				pgprot_t prot,
+				phys_addr_t (*pgtable_alloc)(int),
+				int flags)
 {
-	if (virt < PAGE_OFFSET) {
-		pr_warn("BUG: not updating mapping for %pa at 0x%016lx - outside kernel range\n",
-			&phys, virt);
-		return;
+	unsigned long next;
+	pmd_t pmd = READ_ONCE(*pmdp);
+
+	BUG_ON(pmd_sect(pmd));
+	if (pmd_none(pmd)) {
+		phys_addr_t pte_phys;
+		BUG_ON(!pgtable_alloc);
+		pte_phys = pgtable_alloc(PAGE_SHIFT);
+		__iee_pmd_populate_pre_init(pmdp, pte_phys, PMD_TYPE_TABLE);
+		pmd = READ_ONCE(*pmdp);
 	}
+	BUG_ON(pmd_bad(pmd));
 
-	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot, NULL,
-			     NO_CONT_MAPPINGS);
+	do {
+		pgprot_t __prot = prot;
 
-	/* flush the TLBs after updating live kernel mappings */
-	flush_tlb_kernel_range(virt, virt + size);
-}
+		next = pte_cont_addr_end(addr, end);
 
-static void __init __map_memblock(pgd_t *pgdp, phys_addr_t start,
-				  phys_addr_t end, pgprot_t prot, int flags)
-{
-	__create_pgd_mapping(pgdp, start, __phys_to_virt(start), end - start,
-			     prot, early_pgtable_alloc, flags);
-}
+		/* use a contiguous mapping if the range is suitably aligned */
+		if ((((addr | next | phys) & ~CONT_PTE_MASK) == 0) &&
+		    (flags & NO_CONT_MAPPINGS) == 0)
+			__prot = __pgprot(pgprot_val(prot) | PTE_CONT);
 
-void __init mark_linear_text_alias_ro(void)
-{
-	/*
-	 * Remove the write permissions from the linear alias of .text/.rodata
-	 */
-	update_mapping_prot(__pa_symbol(_text), (unsigned long)lm_alias(_text),
-			    (unsigned long)__init_begin - (unsigned long)_text,
-			    PAGE_KERNEL_RO);
+		iee_init_pte_pre_init(pmdp, addr, next, phys, __prot);
+
+		phys += next - addr;
+	} while (addr = next, addr != end);
 }
 
-static void __init map_mem(pgd_t *pgdp)
+static __init void iee_init_pmd_pre_init(pud_t *pudp, unsigned long addr, unsigned long end,
+		     phys_addr_t phys, pgprot_t prot,
+		     phys_addr_t (*pgtable_alloc)(int), int flags)
 {
-	phys_addr_t kernel_start = __pa_symbol(_text);
-	phys_addr_t kernel_end = __pa_symbol(__init_begin);
-	phys_addr_t start, end;
-	int flags = 0, eflags = 0;
+	unsigned long next;
+	pmd_t *pmdp;
+
+	pmdp = pmd_set_fixmap_offset_init(pudp, addr);
+	do {
+		pmd_t old_pmd = READ_ONCE(*pmdp);
+
+		next = pmd_addr_end(addr, end);
+
+		/* try section mapping first */
+		if (((addr | next | phys) & ~SECTION_MASK) == 0 &&
+		    (flags & NO_BLOCK_MAPPINGS) == 0) {
+			iee_pmd_set_huge_pre_init(pmdp, phys, prot);
+
+			/*
+			 * After the PMD entry has been populated once, we
+			 * only allow updates to the permission attributes.
+			 */
+			BUG_ON(!pgattr_change_is_safe(pmd_val(old_pmd),
+						      READ_ONCE(pmd_val(*pmdp))));
+		} else {
+			iee_alloc_init_cont_pte_pre_init(pmdp, addr, next, phys, prot,
+					    pgtable_alloc, flags);
+
+			BUG_ON(pmd_val(old_pmd) != 0 &&
+			       pmd_val(old_pmd) != READ_ONCE(pmd_val(*pmdp)));
+		}
+		phys += next - addr;
+	} while (pmdp++, addr = next, addr != end);
+
+	pmd_clear_fixmap_init();
+}
+
+static __init void iee_alloc_init_cont_pmd_pre_init(pud_t *pudp, unsigned long addr,
+				unsigned long end, phys_addr_t phys,
+				pgprot_t prot,
+				phys_addr_t (*pgtable_alloc)(int), int flags)
+{
+	unsigned long next;
+	pud_t pud = READ_ONCE(*pudp);
+
+	/*
+	 * Check for initial section mappings in the pgd/pud.
+	 */
+	BUG_ON(pud_sect(pud));
+	if (pud_none(pud)) {
+		phys_addr_t pmd_phys;
+		BUG_ON(!pgtable_alloc);
+		pmd_phys = pgtable_alloc(PMD_SHIFT);
+		__iee_pud_populate_pre_init(pudp, pmd_phys, PUD_TYPE_TABLE);
+		pud = READ_ONCE(*pudp);
+	}
+	BUG_ON(pud_bad(pud));
+
+	do {
+		pgprot_t __prot = prot;
+
+		next = pmd_cont_addr_end(addr, end);
+
+		/* use a contiguous mapping if the range is suitably aligned */
+		if ((((addr | next | phys) & ~CONT_PMD_MASK) == 0) &&
+		    (flags & NO_CONT_MAPPINGS) == 0)
+			__prot = __pgprot(pgprot_val(prot) | PTE_CONT);
+
+		iee_init_pmd_pre_init(pudp, addr, next, phys, __prot, pgtable_alloc, flags);
+
+		phys += next - addr;
+	} while (addr = next, addr != end);
+}
+
+static __init void iee_alloc_init_pud_pre_init(pgd_t *pgdp, unsigned long addr, unsigned long end,
+			   phys_addr_t phys, pgprot_t prot,
+			   phys_addr_t (*pgtable_alloc)(int),
+			   int flags)
+{
+	unsigned long next;
+	pud_t *pudp;
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+	p4d_t p4d = READ_ONCE(*p4dp);
+
+	if (p4d_none(p4d)) {
+		phys_addr_t pud_phys;
+		BUG_ON(!pgtable_alloc);
+		pud_phys = pgtable_alloc(PUD_SHIFT);
+		__iee_p4d_populate_pre_init(p4dp, pud_phys, PUD_TYPE_TABLE);
+		p4d = READ_ONCE(*p4dp);
+	}
+	BUG_ON(p4d_bad(p4d));
+
+	pudp = pud_set_fixmap_offset_init(p4dp, addr);
+	do {
+		pud_t old_pud = READ_ONCE(*pudp);
+
+		next = pud_addr_end(addr, end);
+
+		/*
+		 * For 4K granule only, attempt to put down a 1GB block
+		 */
+		if (use_1G_block(addr, next, phys) &&
+		    (flags & NO_BLOCK_MAPPINGS) == 0) {
+			iee_pud_set_huge_pre_init(pudp, phys, prot);
+
+			/*
+			 * After the PUD entry has been populated once, we
+			 * only allow updates to the permission attributes.
+			 */
+			BUG_ON(!pgattr_change_is_safe(pud_val(old_pud),
+						      READ_ONCE(pud_val(*pudp))));
+		} else {
+			iee_alloc_init_cont_pmd_pre_init(pudp, addr, next, phys, prot,
+					    pgtable_alloc, flags);
+
+			BUG_ON(pud_val(old_pud) != 0 &&
+			       pud_val(old_pud) != READ_ONCE(pud_val(*pudp)));
+		}
+		phys += next - addr;
+	} while (pudp++, addr = next, addr != end);
+
+	pud_clear_fixmap_init();
+}
+
+static __init void __iee_create_pgd_mapping_pre_init(pgd_t *pgdir, phys_addr_t phys,
+				 unsigned long virt, phys_addr_t size,
+				 pgprot_t prot,
+				 phys_addr_t (*pgtable_alloc)(int),
+				 int flags)
+{
+	unsigned long addr, end, next;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, virt);
+
+	/*
+	 * If the virtual and physical address don't have the same offset
+	 * within a page, we cannot map the region as the caller expects.
+	 */
+	if (WARN_ON((phys ^ virt) & ~PAGE_MASK))
+		return;
+
+	phys &= PAGE_MASK;
+	addr = virt & PAGE_MASK;
+	end = PAGE_ALIGN(virt + size);
+
+	do {
+		next = pgd_addr_end(addr, end);
+		iee_alloc_init_pud_pre_init(pgdp, addr, next, phys, prot, pgtable_alloc,
+			       flags);
+		phys += next - addr;
+	} while (pgdp++, addr = next, addr != end);
+}
+#endif
+
+static phys_addr_t __pgd_pgtable_alloc(int shift)
+{
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
+	void *ptr = (void *)__get_free_page(GFP_PGTABLE_KERNEL);
+	BUG_ON(!ptr);
+
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(ptr));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)ptr);
+	#endif
+
+	/* Ensure the zeroed page is visible to the page table walker */
+	dsb(ishst);
+	return __pa(ptr);
+}
+
+static phys_addr_t pgd_pgtable_alloc(int shift)
+{
+	phys_addr_t pa = __pgd_pgtable_alloc(shift);
+
+	/*
+	 * Call proper page table ctor in case later we need to
+	 * call core mm functions like apply_to_page_range() on
+	 * this pre-allocated page table.
+	 *
+	 * We don't select ARCH_ENABLE_SPLIT_PMD_PTLOCK if pmd is
+	 * folded, and if so pgtable_pmd_page_ctor() becomes nop.
+	 */
+	if (shift == PAGE_SHIFT)
+		BUG_ON(!pgtable_pte_page_ctor(phys_to_page(pa)));
+	else if (shift == PMD_SHIFT)
+		BUG_ON(!pgtable_pmd_page_ctor(phys_to_page(pa)));
+
+	return pa;
+}
+
+/*
+ * This function can only be used to modify existing table entries,
+ * without allocating new levels of table. Note that this permits the
+ * creation of new section or page entries.
+ */
+static void __init create_mapping_noalloc(phys_addr_t phys, unsigned long virt,
+				  phys_addr_t size, pgprot_t prot)
+{
+	if (virt < PAGE_OFFSET) {
+		pr_warn("BUG: not creating mapping for %pa at 0x%016lx - outside kernel range\n",
+			&phys, virt);
+		return;
+	}
+
+	#ifdef CONFIG_PTP
+	__iee_create_pgd_mapping_pre_init(init_mm.pgd, phys, virt, size, prot, NULL,
+			     NO_CONT_MAPPINGS);
+	#else
+	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot, NULL,
+			     NO_CONT_MAPPINGS);
+	#endif
+}
+
+void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+			       unsigned long virt, phys_addr_t size,
+			       pgprot_t prot, bool page_mappings_only)
+{
+	int flags = 0;
+
+	BUG_ON(mm == &init_mm);
+
+	if (page_mappings_only)
+		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
+
+	__create_pgd_mapping(mm->pgd, phys, virt, size, prot,
+			     pgd_pgtable_alloc, flags);
+}
+
+static void update_mapping_prot(phys_addr_t phys, unsigned long virt,
+				phys_addr_t size, pgprot_t prot)
+{
+	if (virt < PAGE_OFFSET) {
+		pr_warn("BUG: not updating mapping for %pa at 0x%016lx - outside kernel range\n",
+			&phys, virt);
+		return;
+	}
+
+	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot, NULL,
+			     NO_CONT_MAPPINGS);
+
+	/* flush the TLBs after updating live kernel mappings */
+	flush_tlb_kernel_range(virt, virt + size);
+}
+
+static void __init __map_memblock(pgd_t *pgdp, phys_addr_t start,
+				  phys_addr_t end, pgprot_t prot, int flags)
+{
+	#ifdef CONFIG_PTP
+	__iee_create_pgd_mapping_pre_init(pgdp, start, __phys_to_virt(start), end - start,
+			     prot, early_pgtable_alloc, flags);
+	#else
+	__create_pgd_mapping(pgdp, start, __phys_to_virt(start), end - start,
+			     prot, early_pgtable_alloc, flags);
+	#endif
+}
+
+void __init mark_linear_text_alias_ro(void)
+{
+	/*
+	 * Remove the write permissions from the linear alias of .text/.rodata
+	 */
+	update_mapping_prot(__pa_symbol(_text), (unsigned long)lm_alias(_text),
+			    (unsigned long)__init_begin - (unsigned long)_text,
+			    PAGE_KERNEL_RO);
+}
+
+static void __init map_mem(pgd_t *pgdp)
+{
+	phys_addr_t kernel_start = __pa_symbol(_text);
+	phys_addr_t kernel_end = __pa_symbol(__init_begin);
+	phys_addr_t start, end;
+	int flags = 0, eflags = 0;
 	u64 i;
 
 	if (rodata_full || debug_pagealloc_enabled())
 		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
 
+	#ifdef CONFIG_IEE
+	flags |= NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
+	#endif
+
 #ifdef CONFIG_KFENCE
 	/*
 	 * KFENCE requires linear map to be mapped at page granularity, so
@@ -565,8 +1160,13 @@ static void __init map_mem(pgd_t *pgdp)
 	 * Note that contiguous mappings cannot be remapped in this way,
 	 * so we should avoid them here.
 	 */
+	#ifdef CONFIG_IEE
+	__map_memblock(pgdp, kernel_start, kernel_end,
+		       PAGE_KERNEL, flags);
+	#else
 	__map_memblock(pgdp, kernel_start, kernel_end,
 		       PAGE_KERNEL, NO_CONT_MAPPINGS);
+	#endif
 	memblock_clear_nomap(kernel_start, kernel_end - kernel_start);
 
 #ifdef CONFIG_KFENCE
@@ -599,173 +1199,1135 @@ static void __init map_mem(pgd_t *pgdp)
 #endif
 }
 
-void mark_rodata_ro(void)
+void mark_rodata_ro(void)
+{
+	unsigned long section_size;
+
+	/*
+	 * mark .rodata as read only. Use __init_begin rather than __end_rodata
+	 * to cover NOTES and EXCEPTION_TABLE.
+	 */
+	section_size = (unsigned long)__init_begin - (unsigned long)__start_rodata;
+	update_mapping_prot(__pa_symbol(__start_rodata), (unsigned long)__start_rodata,
+			    section_size, PAGE_KERNEL_RO);
+
+	debug_checkwx();
+}
+
+static void __init map_kernel_segment(pgd_t *pgdp, void *va_start, void *va_end,
+				      pgprot_t prot, struct vm_struct *vma,
+				      int flags, unsigned long vm_flags)
+{
+	phys_addr_t pa_start = __pa_symbol(va_start);
+	unsigned long size = va_end - va_start;
+
+	BUG_ON(!PAGE_ALIGNED(pa_start));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	#ifdef CONFIG_PTP
+	__iee_create_pgd_mapping_pre_init(pgdp, pa_start, (unsigned long)va_start, size, prot,
+			     early_pgtable_alloc, flags);
+	#else
+	__create_pgd_mapping(pgdp, pa_start, (unsigned long)va_start, size, prot,
+			     early_pgtable_alloc, flags);
+	#endif
+
+	if (!(vm_flags & VM_NO_GUARD))
+		size += PAGE_SIZE;
+
+	vma->addr	= va_start;
+	vma->phys_addr	= pa_start;
+	vma->size	= size;
+	vma->flags	= VM_MAP | vm_flags;
+	vma->caller	= __builtin_return_address(0);
+
+	vm_area_add_early(vma);
+}
+
+#ifdef CONFIG_UNMAP_KERNEL_AT_EL0
+static int __init map_entry_trampoline(void)
+{
+	int i;
+
+	pgprot_t prot = rodata_enabled ? PAGE_KERNEL_ROX : PAGE_KERNEL_EXEC;
+	phys_addr_t pa_start = __pa_symbol(__entry_tramp_text_start);
+
+	/* The trampoline is always mapped and can therefore be global */
+	pgprot_val(prot) &= ~PTE_NG;
+
+	/* Map only the text into the trampoline page table */
+	memset(tramp_pg_dir, 0, PGD_SIZE);
+	#ifdef CONFIG_PTP
+	iee_set_logical_mem_ro((unsigned long)tramp_pg_dir);
+	#endif
+	__create_pgd_mapping(tramp_pg_dir, pa_start, TRAMP_VALIAS,
+			     entry_tramp_text_size(), prot,
+			     __pgd_pgtable_alloc, NO_BLOCK_MAPPINGS);
+
+	/* Map both the text and data into the kernel page table */
+	for (i = 0; i < DIV_ROUND_UP(entry_tramp_text_size(), PAGE_SIZE); i++)
+		__set_fixmap(FIX_ENTRY_TRAMP_TEXT1 - i,
+			     pa_start + i * PAGE_SIZE, prot);
+
+	if (IS_ENABLED(CONFIG_RANDOMIZE_BASE)) {
+		extern char __entry_tramp_data_start[];
+
+		__set_fixmap(FIX_ENTRY_TRAMP_DATA,
+			     __pa_symbol(__entry_tramp_data_start),
+			     PAGE_KERNEL_RO);
+	}
+
+	return 0;
+}
+core_initcall(map_entry_trampoline);
+#endif
+
+/*
+ * Open coded check for BTI, only for use to determine configuration
+ * for early mappings for before the cpufeature code has run.
+ */
+static bool arm64_early_this_cpu_has_bti(void)
+{
+	u64 pfr1;
+
+	if (!IS_ENABLED(CONFIG_ARM64_BTI_KERNEL))
+		return false;
+
+	pfr1 = read_sysreg_s(SYS_ID_AA64PFR1_EL1);
+	return cpuid_feature_extract_unsigned_field(pfr1,
+						    ID_AA64PFR1_BT_SHIFT);
+}
+
+/*
+ * Create fine-grained mappings for the kernel.
+ */
+static void __init map_kernel(pgd_t *pgdp)
+{
+	static struct vm_struct vmlinux_text, vmlinux_rodata, vmlinux_inittext,
+				vmlinux_initdata, vmlinux_data;
+	
+	#ifdef CONFIG_IEE
+	static struct vm_struct vmlinux_text_idmap, vmlinux_iee_base, vmlinux_iee_code, 
+				vmlinux_iee_data, vmlinux_text_end;
+	#endif
+
+	/*
+	 * External debuggers may need to write directly to the text
+	 * mapping to install SW breakpoints. Allow this (only) when
+	 * explicitly requested with rodata=off.
+	 */
+	pgprot_t text_prot = rodata_enabled ? PAGE_KERNEL_ROX : PAGE_KERNEL_EXEC;
+
+	/*
+	 * If we have a CPU that supports BTI and a kernel built for
+	 * BTI then mark the kernel executable text as guarded pages
+	 * now so we don't have to rewrite the page tables later.
+	 */
+	if (arm64_early_this_cpu_has_bti())
+		text_prot = __pgprot_modify(text_prot, PTE_GP, PTE_GP);
+
+	/*
+	 * Only rodata will be remapped with different permissions later on,
+	 * all other segments are allowed to use contiguous mappings.
+	 */
+	#ifdef CONFIG_IEE
+	map_kernel_segment(pgdp, _text, __idmap_text_start, text_prot, &vmlinux_text,
+			   0, VM_NO_GUARD);
+	/*
+	 * Cancel contiguous mappings of idmap and iee si sections to 
+	 * simplify modifications later.
+	 */
+	map_kernel_segment(pgdp, __idmap_text_start, __iee_si_base_start, text_prot, &vmlinux_text_idmap,
+			   NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	/*
+	 * Map iee_si_base and iee_si_data as RW page, iee codes as NG page.
+	 */
+	map_kernel_segment(pgdp, __iee_si_base_start, __iee_exec_entry_start, SET_NG(PAGE_KERNEL), &vmlinux_iee_base,
+			   NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __iee_exec_entry_start, __iee_si_data_start, SET_NG(text_prot), &vmlinux_iee_code,
+			   NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __iee_si_data_start, __iee_si_data_end, SET_NG(PAGE_KERNEL), &vmlinux_iee_data,
+			   NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __iee_si_data_end, _etext, SET_NG(text_prot), &vmlinux_text_end,
+			   NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __start_rodata, __inittext_begin, PAGE_KERNEL,
+			   &vmlinux_rodata, NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __inittext_begin, __inittext_end, text_prot,
+			   &vmlinux_inittext, 0, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __initdata_begin, __initdata_end, PAGE_KERNEL,
+			   &vmlinux_initdata, 0, VM_NO_GUARD);
+	map_kernel_segment(pgdp, _data, _end, PAGE_KERNEL, &vmlinux_data, NO_CONT_MAPPINGS | NO_BLOCK_MAPPINGS, 0);
+	#else
+	map_kernel_segment(pgdp, _text, _etext, text_prot, &vmlinux_text, 0,
+			   VM_NO_GUARD);
+	map_kernel_segment(pgdp, __start_rodata, __inittext_begin, PAGE_KERNEL,
+			   &vmlinux_rodata, NO_CONT_MAPPINGS, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __inittext_begin, __inittext_end, text_prot,
+			   &vmlinux_inittext, 0, VM_NO_GUARD);
+	map_kernel_segment(pgdp, __initdata_begin, __initdata_end, PAGE_KERNEL,
+			   &vmlinux_initdata, 0, VM_NO_GUARD);
+	map_kernel_segment(pgdp, _data, _end, PAGE_KERNEL, &vmlinux_data, 0, 0);
+	#endif
+
+
+	if (!READ_ONCE(pgd_val(*pgd_offset_pgd(pgdp, FIXADDR_START)))) {
+		/*
+		 * The fixmap falls in a separate pgd to the kernel, and doesn't
+		 * live in the carveout for the swapper_pg_dir. We can simply
+		 * re-use the existing dir for the fixmap.
+		 */
+		#ifdef CONFIG_PTP
+		set_pgd_init(pgd_offset_pgd(pgdp, FIXADDR_START),
+			READ_ONCE(*pgd_offset_k(FIXADDR_START)));
+		#else
+		set_pgd(pgd_offset_pgd(pgdp, FIXADDR_START),
+			READ_ONCE(*pgd_offset_k(FIXADDR_START)));
+		#endif
+	} else if (CONFIG_PGTABLE_LEVELS > 3) {
+		pgd_t *bm_pgdp;
+		p4d_t *bm_p4dp;
+		pud_t *bm_pudp;
+		/*
+		 * The fixmap shares its top level pgd entry with the kernel
+		 * mapping. This can really only occur when we are running
+		 * with 16k/4 levels, so we can simply reuse the pud level
+		 * entry instead.
+		 */
+		BUG_ON(!IS_ENABLED(CONFIG_ARM64_16K_PAGES));
+		bm_pgdp = pgd_offset_pgd(pgdp, FIXADDR_START);
+		bm_p4dp = p4d_offset(bm_pgdp, FIXADDR_START);
+		#ifdef CONFIG_PTP
+		bm_pudp = pud_set_fixmap_offset_init(bm_p4dp, FIXADDR_START);
+		__iee_pud_populate_pre_init(bm_pudp, __pa(lm_alias(bm_pmd)), PMD_TYPE_TABLE);
+		pud_clear_fixmap_init();
+		#else
+		bm_pudp = pud_set_fixmap_offset(bm_p4dp, FIXADDR_START);
+		pud_populate(&init_mm, bm_pudp, lm_alias(bm_pmd));
+		pud_clear_fixmap();
+		#endif
+	} else {
+		BUG();
+	}
+
+	kasan_copy_shadow(pgdp);
+}
+
+#ifdef CONFIG_IEE
+static void __create_pgd_mapping_for_iee(pgd_t *pgdir, phys_addr_t phys,
+				 unsigned long virt, phys_addr_t size,
+				 pgprot_t prot,
+				 phys_addr_t (*pgtable_alloc)(int),
+				 int flags)
+{
+	unsigned long addr, end, next;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, virt);
+	p4d_t *p4dp;
+	p4d_t p4d;
+
+	/*
+	 * If the virtual and physical address don't have the same offset
+	 * within a page, we cannot map the region as the caller expects.
+	 */
+	if (WARN_ON((phys ^ virt) & ~PAGE_MASK))
+		return;
+
+	phys &= PAGE_MASK;
+	addr = virt & PAGE_MASK;
+	end = PAGE_ALIGN(virt + size);
+
+	do {
+		next = pgd_addr_end(addr, end);
+		#ifdef CONFIG_PTP
+		iee_alloc_init_pud_pre_init(pgdp, addr, next, phys, prot, pgtable_alloc,
+			       flags);
+		#else
+		alloc_init_pud(pgdp, addr, next, phys, prot, pgtable_alloc,
+			       flags);
+		#endif
+		p4dp = p4d_offset(pgdp, addr);
+		p4d = READ_ONCE(*p4dp);
+		#ifdef CONFIG_PTP
+		__iee_p4d_populate_pre_init(p4dp, __p4d_to_phys(p4d), (PGD_APT | PGD_PXN | PGD_UXN | PUD_TYPE_TABLE));
+		#else
+		__p4d_populate(p4dp, __p4d_to_phys(p4d), (PGD_APT | PGD_PXN | PGD_UXN | PUD_TYPE_TABLE));
+		#endif
+		phys += next - addr;
+	} while (pgdp++, addr = next, addr != end);
+}
+
+static void __init __map_memblock_for_iee(pgd_t *pgdp, phys_addr_t start,
+				  phys_addr_t end, pgprot_t prot, int flags)
+{
+	#ifdef CONFIG_PTP
+	__create_pgd_mapping_for_iee(pgdp, start, __phys_to_iee(start), end - start,
+			     prot, early_pgtable_alloc, flags);
+	#else
+	__create_pgd_mapping_for_iee(pgdp, start, __phys_to_iee(start), end - start,
+			     prot, early_pgtable_alloc, flags);
+	#endif
+}
+
+static void __init map_iee(pgd_t *pgdp)
+{
+	phys_addr_t kernel_start = __pa_symbol(_text);
+	phys_addr_t kernel_end = __pa_symbol(__init_begin);
+	phys_addr_t start, end;
+	int flags = 0;
+	u64 i;
+
+	flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
+
+	/*
+	 * Take care not to create a writable alias for the
+	 * read-only text and rodata sections of the kernel image.
+	 * So temporarily mark them as NOMAP to skip mappings in
+	 * the following for-loop
+	 */
+	memblock_mark_nomap(kernel_start, kernel_end - kernel_start);
+#ifdef CONFIG_KEXEC_CORE
+	if (crashk_res.end)
+		memblock_mark_nomap(crashk_res.start,
+				    resource_size(&crashk_res));
+#endif
+
+	/* map all the memory banks */
+	for_each_mem_range(i, &start, &end) {
+		if (start >= end)
+			break;
+		/*
+		 * The linear map must allow allocation tags reading/writing
+		 * if MTE is present. Otherwise, it has the same attributes as
+		 * PAGE_KERNEL.
+		 */
+		__map_memblock_for_iee(pgdp, start, end, SET_NG(SET_INVALID(SET_UPAGE(PAGE_KERNEL))), flags);
+	}
+
+	/*
+	 * Map the linear alias of the [_text, __init_begin) interval
+	 * as non-executable now, and remove the write permission in
+	 * mark_linear_text_alias_ro() below (which will be called after
+	 * alternative patching has completed). This makes the contents
+	 * of the region accessible to subsystems such as hibernate,
+	 * but protects it from inadvertent modification or execution.
+	 * Note that contiguous mappings cannot be remapped in this way,
+	 * so we should avoid them here.
+	 */
+	__map_memblock_for_iee(pgdp, kernel_start, kernel_end,
+		       SET_NG(SET_INVALID(SET_UPAGE(PAGE_KERNEL))), flags);
+	memblock_clear_nomap(kernel_start, kernel_end - kernel_start);
+
+#ifdef CONFIG_KEXEC_CORE
+	/*
+	 * Use page-level mappings here so that we can shrink the region
+	 * in page granularity and put back unused memory to buddy system
+	 * through /sys/kernel/kexec_crash_size interface.
+	 */
+	if (crashk_res.end) {
+		__map_memblock_for_iee(pgdp, crashk_res.start, crashk_res.end + 1,
+			       SET_NG(SET_INVALID(SET_UPAGE(PAGE_KERNEL))),
+			       NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS);
+		memblock_clear_nomap(crashk_res.start,
+				     resource_size(&crashk_res));
+	}
+#endif
+}
+
+/*
+ * Change page access permission, whereas not handling huge pages.
+ * Only used on IEE init functions.
+ */ 
+static int __init iee_si_set_page_attr(unsigned long addr, pteval_t attr)
+{
+    pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+    pte_t *ptep = pte_offset_kernel(pmdp, addr);
+    pte_t pte = READ_ONCE(*ptep);
+    pr_info("IEE: page access permission changed on address 0x%lx, curr pte val: 0x%lx",
+            addr, (unsigned long)pte_val(pte));
+    if(attr & PTE_RDONLY)
+        pte = __pte((pte_val(pte) | PTE_RDONLY) & ~PTE_DBM);
+    pte = __pte(pte_val(pte) | attr);
+    if((pte_val(pte) & PTE_USER) && !(pte_val(*ptep) & PTE_USER))
+	{
+		iee_set_pte_upage(ptep, __pte(pte_val(*ptep) | PTE_USER));
+		set_pte(ptep, pte);
+	}
+	else
+        set_pte(ptep, pte);
+    pr_info("IEE: modified pte val: 0x%lx", (unsigned long)pte_val(pte));
+    return 1;
+}
+
+/*
+ * Copy one pte. Returns 0 if succeeded.
+ */
+static inline int iee_si_copy_present_pte(pte_t *dst_pte, pte_t *src_pte,
+				   unsigned long addr)
+{
+	pte_t pte = *src_pte;
+
+	// if ((pte_val(pte) & PTE_NG) == 0) {
+	// 	printk(KERN_ERR "global pte found: 0x%16llx\n", pte);
+	// } else {
+	// 	printk(KERN_ERR "non global pte found: 0x%16llx\n", pte);
+	// }
+	set_pte(dst_pte, pte);
+	/* Clear src pte after copy to hide this page from kernel. */
+	printk("IEE: Copy src pte %lx to %lx done. Hiding it new...", (unsigned long)src_pte, (unsigned long)dst_pte);
+	set_pte(src_pte, __pte(0));
+	return 0;
+}
+
+/*
+ * copy huge pmd entry from kernel to iee.
+ */
+static int __init iee_si_copy_huge_pmd(struct mm_struct *mm, pmd_t *dst_pmd,
+			    pmd_t *src_pmd, unsigned long addr)
+{
+	pmd_t pmd;
+	int ret = -ENOMEM;
+
+	ret = -EAGAIN;
+	pmd = *src_pmd;
+	mm_inc_nr_ptes(mm);
+
+	set_pte((pte_t *)dst_pmd, pmd_pte(pmd));
+	ret = 0;
+	return ret;
+}
+/*
+ * Allocate page table directory for IEE SI codes.
+ * Would do pmd_populate anyway and overwrite pmd entry. 
+ */
+static int __init __iee_si_pte_alloc_kernel(pmd_t *pmd)
+{
+	unsigned long iee_addr;
+	pte_t *new = pte_alloc_one_kernel(&init_mm);
+	if (!new)
+		return -ENOMEM;
+
+	smp_wmb(); /* See comment in __pte_alloc */
+
+	spin_lock(&init_mm.page_table_lock);
+	#ifdef CONFIG_PTP
+	/* Valid iee mapping of the allocated page. */
+	iee_addr = __phys_to_iee(__pa(new));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)new);
+	#endif
+	pmd_populate_kernel(&init_mm, pmd, new);
+	spin_unlock(&init_mm.page_table_lock);
+	return 0;
+}
+
+#define iee_si_pte_alloc_kernel(pmd, address)			\
+	((__iee_si_pte_alloc_kernel(pmd))? \
+		NULL: pte_offset_kernel(pmd, address))
+
+/*
+ * Copy pte table from kernel mappings to iee mappings
+ */
+static int __init iee_si_copy_pte_range(struct mm_struct *mm, pmd_t *dst_pmd,
+			  pmd_t *src_pmd, unsigned long addr, unsigned long end)
+{
+	pte_t *src_pte, *dst_pte;
+	unsigned long src_pte_t_pg, dst_pte_t_pg;
+	int ret = 0;
+again:
+	dst_pte = iee_si_pte_alloc_kernel(dst_pmd, addr);
+	if (!dst_pte) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	src_pte = pte_offset_kernel(src_pmd, addr);
+
+	#ifdef CONFIG_PTP
+	// Get va of new and old pte table page.
+    dst_pte_t_pg = (unsigned long)__phys_to_iee(pmd_page_paddr(*dst_pmd));
+    src_pte_t_pg = (unsigned long)__phys_to_iee(pmd_page_paddr(*src_pmd));
+	// printk("IEE: src_pte_t:%lx, dst_pte_t:%lx", src_pte_t_pg, dst_pte_t_pg);
+    // Copy all content from source pte table. Disable pan to manipulate inside iee.
+	asm volatile("msr pan, #0");
+    memcpy((void *)dst_pte_t_pg, (void *)src_pte_t_pg, PAGE_SIZE);
+	asm volatile("msr pan, #1");
+	#else
+	// Get va of new and old pte table page.
+    dst_pte_t_pg = (unsigned long)__va(pmd_page_paddr(*dst_pmd));
+    src_pte_t_pg = (unsigned long)__va(pmd_page_paddr(*src_pmd));
+	// printk("IEE: src_pte_t:%lx, dst_pte_t:%lx", src_pte_t_pg, dst_pte_t_pg);
+    // Copy all content from source pte table. Disable pan to manipulate inside iee.
+    memcpy(dst_pte_t_pg, src_pte_t_pg, PAGE_SIZE);
+	#endif
+
+	arch_enter_lazy_mmu_mode();
+	do {
+		if (pte_none(*src_pte))
+			continue;
+		if (unlikely(!pte_present(*src_pte))) {
+			continue;
+		}
+		ret = iee_si_copy_present_pte(dst_pte, src_pte, addr);
+		if (unlikely(ret == -EAGAIN))
+			break;
+	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
+	arch_leave_lazy_mmu_mode();
+
+	if (ret) {
+		WARN_ON_ONCE(ret != -EAGAIN);
+		ret = 0;
+	}
+	if (addr != end)
+		goto again;
+out:
+	return ret;
+}
+
+#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)
+static inline int __iee_si_pmd_alloc(struct mm_struct *mm, pud_t *pud,
+						unsigned long address)
+{
+	return 0;
+}
+#endif
+
+#ifndef __PAGETABLE_PMD_FOLDED
+/*
+ * Allocate page middle directory for IEE SI codes.
+ * Would do pud_populate anyway and overwrite pud entry. 
+ */
+static int __init __iee_si_pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
+{
+	unsigned long iee_addr;
+	spinlock_t *ptl;
+	pmd_t *new = pmd_alloc_one(mm, address);
+	if (!new)
+		return -ENOMEM;
+
+	smp_wmb(); /* See comment in __pte_alloc */
+
+	ptl = pud_lock(mm, pud);
+	#ifdef CONFIG_PTP
+	/* Valid iee mapping of the allocated page. */
+	iee_addr = __phys_to_iee(__pa(new));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)new);
+	#endif
+    mm_inc_nr_pmds(mm);
+    pud_populate(mm, pud, new);
+	spin_unlock(ptl);
+	return 0;
+}
+#endif /* __PAGETABLE_PMD_FOLDED */
+
+static inline pmd_t *iee_si_pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
+{
+	return (__iee_si_pmd_alloc(mm, pud, address))?
+		NULL: pmd_offset(pud, address);
+}
+
+/*
+ * Copy pmd table from kernel mappings to iee mappings
+ */
+static inline int iee_si_copy_pmd_range(struct mm_struct *mm, pud_t *dst_pud,
+				 pud_t *src_pud, unsigned long addr,
+				 unsigned long end)
+{
+	pmd_t *src_pmd, *dst_pmd;
+	unsigned long next, src_pmd_t_pg, dst_pmd_t_pg;
+	int err;
+
+	dst_pmd = iee_si_pmd_alloc(mm, dst_pud, addr);
+	if (!dst_pmd) {
+		return -ENOMEM;
+	}
+	src_pmd = pmd_offset(src_pud, addr);
+	#ifdef CONFIG_PTP
+    // Get va of new and old pmd table page.
+    dst_pmd_t_pg = (unsigned long)__phys_to_iee(pud_page_paddr(*dst_pud));
+    src_pmd_t_pg = (unsigned long)__phys_to_iee(pud_page_paddr(*src_pud));
+	// printk("IEE: src_pmd_t:%lx, dst_pmd_t:%lx", src_pmd_t_pg, dst_pmd_t_pg);
+    // Copy all content from source pmd table. Disable pan to manipulate inside iee.
+	asm volatile("msr pan, #0");
+    memcpy((void *)dst_pmd_t_pg, (void *)src_pmd_t_pg, PAGE_SIZE);
+	asm volatile("msr pan, #1");
+	#else
+	// Get va of new and old pmd table page.
+    dst_pmd_t_pg = (unsigned long)__va(pud_page_paddr(*dst_pud));
+    src_pmd_t_pg = (unsigned long)__va(pud_page_paddr(*src_pud));
+	// printk("IEE: src_pmd_t:%lx, dst_pmd_t:%lx", src_pmd_t_pg, dst_pmd_t_pg);
+    // Copy all content from source pmd table. Disable pan to manipulate inside iee.
+    memcpy(dst_pmd_t_pg, src_pmd_t_pg, PAGE_SIZE);
+	#endif
+	do {
+		next = pmd_addr_end(addr, end);
+		// CONFIG_TRANSPARENT_HUGEPAGE is enabled, so we must add copy_huge_pmd
+		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd) ||
+		    (pmd_devmap(*src_pmd))) {
+			err = iee_si_copy_huge_pmd(mm, dst_pmd, src_pmd,
+					       addr);
+			if (err == -ENOMEM)
+				return -ENOMEM;
+			if (!err)
+				continue;
+		}
+		if (pmd_none_or_clear_bad(src_pmd)) {
+			continue;
+		}
+		if (iee_si_copy_pte_range(mm, dst_pmd, src_pmd, addr, next))
+			return -ENOMEM;
+	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
+	return 0;
+}
+
+#if defined(__PAGETABLE_PUD_FOLDED) || !defined(CONFIG_MMU)
+static inline int __iee_si_pud_alloc(struct mm_struct *mm, p4d_t *p4d,
+						unsigned long address)
+{
+	return 0;
+}
+#endif
+
+#ifndef __PAGETABLE_PUD_FOLDED
+/*
+ * Allocate page upper directory for IEE SI codes.
+ * Would do p4d_populate anyway and overwrite p4d entry. 
+ */
+static int __init __iee_si_pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)
+{
+	unsigned long iee_addr;
+	pud_t *new = pud_alloc_one(mm, address);
+	if (!new)
+		return -ENOMEM;
+
+	smp_wmb(); /* See comment in __pte_alloc */
+
+	spin_lock(&mm->page_table_lock);
+	#ifdef CONFIG_PTP
+	/* Valid iee mapping of the allocated page. */
+	iee_addr = __phys_to_iee(__pa(new));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)new);
+	// printk("IEE: allocated new pud, set iee:%lx valid...", iee_addr);
+	#endif
+    mm_inc_nr_puds(mm);
+	// printk("IEE: populating to p4d, new val:%lx", new);
+    p4d_populate(mm, p4d, new);
+	spin_unlock(&mm->page_table_lock);
+	return 0;
+}
+#endif /* __PAGETABLE_PUD_FOLDED */
+
+static inline pud_t *iee_si_pud_alloc(struct mm_struct *mm, p4d_t *p4d,
+		unsigned long address)
+{
+	return (__iee_si_pud_alloc(mm, p4d, address)) ?
+		NULL : pud_offset(p4d, address);
+}
+
+/*
+ * Copy pud table from kernel mappings to iee mappings
+ */
+static inline int iee_si_copy_pud_range(struct mm_struct *mm, p4d_t *dst_p4d,
+				 p4d_t *src_p4d, unsigned long addr,
+				 unsigned long end)
+{
+	pud_t *src_pud, *dst_pud;
+	unsigned long next, src_pud_t_pg, dst_pud_t_pg;
+	dst_pud = iee_si_pud_alloc(mm, dst_p4d, addr);
+	if (!dst_pud)
+		return -ENOMEM;
+	src_pud = pud_offset(src_p4d, addr);
+	#ifdef CONFIG_PTP
+    // Get iee va of new and old PUD table page.
+	// printk("IEE: src_p4d_t:%lx, dst_p4d_t:%lx", src_p4d, dst_p4d);
+    dst_pud_t_pg = (unsigned long)__phys_to_iee(p4d_page_paddr(*dst_p4d));
+    src_pud_t_pg = (unsigned long)__phys_to_iee(p4d_page_paddr(*src_p4d));
+	// printk("IEE: src_pud_t:%lx, dst_pud_t:%lx", src_pud_t_pg, dst_pud_t_pg);
+    // Copy all content from source PUD table. Disable pan to manipulate inside iee. 
+	asm volatile("msr pan, #0");
+    memcpy((void *)dst_pud_t_pg, (void *)src_pud_t_pg, PAGE_SIZE);
+	asm volatile("msr pan, #1");
+	#else
+	// Get iee va of new and old PUD table page.
+	// printk("IEE: src_p4d_t:%lx, dst_p4d_t:%lx", src_p4d, dst_p4d);
+    dst_pud_t_pg = (unsigned long)__va(p4d_page_paddr(*dst_p4d));
+    src_pud_t_pg = (unsigned long)__va(p4d_page_paddr(*src_p4d));
+	// printk("IEE: src_pud_t:%lx, dst_pud_t:%lx", src_pud_t_pg, dst_pud_t_pg);
+    // Copy all content from source PUD table. Disable pan to manipulate inside iee. 
+    memcpy(dst_pud_t_pg, src_pud_t_pg, PAGE_SIZE);
+	#endif
+	do {
+		next = pud_addr_end(addr, end);
+		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
+			continue;
+			/* fall through */
+		}
+		if (pud_none_or_clear_bad(src_pud))
+			continue;
+		if (iee_si_copy_pmd_range(mm, dst_pud, src_pud, addr, next))
+			return -ENOMEM;
+	} while (dst_pud++, src_pud++, addr = next, addr != end);
+	return 0;
+}
+
+/**
+ * Copy mappings from kernel pagetables to iee pagetables level by level
+ * @mm: the corresponding mm_struct of the new pgd.
+ * @dst_pgd: destination pgd entry
+ * @src_pgd: source pgd entry
+ * @addr: the start address of protected zone
+ * @end:  the end address of protected zone
+ */
+static inline int iee_si_copy_p4d_range(struct mm_struct *mm, pgd_t *dst_pgd,
+				 pgd_t *src_pgd, unsigned long addr,
+				 unsigned long end)
+{
+	p4d_t *src_p4d, *dst_p4d;
+	unsigned long next;
+	dst_p4d = p4d_alloc(mm, dst_pgd, addr);
+	if (!dst_p4d)
+		return -ENOMEM;
+	src_p4d = p4d_offset(src_pgd, addr);
+	do {
+		next = p4d_addr_end(addr, end);
+		if (p4d_none_or_clear_bad(src_p4d))
+			continue;
+		if (iee_si_copy_pud_range(mm, dst_p4d, src_p4d, addr, next)) {
+			return -ENOMEM;
+		}
+	} while (dst_p4d++, src_p4d++, addr = next, addr != end);
+	return 0;
+}
+
+/**
+ * int iee_si_copy_and_clear - Copy mappings of given address range from kernel to iee,
+ * and clear them on kernel pgd to make the isolation. 
+ * The given region would be protected and invisible to kernel.
+ * Remind that iee_pg_dir would reuse kernel pgtable entries so iee can access kernel easily.
+ * @mm: the corresponding mm_struct of the new pgd. if it is not init_mm then kernel would take 
+ * this pgd as users.
+ * @src_pg_dir: original kernel pgd, swapper_pg_dir
+ * @addr: the starting address of protected area
+ * @end:  the end address of protected area
+ */
+static int __init iee_si_copy_and_clear(struct mm_struct *mm, pgd_t *src_pg_dir,
+		   unsigned long addr, unsigned long end)
+{
+	int ret = 0;
+	unsigned long next;
+	pgd_t *src_pgd, *dst_pgd;
+    // Copy all entries from src_pg_dir first to reuse the original mappings.
+    memcpy(iee_pg_dir, src_pg_dir, PAGE_SIZE);
+
+	src_pgd = pgd_offset_pgd(src_pg_dir, addr);
+	dst_pgd = pgd_offset_pgd(iee_pg_dir, addr);
+	// printk("IEE: src_pgd:%lx, dst_pgd:%lx", src_pgd, dst_pgd);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none_or_clear_bad(src_pgd))
+			continue;
+		if (unlikely(iee_si_copy_p4d_range(mm, dst_pgd, src_pgd, addr,
+					    next))) {
+			ret = -ENOMEM;
+			break;
+		}
+	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
+
+	return ret;
+}
+
+void inline iee_si_set_base_swapper_cnp(void)
+{
+	/* phys_to_ttbr() zeros lower 2 bits of ttbr with 52-bit PA */
+	if (system_supports_cnp()) {
+		iee_base_swapper_pg_dir |= TTBR_CNP_BIT;
+	}
+}
+
+/* map idmap function inside vmalloc region to be ROU pages too. */
+void __init mark_idmap_vmallc_map_ROU(void)
+{
+    unsigned long va = (unsigned long)__idmap_text_start;
+    // protect vmalloc mapping of idmap function page.
+    if(iee_si_set_page_attr(va, PTE_PXN | PTE_UXN | PTE_USER | PTE_RDONLY))
+        pr_info("IEE: idmap functions protected.\n");
+    else
+        pr_info("IEE warning: idmap protection failed.\n");
+}
+
+extern unsigned long __iee_si_text_start[];
+extern unsigned long __iee_exec_exit_pg[];
+
+/* Remap protected area on iee pgd to achieve the isolation. */
+void __init isolate_iee_si(void)
+{
+	/* Modify page prot of iee si setions before copy mappings. */
+	unsigned long va = (unsigned long)__iee_si_base_start;
+	iee_si_set_page_attr(va, PTE_PXN | PTE_UXN | PTE_USER | PTE_RDONLY | PTE_NG);
+	va = (unsigned long)__iee_si_data_start;
+	iee_si_set_page_attr(va, PTE_PXN | PTE_UXN | PTE_USER | PTE_RDONLY | PTE_NG);
+	
+	/* Build the isolation on two mappings. */
+	if(iee_si_copy_and_clear(&init_mm, swapper_pg_dir,(unsigned long)__iee_si_text_start,
+			(unsigned long)__iee_exec_exit_pg) == 0)
+		pr_info("IEE: isolated region is built successfully.");
+}
+#endif
+
+#ifdef CONFIG_PTP
+// Attention : Using set_xxx without adding offset.
+static void __init set_iee_valid_pre_init(unsigned long addr)
+{
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
+
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+
+	pud_t *pudp = pud_offset(p4dp, addr);
+
+	pmd_t *pmdp = pmd_offset(pudp, addr);
+
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
+	pte_t pte = READ_ONCE(*ptep);
+
+	if((addr < (((unsigned long)0xffff << 48) + IEE_OFFSET)) | (addr > (((unsigned long)0xffff8 << 44))))
+		return;
+
+	pte = __pte(pte_val(pte) | 0x1);
+	iee_set_pte_pre_init(ptep, pte);
+	flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	isb();
+}
+
+static void __init move_pte_table_into_iee(pmd_t *pmdp, unsigned long addr, unsigned long end)
 {
-	unsigned long section_size;
+	pmd_t pmd = READ_ONCE(*pmdp);
+	unsigned long iee_addr = __phys_to_iee(__pmd_to_phys(pmd));
+	set_iee_valid_pre_init(iee_addr);
+}
 
-	/*
-	 * mark .rodata as read only. Use __init_begin rather than __end_rodata
-	 * to cover NOTES and EXCEPTION_TABLE.
-	 */
-	section_size = (unsigned long)__init_begin - (unsigned long)__start_rodata;
-	update_mapping_prot(__pa_symbol(__start_rodata), (unsigned long)__start_rodata,
-			    section_size, PAGE_KERNEL_RO);
+static void __init move_pmd_table_into_iee(pud_t *pudp, unsigned long addr, unsigned long end)
+{
+	unsigned long next;
+	pud_t pud = READ_ONCE(*pudp);
+	pmd_t *pmdp;
+	pmd_t pmd;
+	unsigned long iee_addr = __phys_to_iee(__pud_to_phys(pud));
+	set_iee_valid_pre_init(iee_addr);
 
-	debug_checkwx();
+	pmdp = pmd_offset(pudp, addr);
+	do {
+		next = pmd_addr_end(addr, end);
+		pmd = READ_ONCE(*pmdp);
+		if((pmd_val(pmd) & PMD_TABLE_BIT) == 0)
+		{
+			continue;
+		}
+		else
+		{
+			move_pte_table_into_iee(pmdp, addr, next);
+		}
+	} while (pmdp++, addr = next, addr != end);
 }
 
-static void __init map_kernel_segment(pgd_t *pgdp, void *va_start, void *va_end,
-				      pgprot_t prot, struct vm_struct *vma,
-				      int flags, unsigned long vm_flags)
+static void __init move_pud_table_into_iee(pgd_t *pgdp, unsigned long addr, unsigned long end)
 {
-	phys_addr_t pa_start = __pa_symbol(va_start);
-	unsigned long size = va_end - va_start;
+	unsigned long next;
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+	p4d_t p4d = READ_ONCE(*p4dp);
+	pud_t *pudp;
+	pud_t pud;
+	unsigned long iee_addr = __phys_to_iee(__p4d_to_phys(p4d));
+	set_iee_valid_pre_init(iee_addr);
 
-	BUG_ON(!PAGE_ALIGNED(pa_start));
-	BUG_ON(!PAGE_ALIGNED(size));
+	pudp = pud_offset(p4dp, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		pud = READ_ONCE(*pudp);
+		if ((pud_val(pud) & PUD_TABLE_BIT) == 0)
+		{
+			continue;
+		}
+		else
+		{
+			move_pmd_table_into_iee(pudp, addr, next);
+		}
+	} while (pudp++, addr = next, addr != end);
+}
 
-	__create_pgd_mapping(pgdp, pa_start, (unsigned long)va_start, size, prot,
-			     early_pgtable_alloc, flags);
+static void __init init_iee_for_one_region(pgd_t *pgdir, unsigned long va_start, unsigned long va_end)
+{
+	unsigned long addr, end, next;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, va_start);
 
-	if (!(vm_flags & VM_NO_GUARD))
-		size += PAGE_SIZE;
+	addr = va_start & PAGE_MASK;
+	end = PAGE_ALIGN(va_end);
 
-	vma->addr	= va_start;
-	vma->phys_addr	= pa_start;
-	vma->size	= size;
-	vma->flags	= VM_MAP | vm_flags;
-	vma->caller	= __builtin_return_address(0);
+	do {
+		next = pgd_addr_end(addr, end);
+		move_pud_table_into_iee(pgdp, addr, next);
+	} while (pgdp++, addr = next, addr != end);
+}
 
-	vm_area_add_early(vma);
+static void __init init_iee(void)
+{
+	unsigned long iee_addr;
+	phys_addr_t start, end;
+	u64 i;
+	pgd_t *pgdp;
+
+	#ifdef CONFIG_UNMAP_KERNEL_AT_EL0
+	// handling 1-level tramp page table tramp_pg_dir
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(tramp_pg_dir));
+	set_iee_valid_pre_init(iee_addr);
+	#endif
+	// handling 1-level page table swapper_pg_dir
+	pgdp = swapper_pg_dir;
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(swapper_pg_dir));
+	set_iee_valid_pre_init(iee_addr);
+	// put iee_pg_dir inside iee for sensitive inst isolation
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(iee_pg_dir));
+	set_iee_valid_pre_init(iee_addr);
+	// handling 2/3/4-level page table for kernel
+	init_iee_for_one_region(pgdp, (unsigned long)_text, (unsigned long)_etext);
+	init_iee_for_one_region(pgdp, (unsigned long)__start_rodata, (unsigned long)__inittext_begin);
+	init_iee_for_one_region(pgdp, (unsigned long)__inittext_begin, (unsigned long)__inittext_end);
+	init_iee_for_one_region(pgdp, (unsigned long)__initdata_begin, (unsigned long)__initdata_end);
+	init_iee_for_one_region(pgdp, (unsigned long)_data, (unsigned long)_end);
+	// handling 2/3/4-level page table for fixmap i.e. remap bm_xxx
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(bm_pte));
+	set_iee_valid_pre_init(iee_addr);
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(bm_pmd));
+	set_iee_valid_pre_init(iee_addr);
+	iee_addr = (unsigned long)__phys_to_iee(__pa_symbol(bm_pud));
+	set_iee_valid_pre_init(iee_addr);
+	// handling 2/3/4-level page table for logical mem and iee
+	for_each_mem_range(i, &start, &end) {
+		if (start >= end)
+			break;
+		/*
+		 * The linear map must allow allocation tags reading/writing
+		 * if MTE is present. Otherwise, it has the same attributes as
+		 * PAGE_KERNEL.
+		 */
+		init_iee_for_one_region(pgdp, (unsigned long)__va(start), (unsigned long)__va(end));
+		init_iee_for_one_region(pgdp, (unsigned long)__phys_to_iee(start), (unsigned long)__phys_to_iee(end));
+	}
 }
 
-#ifdef CONFIG_UNMAP_KERNEL_AT_EL0
-static int __init map_entry_trampoline(void)
+static void iee_set_kernel_upage_pre_init(unsigned long addr)
 {
-	int i;
+	pgd_t *pgdir = swapper_pg_dir;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, addr);
 
-	pgprot_t prot = rodata_enabled ? PAGE_KERNEL_ROX : PAGE_KERNEL_EXEC;
-	phys_addr_t pa_start = __pa_symbol(__entry_tramp_text_start);
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+	p4d_t p4d = READ_ONCE(*p4dp);
 
-	/* The trampoline is always mapped and can therefore be global */
-	pgprot_val(prot) &= ~PTE_NG;
+	pud_t *pudp = pud_offset(p4dp, addr);
 
-	/* Map only the text into the trampoline page table */
-	memset(tramp_pg_dir, 0, PGD_SIZE);
-	__create_pgd_mapping(tramp_pg_dir, pa_start, TRAMP_VALIAS,
-			     entry_tramp_text_size(), prot,
-			     __pgd_pgtable_alloc, NO_BLOCK_MAPPINGS);
+	pmd_t *pmdp = pmd_offset(pudp, addr);
 
-	/* Map both the text and data into the kernel page table */
-	for (i = 0; i < DIV_ROUND_UP(entry_tramp_text_size(), PAGE_SIZE); i++)
-		__set_fixmap(FIX_ENTRY_TRAMP_TEXT1 - i,
-			     pa_start + i * PAGE_SIZE, prot);
+	pte_t *ptep = pte_offset_kernel(pmdp, addr);
 
-	if (IS_ENABLED(CONFIG_RANDOMIZE_BASE)) {
-		extern char __entry_tramp_data_start[];
+	int i;
 
-		__set_fixmap(FIX_ENTRY_TRAMP_DATA,
-			     __pa_symbol(__entry_tramp_data_start),
-			     PAGE_KERNEL_RO);
+	__iee_p4d_populate_pre_init(p4dp, __p4d_to_phys(p4d), PGD_APT | PUD_TYPE_TABLE);
+	for(i = 0; i < 4; i++)
+	{
+		pte_t pte = READ_ONCE(*ptep);
+		pte = __pte(pte_val(pte) | PTE_USER | PTE_NG);
+		iee_set_pte_pre_init(ptep, pte);
+		ptep++;
 	}
+	flush_tlb_kernel_range(addr, addr+4*PAGE_SIZE);
+	isb();
+}
 
-	return 0;
+static void __init iee_set_pte_table_ro(pmd_t *pmdp, unsigned long addr, unsigned long end)
+{
+	pmd_t pmd = READ_ONCE(*pmdp);
+	unsigned long logical_addr = (unsigned long)__va(__pmd_to_phys(pmd));
+	iee_set_logical_mem_ro(logical_addr);
 }
-core_initcall(map_entry_trampoline);
-#endif
 
-/*
- * Open coded check for BTI, only for use to determine configuration
- * for early mappings for before the cpufeature code has run.
- */
-static bool arm64_early_this_cpu_has_bti(void)
+static void __init iee_set_pmd_table_ro(pud_t *pudp, unsigned long addr, unsigned long end)
 {
-	u64 pfr1;
+	unsigned long next;
+	pud_t pud = READ_ONCE(*pudp);
+	pmd_t *pmdp;
+	pmd_t pmd;
+	unsigned long logical_addr = (unsigned long)__va(__pud_to_phys(pud));
+	iee_set_logical_mem_ro(logical_addr);
 
-	if (!IS_ENABLED(CONFIG_ARM64_BTI_KERNEL))
-		return false;
+	pmdp = pmd_offset(pudp, addr);
+	do {
+		next = pmd_addr_end(addr, end);
+		pmd = READ_ONCE(*pmdp);
+		if((pmd_val(pmd) & PMD_TABLE_BIT) == 0)
+		{
+			continue;
+		}
+		else
+		{
+			iee_set_pte_table_ro(pmdp, addr, next);
+		}
+	} while (pmdp++, addr = next, addr != end);
+}
 
-	pfr1 = read_sysreg_s(SYS_ID_AA64PFR1_EL1);
-	return cpuid_feature_extract_unsigned_field(pfr1,
-						    ID_AA64PFR1_BT_SHIFT);
+static void __init iee_set_pud_table_ro(pgd_t *pgdp, unsigned long addr, unsigned long end)
+{
+	unsigned long next;
+	p4d_t *p4dp = p4d_offset(pgdp, addr);
+	p4d_t p4d = READ_ONCE(*p4dp);
+	pud_t *pudp;
+	pud_t pud;
+	unsigned long logical_addr = (unsigned long)__va(__p4d_to_phys(p4d));
+	iee_set_logical_mem_ro(logical_addr);
+
+	pudp = pud_offset(p4dp, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		pud = READ_ONCE(*pudp);
+		if ((pud_val(pud) & PUD_TABLE_BIT) == 0)
+		{
+			continue;
+		}
+		else
+		{
+			iee_set_pmd_table_ro(pudp, addr, next);
+		}
+	} while (pudp++, addr = next, addr != end);
 }
 
-/*
- * Create fine-grained mappings for the kernel.
- */
-static void __init map_kernel(pgd_t *pgdp)
+static void __init iee_mark_pgtable_for_one_region_ro(pgd_t *pgdir, unsigned long va_start, unsigned long va_end)
 {
-	static struct vm_struct vmlinux_text, vmlinux_rodata, vmlinux_inittext,
-				vmlinux_initdata, vmlinux_data;
+	unsigned long addr, end, next;
+	pgd_t *pgdp = pgd_offset_pgd(pgdir, va_start);
 
-	/*
-	 * External debuggers may need to write directly to the text
-	 * mapping to install SW breakpoints. Allow this (only) when
-	 * explicitly requested with rodata=off.
-	 */
-	pgprot_t text_prot = rodata_enabled ? PAGE_KERNEL_ROX : PAGE_KERNEL_EXEC;
+	addr = va_start & PAGE_MASK;
+	end = PAGE_ALIGN(va_end);
 
-	/*
-	 * If we have a CPU that supports BTI and a kernel built for
-	 * BTI then mark the kernel executable text as guarded pages
-	 * now so we don't have to rewrite the page tables later.
-	 */
-	if (arm64_early_this_cpu_has_bti())
-		text_prot = __pgprot_modify(text_prot, PTE_GP, PTE_GP);
+	do {
+		next = pgd_addr_end(addr, end);
+		iee_set_pud_table_ro(pgdp, addr, next);
+	} while (pgdp++, addr = next, addr != end);
+}
 
-	/*
-	 * Only rodata will be remapped with different permissions later on,
-	 * all other segments are allowed to use contiguous mappings.
-	 */
-	map_kernel_segment(pgdp, _text, _etext, text_prot, &vmlinux_text, 0,
-			   VM_NO_GUARD);
-	map_kernel_segment(pgdp, __start_rodata, __inittext_begin, PAGE_KERNEL,
-			   &vmlinux_rodata, NO_CONT_MAPPINGS, VM_NO_GUARD);
-	map_kernel_segment(pgdp, __inittext_begin, __inittext_end, text_prot,
-			   &vmlinux_inittext, 0, VM_NO_GUARD);
-	map_kernel_segment(pgdp, __initdata_begin, __initdata_end, PAGE_KERNEL,
-			   &vmlinux_initdata, 0, VM_NO_GUARD);
-	map_kernel_segment(pgdp, _data, _end, PAGE_KERNEL, &vmlinux_data, 0, 0);
+// TODO : Mark pgtable outside as RO.
+static void __init iee_mark_all_lm_pgtable_ro(void)
+{
+	unsigned long logical_addr;
+	phys_addr_t start, end;
+	u64 i;
+	pgd_t *pgdp;
 
-	if (!READ_ONCE(pgd_val(*pgd_offset_pgd(pgdp, FIXADDR_START)))) {
-		/*
-		 * The fixmap falls in a separate pgd to the kernel, and doesn't
-		 * live in the carveout for the swapper_pg_dir. We can simply
-		 * re-use the existing dir for the fixmap.
-		 */
-		set_pgd(pgd_offset_pgd(pgdp, FIXADDR_START),
-			READ_ONCE(*pgd_offset_k(FIXADDR_START)));
-	} else if (CONFIG_PGTABLE_LEVELS > 3) {
-		pgd_t *bm_pgdp;
-		p4d_t *bm_p4dp;
-		pud_t *bm_pudp;
+	// handling static allocated page table
+	#ifdef CONFIG_UNMAP_KERNEL_AT_EL0
+	// handling 1-level tramp page table tramp_pg_dir
+	logical_addr = (unsigned long)__va(__pa_symbol(tramp_pg_dir));
+	iee_set_logical_mem_ro(logical_addr);
+	#endif
+	// handling 1-level page table swapper_pg_dir
+	pgdp = swapper_pg_dir;
+	iee_set_logical_mem_ro((unsigned long)swapper_pg_dir);
+	logical_addr = (unsigned long)__va(__pa_symbol(swapper_pg_dir));
+	iee_set_logical_mem_ro(logical_addr);
+
+	// handling 2/3/4-level page table for kernel
+	iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)_text, (unsigned long)_etext);
+	iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)__start_rodata, (unsigned long)__inittext_begin);
+	iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)__inittext_begin, (unsigned long)__inittext_end);
+	iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)__initdata_begin, (unsigned long)__initdata_end);
+	iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)_data, (unsigned long)_end);
+
+	// handling 2/3/4-level page table for fixmap i.e. remap bm_xxx
+	logical_addr = (unsigned long)__va(__pa_symbol(bm_pte));
+	iee_set_logical_mem_ro(logical_addr);
+
+	iee_set_logical_mem_ro((unsigned long)bm_pmd);
+	logical_addr = (unsigned long)__va(__pa_symbol(bm_pmd));
+	iee_set_logical_mem_ro(logical_addr);
+
+	iee_set_logical_mem_ro((unsigned long)bm_pud);
+	logical_addr = (unsigned long)__va(__pa_symbol(bm_pud));
+	iee_set_logical_mem_ro(logical_addr);
+
+	// handling 2/3/4-level page table for logical mem and iee
+	for_each_mem_range(i, &start, &end) {
+		if (start >= end)
+			break;
 		/*
-		 * The fixmap shares its top level pgd entry with the kernel
-		 * mapping. This can really only occur when we are running
-		 * with 16k/4 levels, so we can simply reuse the pud level
-		 * entry instead.
+		 * The linear map must allow allocation tags reading/writing
+		 * if MTE is present. Otherwise, it has the same attributes as
+		 * PAGE_KERNEL.
 		 */
-		BUG_ON(!IS_ENABLED(CONFIG_ARM64_16K_PAGES));
-		bm_pgdp = pgd_offset_pgd(pgdp, FIXADDR_START);
-		bm_p4dp = p4d_offset(bm_pgdp, FIXADDR_START);
-		bm_pudp = pud_set_fixmap_offset(bm_p4dp, FIXADDR_START);
-		pud_populate(&init_mm, bm_pudp, lm_alias(bm_pmd));
-		pud_clear_fixmap();
-	} else {
-		BUG();
+		iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)__va(start), (unsigned long)__va(end));
+		iee_mark_pgtable_for_one_region_ro(pgdp, (unsigned long)__phys_to_iee(start), (unsigned long)__phys_to_iee(end));
 	}
-
-	kasan_copy_shadow(pgdp);
 }
+#endif
+
+#ifdef CONFIG_KOI
+extern s64 koi_offset;
+#endif
 
 void __init paging_init(void)
 {
-	pgd_t *pgdp = pgd_set_fixmap(__pa_symbol(swapper_pg_dir));
+	pgd_t *pgdp;
+	#ifdef CONFIG_IEE
+	unsigned long SP_EL0;
+	void *new;
+	void *init_token;
+	struct task_token *token;
+	unsigned long tcr;
+	#endif
+
+	// Check if cpu has PAN and HPDS.
+	#ifdef CONFIG_IEE
+	if(!cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64MMFR1_EL1),
+						ID_AA64MMFR1_PAN_SHIFT))
+		panic("Architecture doesn't support PAN, please disable CONFIG_IEE.\n");
+	
+	if(!cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64MMFR1_EL1),
+						ID_AA64MMFR1_HPD_SHIFT))
+		panic("Architecture doesn't support HPDS, please disable CONFIG_IEE.\n");
+	#endif
+
+	// Avoid using iee code to modify pgtable before iee initialized.
+	#ifdef CONFIG_PTP
+	pgdp = pgd_set_fixmap_init(__pa_symbol(swapper_pg_dir));
+	#else
+	pgdp = pgd_set_fixmap(__pa_symbol(swapper_pg_dir));
+	#endif
 
 	map_kernel(pgdp);
 	map_mem(pgdp);
 
+	// Map the whole physical mem into IEE, but set invalid.
+	#ifdef CONFIG_IEE
+	map_iee(pgdp);
+    // map_iee_si(pgdp);
+	#endif
+
+	// Avoid using iee code to modify pgtable before iee initialized.
+	#ifdef CONFIG_PTP
+	pgd_clear_fixmap_init();
+	#else
 	pgd_clear_fixmap();
+	#endif
+
+	#ifdef CONFIG_IEE
+	// Initialize iee_pg_dir.
+    memcpy(iee_pg_dir, swapper_pg_dir, PAGE_SIZE);
+	flush_tlb_all();
+	#endif
 
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 	init_mm.pgd = swapper_pg_dir;
@@ -774,6 +2336,78 @@ void __init paging_init(void)
 		      __pa_symbol(init_pg_end) - __pa_symbol(init_pg_dir));
 
 	memblock_allow_resize();
+
+	#ifdef CONFIG_IEE
+    // test iee_exec_entry
+    iee_rwx_gate_entry(IEE_SI_TEST);
+	// Initialize init iee stack.
+	#ifdef CONFIG_PTP
+	iee_set_kernel_upage_pre_init((unsigned long)init_iee_stack_begin);
+	iee_set_kernel_upage_pre_init((unsigned long)__va(__pa_symbol(init_iee_stack_begin)));
+	#else
+	iee_set_kernel_upage((unsigned long)init_iee_stack_begin);
+	iee_set_kernel_upage((unsigned long)__va(__pa_symbol(init_iee_stack_begin)));
+	#endif
+	#endif
+
+	// Init token for init_task.
+	#ifdef CONFIG_IEE
+	// Change SP_EL0 from Image VA to Logical VA.
+	SP_EL0 = (unsigned long)__va(__pa_symbol(&init_task));
+	write_sysreg(SP_EL0, sp_el0);
+	init_task.cpus_ptr = &(((struct task_struct *)(__va(__pa_symbol(&init_task))))->cpus_mask);
+	init_task.children.prev = (__va(__pa_symbol(init_task.children.prev)));
+	init_task.children.next = (__va(__pa_symbol(init_task.children.next)));
+	// Set init_task into __entry_task before per_cpu init.
+	*(struct task_struct **)__entry_task = __va(__pa_symbol(&init_task));
+	// Alloc a page for init_token.
+	new = __va(early_pgtable_alloc(0));
+	init_token = (void *)__phys_to_iee(__pa_symbol(&init_task));
+	#ifdef CONFIG_PTP
+	iee_set_token_page_valid_pre_init(init_token, new);
+	#else
+	iee_set_token_page_valid(init_token, new);
+	#endif
+	// Use lm to write token before IEE initialized.
+	token = (struct task_token *)((unsigned long)new + (((unsigned long)&init_task) & ~PAGE_MASK));
+	token->mm = &init_mm;
+	token->pgd = NULL;
+	token->iee_stack = (void *)init_iee_stack_end;
+	token->valid = true;
+	#endif
+
+	#ifdef CONFIG_PTP
+	// Map the existing pgtable into IEE, set valid.
+	init_iee();
+	#endif
+
+	#ifdef CONFIG_IEE
+	sysreg_clear_set(sctlr_el1, 0, SCTLR_EL1_SPAN);
+	#endif
+
+	#ifdef CONFIG_PTP
+	// IEE ready.
+	// Page Table writing before uses logical memory and after uses IEE memory.
+	iee_mark_all_lm_pgtable_ro();
+	#endif
+
+	// Set the init token readonly.
+	#ifdef CONFIG_IEE
+	set_iee_page_valid(__phys_to_iee(__pa(new)));
+	iee_set_logical_mem_ro((unsigned long)new);
+	
+	// Set HPD1 as 1.
+	tcr = read_sysreg(tcr_el1);
+	tcr |= ((unsigned long)0x1 << 42);
+	write_sysreg(tcr, tcr_el1);
+	isb();
+
+	// Flush tlb to enable IEE.
+	flush_tlb_all();
+
+	// mark that iee is prepared.
+	iee_init_done = true;
+	#endif
 }
 
 /*
@@ -1265,13 +2899,25 @@ void __init early_fixmap_init(void)
 		pudp = pud_offset_kimg(p4dp, addr);
 	} else {
 		if (p4d_none(p4d))
+			#ifdef CONFIG_PTP
+			__iee_p4d_populate_pre_init(p4dp, __pa_symbol(bm_pud), PUD_TYPE_TABLE);
+			#else
 			__p4d_populate(p4dp, __pa_symbol(bm_pud), PUD_TYPE_TABLE);
+			#endif
 		pudp = fixmap_pud(addr);
 	}
 	if (pud_none(READ_ONCE(*pudp)))
+		#ifdef CONFIG_PTP
+		__iee_pud_populate_pre_init(pudp, __pa_symbol(bm_pmd), PMD_TYPE_TABLE);
+		#else
 		__pud_populate(pudp, __pa_symbol(bm_pmd), PMD_TYPE_TABLE);
+		#endif
 	pmdp = fixmap_pmd(addr);
+	#ifdef CONFIG_PTP
+	__iee_pmd_populate_pre_init(pmdp, __pa_symbol(bm_pte), PMD_TYPE_TABLE);
+	#else
 	__pmd_populate(pmdp, __pa_symbol(bm_pte), PMD_TYPE_TABLE);
+	#endif
 
 	/*
 	 * The boot-ioremap range spans multiple pmds, for which
@@ -1296,6 +2942,26 @@ void __init early_fixmap_init(void)
 	}
 }
 
+#ifdef CONFIG_PTP
+void __iee_set_fixmap_pre_init(enum fixed_addresses idx,
+			       phys_addr_t phys, pgprot_t flags)
+{
+	unsigned long addr = __fix_to_virt(idx);
+	pte_t *ptep;
+
+	BUG_ON(idx <= FIX_HOLE || idx >= __end_of_fixed_addresses);
+
+	ptep = fixmap_pte(addr);
+
+	if (pgprot_val(flags)) {
+		iee_set_pte_pre_init(ptep, pfn_pte(phys >> PAGE_SHIFT, flags));
+	} else {
+		iee_set_pte_pre_init(ptep, __pte(0));
+		flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
+	}
+}
+#endif
+
 /*
  * Unusually, this is also called in IRQ context (ghes_iounmap_irq) so if we
  * ever need to use IPIs for TLB broadcasting, then we're in trouble here.
@@ -1311,10 +2977,17 @@ void __set_fixmap(enum fixed_addresses idx,
 	ptep = fixmap_pte(addr);
 
 	if (pgprot_val(flags)) {
-		flags = pgprot_pbha_bit0(flags);
+		#ifdef CONFIG_PTP
+		iee_set_bm_pte(ptep, pfn_pte(phys >> PAGE_SHIFT, flags));
+		#else
 		set_pte(ptep, pfn_pte(phys >> PAGE_SHIFT, flags));
+		#endif
 	} else {
+		#ifdef CONFIG_PTP
+		iee_set_bm_pte(ptep, __pte(0));
+		#else
 		pte_clear(&init_mm, addr, ptep);
+		#endif
 		flush_tlb_kernel_range(addr, addr+PAGE_SIZE);
 	}
 }
diff --git a/arch/arm64/mm/pgd.c b/arch/arm64/mm/pgd.c
index 4a64089e5771..894bda11c389 100644
--- a/arch/arm64/mm/pgd.c
+++ b/arch/arm64/mm/pgd.c
@@ -15,14 +15,44 @@
 #include <asm/page.h>
 #include <asm/tlbflush.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 static struct kmem_cache *pgd_cache __ro_after_init;
 
+#ifdef CONFIG_KOI
+pgd_t *koi_pgd_alloc(void)
+{
+    pgd_t *pgd;
+#ifdef CONFIG_PTP
+    pgd = (pgd_t *)__get_free_page(GFP_PGTABLE_KERNEL);
+    unsigned long iee_addr = __phys_to_iee(__pa(pgd));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)pgd);
+#else
+	pgd = (pgd_t *)__get_free_page(GFP_PGTABLE_KERNEL);
+#endif
+	return pgd;
+}
+#endif
+
 pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	gfp_t gfp = GFP_PGTABLE_USER;
 
 	if (PGD_SIZE == PAGE_SIZE)
+#ifdef CONFIG_PTP
+    {
+        pgd_t* new = (pgd_t *)__get_free_page(gfp);
+        unsigned long iee_addr = __phys_to_iee(__pa(new));
+	    set_iee_page_valid(iee_addr);
+	    iee_set_logical_mem_ro((unsigned long)new);
+        return new;
+    }
+#else
 		return (pgd_t *)__get_free_page(gfp);
+#endif        
 	else
 		return kmem_cache_alloc(pgd_cache, gfp);
 }
@@ -30,7 +60,16 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 void pgd_free(struct mm_struct *mm, pgd_t *pgd)
 {
 	if (PGD_SIZE == PAGE_SIZE)
+#ifdef CONFIG_PTP
+    {
+        unsigned long iee_addr = __phys_to_iee(__pa(pgd));
+	    set_iee_page_invalid(iee_addr);
+	    iee_set_logical_mem_rw((unsigned long)pgd);
+        free_page((unsigned long)pgd);
+    }
+#else
 		free_page((unsigned long)pgd);
+#endif
 	else
 		kmem_cache_free(pgd_cache, pgd);
 }
diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
index a3fdc71c01eb..bbacdf8a3e20 100644
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@ -20,6 +20,8 @@
 #include <asm/smp.h>
 #include <asm/sysreg.h>
 
+
+
 #ifdef CONFIG_ARM64_64K_PAGES
 #define TCR_TG_FLAGS	TCR_TG0_64K | TCR_TG1_64K
 #elif defined(CONFIG_ARM64_16K_PAGES)
@@ -103,6 +105,18 @@ SYM_FUNC_END(cpu_do_suspend)
  * x0: Address of context pointer
  */
 	.pushsection ".idmap.text", "awx"
+#ifdef CONFIG_IEE
+// SP_EL0 check failed.
+SYM_FUNC_START_LOCAL(sp_el0_check_failed)
+1:
+	nop
+	nop
+	nop
+	nop
+	b 1f
+SYM_FUNC_END(sp_el0_check_failed)
+#endif
+
 SYM_FUNC_START(cpu_do_resume)
 	ldp	x2, x3, [x0]
 	ldp	x4, x5, [x0, #16]
@@ -145,6 +159,13 @@ alternative_else
 	msr	tpidr_el2, x13
 alternative_endif
 	msr	sp_el0, x14
+#ifdef CONFIG_IEE
+	// tsk check.
+	ldr_this_cpu x2, __entry_task, x3
+	mrs x3, sp_el0
+	cmp x2, x3
+	b.ne sp_el0_check_failed
+#endif
 	/*
 	 * Restore oslsr_el1 by writing oslar_el1
 	 */
@@ -190,6 +211,7 @@ SYM_FUNC_START(idmap_cpu_replace_ttbr1)
 	__idmap_cpu_set_reserved_ttbr1 x1, x3
 
 	offset_ttbr1 x0, x3
+
 	msr	ttbr1_el1, x0
 	isb
 
diff --git a/drivers/firmware/efi/arm-runtime.c b/drivers/firmware/efi/arm-runtime.c
index 3359ae2adf24..3b2734254ac3 100644
--- a/drivers/firmware/efi/arm-runtime.c
+++ b/drivers/firmware/efi/arm-runtime.c
@@ -94,7 +94,11 @@ static int __init arm_enable_runtime_services(void)
 		return 0;
 	}
 
+	#ifdef CONFIG_PTP
+	efi_memmap_unmap_after_init();
+	#else
 	efi_memmap_unmap();
+	#endif
 
 	mapsize = efi.memmap.desc_size * efi.memmap.nr_map;
 
diff --git a/drivers/firmware/efi/memmap.c b/drivers/firmware/efi/memmap.c
index 0155bf066ba5..74fba95fbdf2 100644
--- a/drivers/firmware/efi/memmap.c
+++ b/drivers/firmware/efi/memmap.c
@@ -174,6 +174,26 @@ void __init efi_memmap_unmap(void)
 	clear_bit(EFI_MEMMAP, &efi.flags);
 }
 
+#ifdef CONFIG_PTP
+void __init efi_memmap_unmap_after_init(void)
+{
+	if (!efi_enabled(EFI_MEMMAP))
+		return;
+
+	if (!(efi.memmap.flags & EFI_MEMMAP_LATE)) {
+		unsigned long size;
+
+		size = efi.memmap.desc_size * efi.memmap.nr_map;
+		early_iounmap_after_init((__force void __iomem *)efi.memmap.map, size);
+	} else {
+		memunmap(efi.memmap.map);
+	}
+
+	efi.memmap.map = NULL;
+	clear_bit(EFI_MEMMAP, &efi.flags);
+}
+#endif
+
 /**
  * efi_memmap_init_late - Map efi.memmap with memremap()
  * @phys_addr: Physical address of the new EFI memory map
diff --git a/drivers/tty/serial/earlycon.c b/drivers/tty/serial/earlycon.c
index b70877932d47..fe8f3d8351ff 100644
--- a/drivers/tty/serial/earlycon.c
+++ b/drivers/tty/serial/earlycon.c
@@ -40,7 +40,11 @@ static void __iomem * __init earlycon_map(resource_size_t paddr, size_t size)
 {
 	void __iomem *base;
 #ifdef CONFIG_FIX_EARLYCON_MEM
+	#ifdef CONFIG_PTP
+	__iee_set_fixmap_pre_init(FIX_EARLYCON_MEM_BASE, paddr & PAGE_MASK, FIXMAP_PAGE_IO);
+	#else
 	set_fixmap_io(FIX_EARLYCON_MEM_BASE, paddr & PAGE_MASK);
+	#endif
 	base = (void __iomem *)__fix_to_virt(FIX_EARLYCON_MEM_BASE);
 	base += paddr & ~PAGE_MASK;
 #else
diff --git a/drivers/usb/early/ehci-dbgp.c b/drivers/usb/early/ehci-dbgp.c
index 45b42d8f6453..b71072d6957e 100644
--- a/drivers/usb/early/ehci-dbgp.c
+++ b/drivers/usb/early/ehci-dbgp.c
@@ -879,7 +879,11 @@ int __init early_dbgp_init(char *s)
 	 * FIXME I don't have the bar size so just guess PAGE_SIZE is more
 	 * than enough.  1K is the biggest I have seen.
 	 */
+	#ifdef CONFIG_PTP
+	__iee_set_fixmap_pre_init(FIX_DBGP_BASE, bar_val & PAGE_MASK, FIXMAP_PAGE_NOCACHE);
+	#else
 	set_fixmap_nocache(FIX_DBGP_BASE, bar_val & PAGE_MASK);
+	#endif
 	ehci_bar = (void __iomem *)__fix_to_virt(FIX_DBGP_BASE);
 	ehci_bar += bar_val & ~PAGE_MASK;
 	dbgp_printk("ehci_bar: %p\n", ehci_bar);
diff --git a/fs/cifs/cifs_spnego.c b/fs/cifs/cifs_spnego.c
index 4f9d08ac9dde..280c2898be88 100644
--- a/fs/cifs/cifs_spnego.c
+++ b/fs/cifs/cifs_spnego.c
@@ -30,6 +30,10 @@
 #include "cifs_spnego.h"
 #include "cifs_debug.h"
 #include "cifsproto.h"
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 static const struct cred *spnego_cred;
 
 /* create a new cifs key */
@@ -224,8 +228,13 @@ init_cifs_spnego(void)
 	 * the results it looks up
 	 */
 	set_bit(KEY_FLAG_ROOT_CAN_CLEAR, &keyring->flags);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(cred,keyring);
+	iee_set_cred_jit_keyring(cred,KEY_REQKEY_DEFL_THREAD_KEYRING);
+	#else
 	cred->thread_keyring = keyring;
 	cred->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;
+	#endif
 	spnego_cred = cred;
 
 	cifs_dbg(FYI, "cifs spnego keyring: %d\n", key_serial(keyring));
diff --git a/fs/cifs/cifsacl.c b/fs/cifs/cifsacl.c
index ef4784e72b1d..ce68418d0974 100644
--- a/fs/cifs/cifsacl.c
+++ b/fs/cifs/cifsacl.c
@@ -33,6 +33,10 @@
 #include "cifsproto.h"
 #include "cifs_debug.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 /* security id for everyone/world system group */
 static const struct cifs_sid sid_everyone = {
 	1, 1, {0, 0, 0, 0, 0, 1}, {0} };
@@ -496,8 +500,13 @@ init_cifs_idmap(void)
 	/* instruct request_key() to use this special keyring as a cache for
 	 * the results it looks up */
 	set_bit(KEY_FLAG_ROOT_CAN_CLEAR, &keyring->flags);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(cred,keyring);
+	iee_set_cred_jit_keyring(cred,KEY_REQKEY_DEFL_THREAD_KEYRING);
+	#else
 	cred->thread_keyring = keyring;
 	cred->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;
+	#endif
 	root_cred = cred;
 
 	cifs_dbg(FYI, "cifs idmap keyring: %d\n", key_serial(keyring));
diff --git a/fs/coredump.c b/fs/coredump.c
index 535c3fdc1598..ba0dfac2aa86 100644
--- a/fs/coredump.c
+++ b/fs/coredump.c
@@ -53,6 +53,10 @@
 
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 static bool dump_vma_snapshot(struct coredump_params *cprm);
 static void free_vma_snapshot(struct coredump_params *cprm);
 
@@ -627,7 +631,11 @@ void do_coredump(const kernel_siginfo_t *siginfo)
 	 */
 	if (__get_dumpable(cprm.mm_flags) == SUID_DUMP_ROOT) {
 		/* Setuid core dump mode */
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsuid(cred,GLOBAL_ROOT_UID);
+		#else
 		cred->fsuid = GLOBAL_ROOT_UID;	/* Dump root private */
+		#endif
 		need_suid_safe = true;
 	}
 
diff --git a/fs/exec.c b/fs/exec.c
index 981b3ac90c44..1eda61cced1a 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -74,6 +74,14 @@
 
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
+#ifdef CONFIG_IEE
+#include <asm/iee-token.h>
+#endif
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -1003,6 +1011,10 @@ static int exec_mmap(struct mm_struct *mm)
 	if (!IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
 		local_irq_enable();
 	activate_mm(active_mm, mm);
+	#ifdef CONFIG_IEE
+	iee_set_token_mm(tsk, mm);
+	iee_set_token_pgd(tsk, mm->pgd);
+	#endif
 	if (IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
 		local_irq_enable();
 	tsk->mm->vmacache_seqnum = 0;
@@ -1595,12 +1607,20 @@ static void bprm_fill_uid(struct linux_binprm *bprm, struct file *file)
 
 	if (mode & S_ISUID) {
 		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_euid(bprm->cred,uid);
+		#else
 		bprm->cred->euid = uid;
+		#endif
 	}
 
 	if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
 		bprm->per_clear |= PER_CLEAR_ON_SETID;
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_egid(bprm->cred,gid);
+		#else
 		bprm->cred->egid = gid;
+		#endif
 	}
 }
 
diff --git a/fs/nfs/flexfilelayout/flexfilelayout.c b/fs/nfs/flexfilelayout/flexfilelayout.c
index e4f2820ba5a5..5c1645f6eb35 100644
--- a/fs/nfs/flexfilelayout/flexfilelayout.c
+++ b/fs/nfs/flexfilelayout/flexfilelayout.c
@@ -15,6 +15,10 @@
 
 #include <linux/sunrpc/metrics.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #include "flexfilelayout.h"
 #include "../nfs4session.h"
 #include "../nfs4idmap.h"
@@ -496,8 +500,13 @@ ff_layout_alloc_lseg(struct pnfs_layout_hdr *lh,
 		rc = -ENOMEM;
 		if (!kcred)
 			goto out_err_free;
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsuid(kcred,uid);
+		iee_set_cred_fsgid(kcred,gid);
+		#else
 		kcred->fsuid = uid;
 		kcred->fsgid = gid;
+		#endif
 		cred = RCU_INITIALIZER(kcred);
 
 		if (lgr->range.iomode == IOMODE_READ)
diff --git a/fs/nfs/nfs4idmap.c b/fs/nfs/nfs4idmap.c
index ec6afd3c4bca..af993bdedca5 100644
--- a/fs/nfs/nfs4idmap.c
+++ b/fs/nfs/nfs4idmap.c
@@ -48,6 +48,10 @@
 #include <linux/module.h>
 #include <linux/user_namespace.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #include "internal.h"
 #include "netns.h"
 #include "nfs4idmap.h"
@@ -226,8 +230,13 @@ int nfs_idmap_init(void)
 		goto failed_reg_legacy;
 
 	set_bit(KEY_FLAG_ROOT_CAN_CLEAR, &keyring->flags);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(cred,keyring);
+	iee_set_cred_jit_keyring(cred,KEY_REQKEY_DEFL_THREAD_KEYRING);
+	#else
 	cred->thread_keyring = keyring;
 	cred->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;
+	#endif
 	id_resolver_cache = cred;
 	return 0;
 
diff --git a/fs/nfsd/auth.c b/fs/nfsd/auth.c
index fdf2aad73470..bedbe237a13f 100644
--- a/fs/nfsd/auth.c
+++ b/fs/nfsd/auth.c
@@ -2,6 +2,9 @@
 /* Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de> */
 
 #include <linux/sched.h>
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
 #include "nfsd.h"
 #include "auth.h"
 
@@ -34,22 +37,40 @@ int nfsd_setuser(struct svc_rqst *rqstp, struct svc_export *exp)
 	if (!new)
 		return -ENOMEM;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,rqstp->rq_cred.cr_uid);
+	iee_set_cred_fsgid(new,rqstp->rq_cred.cr_gid);
+	#else
 	new->fsuid = rqstp->rq_cred.cr_uid;
 	new->fsgid = rqstp->rq_cred.cr_gid;
+	#endif
 
 	rqgi = rqstp->rq_cred.cr_group_info;
 
 	if (flags & NFSEXP_ALLSQUASH) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsuid(new,exp->ex_anon_uid);
+		iee_set_cred_fsgid(new,exp->ex_anon_gid);
+		#else
 		new->fsuid = exp->ex_anon_uid;
 		new->fsgid = exp->ex_anon_gid;
+		#endif
 		gi = groups_alloc(0);
 		if (!gi)
 			goto oom;
 	} else if (flags & NFSEXP_ROOTSQUASH) {
 		if (uid_eq(new->fsuid, GLOBAL_ROOT_UID))
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_fsuid(new,exp->ex_anon_uid);
+			#else
 			new->fsuid = exp->ex_anon_uid;
+			#endif
 		if (gid_eq(new->fsgid, GLOBAL_ROOT_GID))
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_fsgid(new,exp->ex_anon_gid);
+			#else
 			new->fsgid = exp->ex_anon_gid;
+			#endif
 
 		gi = groups_alloc(rqgi->ngroups);
 		if (!gi)
@@ -69,18 +90,35 @@ int nfsd_setuser(struct svc_rqst *rqstp, struct svc_export *exp)
 	}
 
 	if (uid_eq(new->fsuid, INVALID_UID))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsuid(new,exp->ex_anon_uid);
+		#else
 		new->fsuid = exp->ex_anon_uid;
+		#endif
 	if (gid_eq(new->fsgid, INVALID_GID))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsgid(new,exp->ex_anon_gid);
+		#else
 		new->fsgid = exp->ex_anon_gid;
+		#endif
 
 	set_groups(new, gi);
 	put_group_info(gi);
 
 	if (!uid_eq(new->fsuid, GLOBAL_ROOT_UID))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,cap_drop_nfsd_set(new->cap_effective));
+		#else
 		new->cap_effective = cap_drop_nfsd_set(new->cap_effective);
+		#endif
 	else
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,cap_raise_nfsd_set(new->cap_effective,
+							new->cap_permitted));
+		#else
 		new->cap_effective = cap_raise_nfsd_set(new->cap_effective,
 							new->cap_permitted);
+		#endif
 	validate_process_creds();
 	put_cred(override_creds(new));
 	put_cred(new);
diff --git a/fs/nfsd/nfs4callback.c b/fs/nfsd/nfs4callback.c
index f5b7ad0847f2..8ef1f504d3a6 100644
--- a/fs/nfsd/nfs4callback.c
+++ b/fs/nfsd/nfs4callback.c
@@ -41,6 +41,9 @@
 #include "trace.h"
 #include "xdr4cb.h"
 #include "xdr4.h"
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
 
 #define NFSDDBG_FACILITY                NFSDDBG_PROC
 
@@ -875,8 +878,13 @@ static const struct cred *get_backchannel_cred(struct nfs4_client *clp, struct r
 		if (!kcred)
 			return NULL;
 
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_uid(kcred,ses->se_cb_sec.uid);
+		iee_set_cred_gid(kcred,ses->se_cb_sec.gid);
+		#else
 		kcred->fsuid = ses->se_cb_sec.uid;
 		kcred->fsgid = ses->se_cb_sec.gid;
+		#endif
 		return kcred;
 	}
 }
diff --git a/fs/nfsd/nfs4recover.c b/fs/nfsd/nfs4recover.c
index 83c4e6883953..6e201013c73e 100644
--- a/fs/nfsd/nfs4recover.c
+++ b/fs/nfsd/nfs4recover.c
@@ -44,6 +44,10 @@
 #include <linux/sunrpc/clnt.h>
 #include <linux/nfsd/cld.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #include "nfsd.h"
 #include "state.h"
 #include "vfs.h"
@@ -78,8 +82,13 @@ nfs4_save_creds(const struct cred **original_creds)
 	if (!new)
 		return -ENOMEM;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,GLOBAL_ROOT_UID);
+	iee_set_cred_fsgid(new,GLOBAL_ROOT_GID);
+	#else
 	new->fsuid = GLOBAL_ROOT_UID;
 	new->fsgid = GLOBAL_ROOT_GID;
+	#endif
 	*original_creds = override_creds(new);
 	put_cred(new);
 	return 0;
diff --git a/fs/nfsd/nfsfh.c b/fs/nfsd/nfsfh.c
index c81dbbad8792..945367a74386 100644
--- a/fs/nfsd/nfsfh.c
+++ b/fs/nfsd/nfsfh.c
@@ -16,6 +16,10 @@
 #include "auth.h"
 #include "trace.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #define NFSDDBG_FACILITY		NFSDDBG_FH
 
 
@@ -234,9 +238,14 @@ static __be32 nfsd_set_fh_dentry(struct svc_rqst *rqstp, struct svc_fh *fhp)
 			error =  nfserrno(-ENOMEM);
 			goto out;
 		}
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,cap_raise_nfsd_set(new->cap_effective,
+					   new->cap_permitted));
+		#else
 		new->cap_effective =
 			cap_raise_nfsd_set(new->cap_effective,
 					   new->cap_permitted);
+		#endif
 		put_cred(override_creds(new));
 		put_cred(new);
 	} else {
diff --git a/fs/open.c b/fs/open.c
index 8092178ceab0..510398b1568b 100644
--- a/fs/open.c
+++ b/fs/open.c
@@ -33,6 +33,10 @@
 #include <linux/dnotify.h>
 #include <linux/compat.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #include "internal.h"
 
 int do_truncate(struct dentry *dentry, loff_t length, unsigned int time_attrs,
@@ -354,17 +358,32 @@ static const struct cred *access_override_creds(void)
 	if (!override_cred)
 		return NULL;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(override_cred,override_cred->uid);
+	iee_set_cred_fsgid(override_cred,override_cred->gid);
+	#else
 	override_cred->fsuid = override_cred->uid;
 	override_cred->fsgid = override_cred->gid;
+	#endif
 
 	if (!issecure(SECURE_NO_SETUID_FIXUP)) {
 		/* Clear the capabilities if we switch to a non-root user */
 		kuid_t root_uid = make_kuid(override_cred->user_ns, 0);
 		if (!uid_eq(override_cred->uid, root_uid))
+			#ifdef CONFIG_CREDP
+			do {
+				iee_set_cred_cap_effective(override_cred, __cap_empty_set);
+			} while (0);
+			#else
 			cap_clear(override_cred->cap_effective);
+			#endif
 		else
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_cap_effective(override_cred,override_cred->cap_permitted);
+			#else
 			override_cred->cap_effective =
 				override_cred->cap_permitted;
+			#endif
 	}
 
 	/*
@@ -384,7 +403,11 @@ static const struct cred *access_override_creds(void)
 	 * expecting RCU freeing. But normal thread-synchronous
 	 * cred accesses will keep things non-RCY.
 	 */
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_non_rcu(override_cred,1);
+	#else
 	override_cred->non_rcu = 1;
+	#endif
 
 	old_cred = override_creds(override_cred);
 
diff --git a/fs/overlayfs/dir.c b/fs/overlayfs/dir.c
index b2eb48b495f7..42f60bf8ec79 100644
--- a/fs/overlayfs/dir.c
+++ b/fs/overlayfs/dir.c
@@ -16,6 +16,10 @@
 #include <linux/ratelimit.h>
 #include "overlayfs.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 static unsigned short ovl_redirect_max = 256;
 module_param_named(redirect_max, ovl_redirect_max, ushort, 0644);
 MODULE_PARM_DESC(redirect_max,
@@ -603,8 +607,13 @@ static int ovl_create_or_link(struct dentry *dentry, struct inode *inode,
 		 * create a new inode, so just use the ovl mounter's
 		 * fs{u,g}id.
 		 */
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_fsuid(override_cred,inode->i_uid);
+		iee_set_cred_fsgid(override_cred,inode->i_gid);
+		#else
 		override_cred->fsuid = inode->i_uid;
 		override_cred->fsgid = inode->i_gid;
+		#endif
 		err = security_dentry_create_files_as(dentry,
 				attr->mode, &dentry->d_name, old_cred,
 				override_cred);
diff --git a/fs/overlayfs/super.c b/fs/overlayfs/super.c
index 5d7df839902d..93bbe723c422 100644
--- a/fs/overlayfs/super.c
+++ b/fs/overlayfs/super.c
@@ -17,6 +17,10 @@
 #include <linux/exportfs.h>
 #include "overlayfs.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 MODULE_AUTHOR("Miklos Szeredi <miklos@szeredi.hu>");
 MODULE_DESCRIPTION("Overlay filesystem");
 MODULE_LICENSE("GPL");
@@ -2022,7 +2026,15 @@ static int ovl_fill_super(struct super_block *sb, void *data, int silent)
 		sb->s_export_op = &ovl_export_operations;
 
 	/* Never override disk quota limits or use reserved space */
+	#ifdef CONFIG_CREDP
+	{
+		kernel_cap_t tmp = cred->cap_effective;
+		cap_lower(tmp, CAP_SYS_RESOURCE);
+		iee_set_cred_cap_effective(cred, tmp);
+	}
+	#else
 	cap_lower(cred->cap_effective, CAP_SYS_RESOURCE);
+	#endif
 
 	sb->s_magic = OVERLAYFS_SUPER_MAGIC;
 	sb->s_xattr = ovl_xattr_handlers;
diff --git a/include/asm-generic/early_ioremap.h b/include/asm-generic/early_ioremap.h
index 9def22e6e2b3..13b287b7cdc3 100644
--- a/include/asm-generic/early_ioremap.h
+++ b/include/asm-generic/early_ioremap.h
@@ -17,6 +17,9 @@ extern void *early_memremap_ro(resource_size_t phys_addr,
 extern void *early_memremap_prot(resource_size_t phys_addr,
 				 unsigned long size, unsigned long prot_val);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
+#ifdef CONFIG_PTP
+extern void early_iounmap_after_init(void __iomem *addr, unsigned long size);
+#endif
 extern void early_memunmap(void *addr, unsigned long size);
 
 /*
diff --git a/include/asm-generic/fixmap.h b/include/asm-generic/fixmap.h
index 8cc7b09c1bc7..83158589a545 100644
--- a/include/asm-generic/fixmap.h
+++ b/include/asm-generic/fixmap.h
@@ -70,6 +70,24 @@ static inline unsigned long virt_to_fix(const unsigned long vaddr)
 	__set_fixmap(idx, 0, FIXMAP_PAGE_CLEAR)
 #endif
 
+#ifdef CONFIG_PTP
+#ifndef clear_fixmap_init
+#define clear_fixmap_init(idx)			\
+	__iee_set_fixmap_pre_init(idx, 0, FIXMAP_PAGE_CLEAR)
+#endif
+
+#define __iee_set_fixmap_offset_pre_init(idx, phys, flags)				\
+({									\
+	unsigned long ________addr;					\
+	__iee_set_fixmap_pre_init(idx, phys, flags);					\
+	________addr = fix_to_virt(idx) + ((phys) & (PAGE_SIZE - 1));	\
+	________addr;							\
+})
+
+#define iee_set_fixmap_offset_pre_init(idx, phys) \
+	__iee_set_fixmap_offset_pre_init(idx, phys, FIXMAP_PAGE_NORMAL)
+#endif
+
 /* Return a pointer with offset calculated */
 #define __set_fixmap_offset(idx, phys, flags)				\
 ({									\
diff --git a/include/asm-generic/pgalloc.h b/include/asm-generic/pgalloc.h
index 02932efad3ab..a0bfdd8f6e4a 100644
--- a/include/asm-generic/pgalloc.h
+++ b/include/asm-generic/pgalloc.h
@@ -7,6 +7,10 @@
 #define GFP_PGTABLE_KERNEL	(GFP_KERNEL | __GFP_ZERO)
 #define GFP_PGTABLE_USER	(GFP_PGTABLE_KERNEL | __GFP_ACCOUNT)
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 /**
  * __pte_alloc_one_kernel - allocate a page for PTE-level kernel page table
  * @mm: the mm_struct of the current context
@@ -41,6 +45,11 @@ static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm)
  */
 static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
 {
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr = __phys_to_iee(__pa(pte));
+	set_iee_page_invalid(iee_addr);
+	iee_set_logical_mem_rw((unsigned long)pte);
+	#endif
 	free_page((unsigned long)pte);
 }
 
@@ -98,7 +107,16 @@ static inline pgtable_t pte_alloc_one(struct mm_struct *mm)
  */
 static inline void pte_free(struct mm_struct *mm, struct page *pte_page)
 {
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
+
 	pgtable_pte_page_dtor(pte_page);
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(page_address(pte_page)));
+	set_iee_page_invalid(iee_addr);
+	iee_set_logical_mem_rw((unsigned long)page_address(pte_page));
+	#endif
 	__free_page(pte_page);
 }
 
@@ -137,8 +155,17 @@ static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)
 #ifndef __HAVE_ARCH_PMD_FREE
 static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)
 {
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
+
 	BUG_ON((unsigned long)pmd & (PAGE_SIZE-1));
 	pgtable_pmd_page_dtor(virt_to_page(pmd));
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(pmd));
+	set_iee_page_invalid(iee_addr);
+	iee_set_logical_mem_rw((unsigned long)pmd);
+	#endif
 	free_page((unsigned long)pmd);
 }
 #endif
@@ -169,7 +196,16 @@ static inline pud_t *pud_alloc_one(struct mm_struct *mm, unsigned long addr)
 
 static inline void pud_free(struct mm_struct *mm, pud_t *pud)
 {
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
+
 	BUG_ON((unsigned long)pud & (PAGE_SIZE-1));
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(pud));
+	set_iee_page_invalid(iee_addr);
+	iee_set_logical_mem_rw((unsigned long)pud);
+	#endif
 	free_page((unsigned long)pud);
 }
 
diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8716609fb333..771245a02602 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -324,6 +324,17 @@
 	KEEP(*(.dtb.init.rodata))					\
 	__dtb_end = .;
 
+#ifdef CONFIG_KOI
+#define KOI_DATA() \
+	. = ALIGN(PAGE_SIZE);	\
+	__koi_data_start = .;	\
+	*(.data..koi)	\
+	. = ALIGN(PAGE_SIZE);	\
+	__koi_data_end = .;
+#else
+#define KOI_DATA()		
+#endif
+
 /*
  * .data section
  */
@@ -350,8 +361,8 @@
 	BRANCH_PROFILE()						\
 	TRACE_PRINTKS()							\
 	BPF_RAW_TP()							\
-	TRACEPOINT_STR()
-
+	TRACEPOINT_STR()                        \
+    KOI_DATA()	
 /*
  * Data section helpers
  */
@@ -1133,6 +1144,14 @@
  * They will fit only a subset of the architectures
  */
 
+#ifdef CONFIG_CREDP
+	#define CRED_DATA		\
+		. = ALIGN(PAGE_SIZE);		\
+		*(.iee.cred)		\
+		. = ALIGN(PAGE_SIZE);
+#else
+	#define CRED_DATA
+#endif
 
 /*
  * Writeable data.
@@ -1150,6 +1169,7 @@
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
 		INIT_TASK_DATA(inittask)				\
+		CRED_DATA						\
 		NOSAVE_DATA						\
 		PAGE_ALIGNED_DATA(pagealigned)				\
 		CACHELINE_ALIGNED_DATA(cacheline)			\
diff --git a/include/linux/cred.h b/include/linux/cred.h
index cd1b5fc47d52..1eec12653928 100644
--- a/include/linux/cred.h
+++ b/include/linux/cred.h
@@ -18,6 +18,10 @@
 #include <linux/sched/user.h>
 #include <linux/kabi.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-def.h>
+#endif
+
 struct cred;
 struct inode;
 
@@ -164,6 +168,22 @@ struct cred {
 	KABI_RESERVE(10)
 } __randomize_layout;
 
+#ifdef CONFIG_CREDP
+extern unsigned long long iee_rw_gate(int flag, ...);
+static void iee_set_cred_non_rcu(struct cred *cred, int non_rcu)
+{
+    iee_rw_gate(IEE_OP_SET_CRED_NON_RCU,cred,non_rcu);
+	*(int *)(&(((struct rcu_head *)(cred->rcu.func))->next)) = non_rcu;
+}
+
+static bool noinline iee_set_cred_atomic_op_usage(struct cred *cred, int flag)
+{
+    bool ret;
+    ret = iee_rw_gate(IEE_OP_SET_CRED_ATOP_USAGE,cred,flag);
+    return ret;
+}
+#endif
+
 extern void __put_cred(struct cred *);
 extern void exit_creds(struct task_struct *);
 extern int copy_creds(struct task_struct *, unsigned long);
@@ -239,7 +259,11 @@ static inline bool cap_ambient_invariant_ok(const struct cred *cred)
  */
 static inline struct cred *get_new_cred(struct cred *cred)
 {
+	#ifdef CONFIG_CREDP
+	iee_set_cred_atomic_op_usage(cred,AT_INC);
+	#else
 	atomic_inc(&cred->usage);
+	#endif
 	return cred;
 }
 
@@ -262,7 +286,11 @@ static inline const struct cred *get_cred(const struct cred *cred)
 	if (!cred)
 		return cred;
 	validate_creds(cred);
+	#ifdef CONFIG_CREDP
+	iee_set_cred_non_rcu(nonconst_cred,0);
+	#else
 	nonconst_cred->non_rcu = 0;
+	#endif
 	return get_new_cred(nonconst_cred);
 }
 
@@ -271,10 +299,19 @@ static inline const struct cred *get_cred_rcu(const struct cred *cred)
 	struct cred *nonconst_cred = (struct cred *) cred;
 	if (!cred)
 		return NULL;
+	#ifdef CONFIG_CREDP
+	if (!iee_set_cred_atomic_op_usage(nonconst_cred,AT_INC_NOT_ZERO))
+		return NULL;
+	#else
 	if (!atomic_inc_not_zero(&nonconst_cred->usage))
 		return NULL;
+	#endif
 	validate_creds(cred);
+	#ifdef CONFIG_CREDP
+	iee_set_cred_non_rcu(nonconst_cred,0);
+	#else
 	nonconst_cred->non_rcu = 0;
+	#endif
 	return cred;
 }
 
@@ -295,8 +332,13 @@ static inline void put_cred(const struct cred *_cred)
 
 	if (cred) {
 		validate_creds(cred);
+		#ifdef CONFIG_CREDP
+		if (iee_set_cred_atomic_op_usage(cred,AT_DEC_AND_TEST))
+			__put_cred(cred);
+		#else
 		if (atomic_dec_and_test(&(cred)->usage))
 			__put_cred(cred);
+		#endif
 	}
 }
 
diff --git a/include/linux/efi.h b/include/linux/efi.h
index 9816e03cf05b..ebdd48da4741 100644
--- a/include/linux/efi.h
+++ b/include/linux/efi.h
@@ -644,6 +644,9 @@ extern void __efi_memmap_free(u64 phys, unsigned long size,
 extern int __init efi_memmap_init_early(struct efi_memory_map_data *data);
 extern int __init efi_memmap_init_late(phys_addr_t addr, unsigned long size);
 extern void __init efi_memmap_unmap(void);
+#ifdef CONFIG_PTP
+extern void __init efi_memmap_unmap_after_init(void);
+#endif
 extern int __init efi_memmap_install(struct efi_memory_map_data *data);
 extern int __init efi_memmap_split_count(efi_memory_desc_t *md,
 					 struct range *range);
diff --git a/include/linux/iee-func.h b/include/linux/iee-func.h
new file mode 100644
index 000000000000..79171de67c2a
--- /dev/null
+++ b/include/linux/iee-func.h
@@ -0,0 +1,27 @@
+#ifndef _LINUX_IEE_FUNC_H
+#define _LINUX_IEE_FUNC_H
+
+#ifdef CONFIG_IEE
+// Declare the __entry_task.
+__attribute__((aligned(PAGE_SIZE))) DECLARE_PER_CPU(struct task_struct *[PAGE_SIZE/sizeof(struct task_struct *)], __entry_task);
+
+extern unsigned long long iee_rw_gate(int flag, ...);
+extern u32 get_cpu_asid_bits(void);
+extern unsigned long arm64_mm_context_get(struct mm_struct *mm);
+extern void set_iee_page_valid(unsigned long addr);
+extern void set_iee_page_invalid(unsigned long addr);
+extern void iee_set_logical_mem_ro(unsigned long addr);
+extern void iee_set_logical_mem_rw(unsigned long addr);
+extern void iee_set_token_mm(struct task_struct *tsk, struct mm_struct *mm);
+extern void iee_set_token_pgd(struct task_struct *tsk, pgd_t *pgd);
+extern void iee_init_token(struct task_struct *tsk, void *kernel_stack, void *iee_stack);
+extern void iee_free_token(struct task_struct *tsk);
+extern unsigned long iee_read_token_stack(struct task_struct *tsk);
+extern void iee_set_token_page_valid(void *token, void *new);
+extern void iee_set_token_page_invalid(void *token);
+extern void iee_set_kernel_ppage(unsigned long addr);
+extern void iee_set_kernel_upage(unsigned long addr);
+extern void iee_write_in_byte(void *ptr, u64 data, int length);
+#endif
+
+#endif
\ No newline at end of file
diff --git a/include/linux/module.h b/include/linux/module.h
index b2b2c742a397..9d23bc618e9d 100644
--- a/include/linux/module.h
+++ b/include/linux/module.h
@@ -568,6 +568,7 @@ struct module {
 	KABI_RESERVE(2)
 	KABI_RESERVE(3)
 	KABI_RESERVE(4)
+
 } ____cacheline_aligned __randomize_layout;
 #ifndef MODULE_ARCH_INIT
 #define MODULE_ARCH_INIT {}
diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4ab407cab37..e5f90ced5673 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -694,6 +694,24 @@ struct task_struct_resvd {
 #endif
 };
 
+#if defined(CONFIG_IEE) || defined(CONFIG_KOI)
+struct task_token {
+#ifdef CONFIG_IEE
+	struct mm_struct *mm; /* VA */
+	pgd_t *pgd; /* Logical VA */
+	void *iee_stack; /* VA */
+	bool valid;
+    void *kernel_stack; /* VA */
+#endif
+#ifdef CONFIG_KOI
+    void *koi_kernel_stack; /* VA */
+    void *koi_stack;    /* VA */
+    void *koi_stack_base; /* VA */
+    unsigned long   current_ttbr1;
+#endif
+};
+#endif
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -712,6 +730,7 @@ struct task_struct {
 	randomized_struct_fields_start
 
 	void				*stack;
+
 	refcount_t			usage;
 	/* Per task flags (PF_*), defined further below: */
 	unsigned int			flags;
diff --git a/include/linux/stacktrace.h b/include/linux/stacktrace.h
index 52e59df0faf4..241276592268 100644
--- a/include/linux/stacktrace.h
+++ b/include/linux/stacktrace.h
@@ -21,6 +21,10 @@ unsigned int stack_trace_save_tsk(struct task_struct *task,
 unsigned int stack_trace_save_regs(struct pt_regs *regs, unsigned long *store,
 				   unsigned int size, unsigned int skipnr);
 unsigned int stack_trace_save_user(unsigned long *store, unsigned int size);
+#ifdef CONFIG_IEE
+unsigned int stack_trace_save_iee(unsigned long *store, unsigned int size,
+                             unsigned int skipnr);
+#endif
 unsigned int filter_irq_stacks(unsigned long *entries, unsigned int nr_entries);
 
 /* Internal interfaces. Do not use in generic code */
diff --git a/init/main.c b/init/main.c
index f06fbe79a84a..bfbb7116c9b6 100644
--- a/init/main.c
+++ b/init/main.c
@@ -100,6 +100,12 @@
 #include <linux/init_syscalls.h>
 #include <linux/randomize_kstack.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#include <asm/iee-si.h>
+#include <linux/stop_machine.h>
+#endif
+
 #include <asm/io.h>
 #include <asm/setup.h>
 #include <asm/sections.h>
@@ -110,6 +116,10 @@
 
 #include <kunit/test.h>
 
+#ifdef CONFIG_PTP
+extern void *bm_pte_addr;
+#endif
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -842,6 +852,11 @@ static void __init mm_init(void)
 	/* Should be run after espfix64 is set up. */
 	pti_init();
 	mm_cache_init();
+#ifdef CONFIG_IEE
+    /* Copy swapper to iee_pg_dir again after vmemmap is built. */
+    memcpy(iee_pg_dir, swapper_pg_dir, PAGE_SIZE);
+	flush_tlb_all();
+#endif
 }
 
 #ifdef CONFIG_RANDOMIZE_KSTACK_OFFSET
@@ -876,6 +891,9 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 {
 	char *command_line;
 	char *after_dashes;
+	#ifdef CONFIG_IEE
+	unsigned int cpu;
+	#endif
 
 	set_task_stack_end_magic(&init_task);
 	smp_setup_processor_id();
@@ -899,6 +917,16 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	setup_command_line(command_line);
 	setup_nr_cpu_ids();
 	setup_per_cpu_areas();
+	#ifdef CONFIG_IEE
+	for_each_possible_cpu(cpu)
+	{
+		// Map the __entry_task to IEE.
+		set_iee_page_valid((unsigned long)__phys_to_iee(__pa(SHIFT_PERCPU_PTR(__entry_task,__per_cpu_offset[cpu]))));
+		// Set the __entry_task of cpu 0 readonly in lm.
+		if(cpu == smp_processor_id())
+			iee_set_logical_mem_ro((unsigned long)SHIFT_PERCPU_PTR(__entry_task,__per_cpu_offset[cpu]));
+	}
+	#endif
 	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */
 	boot_cpu_hotplug_init();
 
@@ -1435,11 +1463,137 @@ void __weak free_initmem(void)
 	free_initmem_default(POISON_FREE_INITMEM);
 }
 
+#ifdef CONFIG_IEE
+
+// Set up breakpoint control registers to protect iee rwx gate
+static void __init iee_si_init_dbg(void)
+{
+    unsigned long mdscr;
+    unsigned long dbgbcr, dbgbvr;
+
+    // use afsr0_el1 to mark whether user proc user breakpoint 0
+    write_sysreg(0, afsr0_el1);
+
+    // set mdscr_el1.MDE to enable breakpoint exception
+    mdscr = read_sysreg(mdscr_el1);
+    mdscr |= DBG_MDSCR_MDE | DBG_MDSCR_KDE;
+    write_sysreg(mdscr, mdscr_el1);
+
+    // set breakpoint control registers
+    dbgbcr = IEE_DBGBCR;
+	// calculate the location of msr ttbr1 inst.
+#ifdef CONFIG_KOI
+	dbgbvr = (unsigned long)(iee_rwx_gate_entry + 120);
+#else
+    dbgbvr = (unsigned long)(iee_rwx_gate_entry + 36);
+#endif
+    write_sysreg(dbgbcr, dbgbcr0_el1);
+    write_sysreg(dbgbvr, dbgbvr0_el1);
+    isb();
+}
+
+/* All software preparation for iee rwx gate is done after this function. */
+static void __init iee_si_sw_init_end(void)
+{
+    // void (*__jump_to_iee_si)(int flag, ...);
+	// remove exec permission of idmap functions inside kernel image.
+    mark_idmap_vmallc_map_ROU();
+    // test 0: jump to the start of protected area
+    // __jump_to_iee_si = IEE_SI_PGD_ENTRY;
+    
+    // test 1: jump to the inst of msr ttbr1 to test breakpoint prot
+    // iee_si_init_dbg();
+    // asm volatile("mrs x13, daif");
+	// asm volatile("msr daifset, #0b1010");
+    // __jump_to_iee_si = IEE_SI_PGD_ENTRY - 36;
+    // __jump_to_iee_si(IEE_SI_TEST);
+    // asm volatile("msr daif, x13");
+
+	/* Cpu features has been initialized now. Check whether cnp is enabled.*/
+	iee_si_set_base_swapper_cnp();
+	/*
+	 * Finally finish isolation by remap iee si setions and clear their kernel mapping.
+	 */ 
+	isolate_iee_si();
+	// iee_si_handler(IEE_SI_TEST);
+    flush_tlb_all();
+}
+
+atomic_t __initdata iee_si_cpu_cnt;
+
+static int __init iee_si_hw_init_cpu(void *__unused)
+{
+    int total_cpu;
+    iee_si_init_dbg();
+    // loop until setting all cpus
+    atomic_inc(&iee_si_cpu_cnt);
+    total_cpu = num_online_cpus();
+    while (atomic_read(&iee_si_cpu_cnt) < total_cpu)
+        cpu_relax();
+
+    return 0;
+}
+// setup hw features on all online cpus
+static void __init iee_si_hw_init(void)
+{
+    atomic_set(&iee_si_cpu_cnt, 0);
+    stop_machine(iee_si_hw_init_cpu, NULL, cpu_online_mask);
+    pr_info("IEE hw config done.\n");
+}
+
+static void __init iee_si_init_end(void)
+{
+	iee_si_hw_init();
+    // protect iee si metadata to finish initialization.
+    iee_si_sw_init_end();
+}
+
+static void iee_si_test_end(void)
+{
+    pr_info("IEE: testing iee_exec_entry sctlr...\n");
+    iee_rwx_gate_entry(IEE_WRITE_SCTLR, read_sysreg(sctlr_el1)& ~SCTLR_ELx_M);
+    pr_info("IEE: testing iee_exec_entry ttbr0_el1...\n");
+    iee_rwx_gate_entry(IEE_WRITE_TTBR0, read_sysreg(ttbr0_el1));
+    pr_info("IEE: testing iee_exec_entry vbar...\n");
+    iee_rwx_gate_entry(IEE_WRITE_VBAR, read_sysreg(vbar_el1));
+    pr_info("IEE: testing iee_exec_entry tcr...\n");
+    iee_rwx_gate_entry(IEE_WRITE_TCR, read_sysreg(tcr_el1));
+    pr_info("IEE: testing iee_exec_entry mdscr...\n");
+    iee_rwx_gate_entry(IEE_WRITE_MDSCR, read_sysreg(mdscr_el1));
+    pr_info("IEE: testing iee_exec_entry afsr0...\n");
+    iee_rwx_gate_entry(IEE_WRITE_AFSR0);
+	#ifdef CONFIG_KOI
+    write_sysreg(read_sysreg(ttbr0_el1)+0x3000000000000, ttbr0_el1);
+	pr_info("IEE: current TTBR1_EL1:%llx, TTBR0:%llx\n", read_sysreg(ttbr1_el1), read_sysreg(ttbr0_el1));
+	pr_info("IEE: testing iee_exec_entry switch to koi...\n");
+    iee_rwx_gate_entry(IEE_SWITCH_TO_KOI, phys_to_ttbr(__pa_symbol(swapper_pg_dir)));
+	pr_info("IEE: current TTBR1_EL1:%llx, TTBR0:%llx\n", read_sysreg(ttbr1_el1), read_sysreg(ttbr0_el1));
+	pr_info("IEE: testing iee_exec_entry switch to kernel...\n");
+    iee_rwx_gate_entry(IEE_SWITCH_TO_KERNEL);
+	#endif
+}
+#else
+static void __init iee_si_hw_init(void){};
+
+static void iee_si_test_end(void)
+{
+    pr_info("IEE: CONFIG_IEE not selected.\n");
+}
+
+#endif
+
 static int __ref kernel_init(void *unused)
 {
 	int ret;
 
 	kernel_init_freeable();
+	#ifdef CONFIG_PTP
+	iee_set_logical_mem_ro((unsigned long)bm_pte_addr);
+	#endif
+    // init hw features that iee_si uses
+#ifdef CONFIG_IEE
+    iee_si_init_end();
+#endif
 	/* need to finish all async __init code before freeing the memory */
 	async_synchronize_full();
 	kprobe_free_init_mem();
@@ -1454,6 +1608,9 @@ static int __ref kernel_init(void *unused)
 	 */
 	pti_finalize();
 
+    // test iee_si_code.
+    iee_si_test_end();
+    
 	system_state = SYSTEM_RUNNING;
 	numa_default_policy();
 
@@ -1543,6 +1700,11 @@ static noinline void __init kernel_init_freeable(void)
 	rcu_init_tasks_generic();
 	do_pre_smp_initcalls();
 
+	#ifdef CONFIG_IEE
+	/* Copy swapper to iee_pg_dir again before smp init to handle KASLR issues. */
+    memcpy(iee_pg_dir, swapper_pg_dir, PAGE_SIZE);
+	#endif
+
 	smp_init();
 	sched_init_smp();
 
diff --git a/kernel/cred.c b/kernel/cred.c
index 421b1149c651..5b124dc1061f 100644
--- a/kernel/cred.c
+++ b/kernel/cred.c
@@ -17,6 +17,11 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#include <linux/iee-func.h>
+#endif
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -31,6 +36,9 @@ do {									\
 #endif
 
 static struct kmem_cache *cred_jar;
+#ifdef CONFIG_CREDP
+static struct kmem_cache *rcu_jar;
+#endif
 
 /* init to 2 - one for init_task, one to ensure it is never freed */
 struct group_info init_groups = { .usage = ATOMIC_INIT(2) };
@@ -38,6 +46,31 @@ struct group_info init_groups = { .usage = ATOMIC_INIT(2) };
 /*
  * The initial credentials for the initial task
  */
+#ifdef CONFIG_CREDP
+struct cred init_cred __section(".iee.cred") = {
+	.usage			= ATOMIC_INIT(4),
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	.subscribers		= ATOMIC_INIT(2),
+	.magic			= CRED_MAGIC,
+#endif
+	.uid			= GLOBAL_ROOT_UID,
+	.gid			= GLOBAL_ROOT_GID,
+	.suid			= GLOBAL_ROOT_UID,
+	.sgid			= GLOBAL_ROOT_GID,
+	.euid			= GLOBAL_ROOT_UID,
+	.egid			= GLOBAL_ROOT_GID,
+	.fsuid			= GLOBAL_ROOT_UID,
+	.fsgid			= GLOBAL_ROOT_GID,
+	.securebits		= SECUREBITS_DEFAULT,
+	.cap_inheritable	= CAP_EMPTY_SET,
+	.cap_permitted		= CAP_FULL_SET,
+	.cap_effective		= CAP_FULL_SET,
+	.cap_bset		= CAP_FULL_SET,
+	.user			= INIT_USER,
+	.user_ns		= &init_user_ns,
+	.group_info		= &init_groups,
+};
+#else
 struct cred init_cred = {
 	.usage			= ATOMIC_INIT(4),
 #ifdef CONFIG_DEBUG_CREDENTIALS
@@ -61,6 +94,7 @@ struct cred init_cred = {
 	.user_ns		= &init_user_ns,
 	.group_info		= &init_groups,
 };
+#endif
 
 static inline void set_cred_subscribers(struct cred *cred, int n)
 {
@@ -92,7 +126,11 @@ static inline void alter_cred_subscribers(const struct cred *_cred, int n)
  */
 static void put_cred_rcu(struct rcu_head *rcu)
 {
+	#ifdef CONFIG_CREDP
+	struct cred *cred = *(struct cred **)(rcu + 1);
+	#else
 	struct cred *cred = container_of(rcu, struct cred, rcu);
+	#endif
 
 	kdebug("put_cred_rcu(%p)", cred);
 
@@ -120,6 +158,9 @@ static void put_cred_rcu(struct rcu_head *rcu)
 		put_group_info(cred->group_info);
 	free_uid(cred->user);
 	put_user_ns(cred->user_ns);
+	#ifdef CONFIG_CREDP
+	kmem_cache_free(rcu_jar, (struct rcu_head *)(cred->rcu.func));
+	#endif
 	kmem_cache_free(cred_jar, cred);
 }
 
@@ -144,10 +185,22 @@ void __put_cred(struct cred *cred)
 	BUG_ON(cred == current->cred);
 	BUG_ON(cred == current->real_cred);
 
+	#ifdef CONFIG_CREDP
+	if (*(int *)(&(((struct rcu_head *)(cred->rcu.func))->next)))
+	#else
 	if (cred->non_rcu)
+	#endif
+		#ifdef CONFIG_CREDP
+		put_cred_rcu((struct rcu_head *)(cred->rcu.func));
+		#else
 		put_cred_rcu(&cred->rcu);
+		#endif
 	else
+		#ifdef CONFIG_CREDP
+		call_rcu((struct rcu_head *)(cred->rcu.func), put_cred_rcu);
+		#else
 		call_rcu(&cred->rcu, put_cred_rcu);
+		#endif
 }
 EXPORT_SYMBOL(__put_cred);
 
@@ -218,7 +271,13 @@ struct cred *cred_alloc_blank(void)
 	if (!new)
 		return NULL;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_rcu(new,kmem_cache_zalloc(rcu_jar, GFP_KERNEL));
+	*(struct cred **)(((struct rcu_head *)(new->rcu.func)) + 1) = new;
+	iee_set_cred_atomic_set_usage(new,1);
+	#else
 	atomic_set(&new->usage, 1);
+	#endif
 #ifdef CONFIG_DEBUG_CREDENTIALS
 	new->magic = CRED_MAGIC;
 #endif
@@ -259,13 +318,25 @@ struct cred *prepare_creds(void)
 	if (!new)
 		return NULL;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_rcu(new,kmem_cache_alloc(rcu_jar, GFP_KERNEL));
+	*(struct cred **)(((struct rcu_head *)(new->rcu.func)) + 1) = new;
+	#endif
+
 	kdebug("prepare_creds() alloc %p", new);
 
 	old = task->cred;
+	#ifdef CONFIG_CRED_ISO
+	iee_copy_cred(old,new);
+
+	iee_set_cred_non_rcu(new,0);
+	iee_set_cred_atomic_set_usage(new,1);
+	#else
 	memcpy(new, old, sizeof(struct cred));
 
 	new->non_rcu = 0;
 	atomic_set(&new->usage, 1);
+	#endif
 	set_cred_subscribers(new, 0);
 	get_group_info(new->group_info);
 	get_uid(new->user);
@@ -279,7 +350,11 @@ struct cred *prepare_creds(void)
 #endif
 
 #ifdef CONFIG_SECURITY
+#ifdef CONFIG_CRED_ISO
+	iee_set_cred_security(new,NULL);
+#else
 	new->security = NULL;
+#endif
 #endif
 
 	if (security_prepare_creds(new, old, GFP_KERNEL_ACCOUNT) < 0)
@@ -308,15 +383,30 @@ struct cred *prepare_exec_creds(void)
 #ifdef CONFIG_KEYS
 	/* newly exec'd tasks don't get a thread keyring */
 	key_put(new->thread_keyring);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(new,NULL);
+	#else
 	new->thread_keyring = NULL;
+	#endif
 
 	/* inherit the session keyring; new process keyring */
 	key_put(new->process_keyring);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_process_keyring(new,NULL);
+	#else
 	new->process_keyring = NULL;
+	#endif
 #endif
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,new->euid);
+	iee_set_cred_suid(new,new->euid);
+	iee_set_cred_fsgid(new,new->egid);
+	iee_set_cred_sgid(new,new->egid);
+	#else
 	new->suid = new->fsuid = new->euid;
 	new->sgid = new->fsgid = new->egid;
+	#endif
 
 	return new;
 }
@@ -370,7 +460,11 @@ int copy_creds(struct task_struct *p, unsigned long clone_flags)
 	 * had one */
 	if (new->thread_keyring) {
 		key_put(new->thread_keyring);
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_thread_keyring(new,NULL);
+		#else
 		new->thread_keyring = NULL;
+		#endif
 		if (clone_flags & CLONE_THREAD)
 			install_thread_keyring_to_cred(new);
 	}
@@ -380,7 +474,11 @@ int copy_creds(struct task_struct *p, unsigned long clone_flags)
 	 */
 	if (!(clone_flags & CLONE_THREAD)) {
 		key_put(new->process_keyring);
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_process_keyring(new,NULL);
+		#else
 		new->process_keyring = NULL;
+		#endif
 	}
 #endif
 
@@ -659,8 +757,21 @@ EXPORT_SYMBOL(cred_fscmp);
 void __init cred_init(void)
 {
 	/* allocate a slab in which we can store credentials */
+	#ifdef CONFIG_CREDP
 	cred_jar = kmem_cache_create("cred_jar", sizeof(struct cred), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT|SLAB_RED_ZONE, NULL);
+	rcu_jar = kmem_cache_create("rcu_jar", sizeof(struct rcu_head) + sizeof(struct cred *), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT, NULL);
+	// Map init_cred
+	*((struct rcu_head **)(&(init_cred.rcu.func))) = (struct rcu_head *)kmem_cache_zalloc(rcu_jar, GFP_KERNEL);
+	*(struct cred **)(((struct rcu_head *)(init_cred.rcu.func)) + 1) = &init_cred;
+	set_iee_page_valid(__phys_to_iee(__pa_symbol(&init_cred)));
+	iee_set_logical_mem_ro((unsigned long)&init_cred);
+	iee_set_logical_mem_ro((unsigned long)__va(__pa_symbol(&init_cred)));
+	#else
+	cred_jar = kmem_cache_create("cred_jar", sizeof(struct cred), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT, NULL);
+	#endif
 }
 
 /**
@@ -688,6 +799,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 	if (!new)
 		return NULL;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_rcu(new,kmem_cache_alloc(rcu_jar, GFP_KERNEL));
+	*(struct cred **)(((struct rcu_head *)(new->rcu.func)) + 1) = new;
+	#endif
+
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
 	if (daemon)
@@ -697,24 +813,42 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	validate_creds(old);
 
+	#ifdef CONFIG_CRED_ISO
+	iee_copy_cred(old,new);
+	iee_set_cred_non_rcu(new,0);
+	iee_set_cred_atomic_set_usage(new,1);
+	#else
 	*new = *old;
 	new->non_rcu = 0;
 	atomic_set(&new->usage, 1);
+	#endif
 	set_cred_subscribers(new, 0);
 	get_uid(new->user);
 	get_user_ns(new->user_ns);
 	get_group_info(new->group_info);
 
 #ifdef CONFIG_KEYS
+#ifdef CONFIG_CRED_ISO
+	iee_set_cred_session_keyring(new,NULL);
+	iee_set_cred_process_keyring(new,NULL);
+	iee_set_cred_thread_keyring(new,NULL);
+	iee_set_cred_request_key_auth(new,NULL);
+	iee_set_cred_jit_keyring(new,KEY_REQKEY_DEFL_THREAD_KEYRING);
+#else
 	new->session_keyring = NULL;
 	new->process_keyring = NULL;
 	new->thread_keyring = NULL;
 	new->request_key_auth = NULL;
 	new->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;
 #endif
+#endif
 
 #ifdef CONFIG_SECURITY
+#ifdef CONFIG_CRED_ISO
+	iee_set_cred_security(new,NULL);
+#else
 	new->security = NULL;
+#endif
 #endif
 	if (security_prepare_creds(new, old, GFP_KERNEL_ACCOUNT) < 0)
 		goto error;
@@ -780,8 +914,13 @@ int set_create_files_as(struct cred *new, struct inode *inode)
 {
 	if (!uid_valid(inode->i_uid) || !gid_valid(inode->i_gid))
 		return -EINVAL;
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,inode->i_uid);
+	iee_set_cred_fsgid(new,inode->i_gid);
+	#else
 	new->fsuid = inode->i_uid;
 	new->fsgid = inode->i_gid;
+	#endif
 	return security_kernel_create_files_as(new, inode);
 }
 EXPORT_SYMBOL(set_create_files_as);
diff --git a/kernel/exit.c b/kernel/exit.c
index 26a81ea63156..1cd03c29d349 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -70,6 +70,10 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#endif
+
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;
@@ -481,6 +485,10 @@ static void exit_mm(void)
 	/* more a memory barrier than a real lock */
 	task_lock(current);
 	current->mm = NULL;
+	#ifdef CONFIG_IEE
+	iee_set_token_mm(current, NULL);
+	iee_set_token_pgd(current, NULL);
+	#endif
 	mmap_read_unlock(mm);
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
diff --git a/kernel/fork.c b/kernel/fork.c
index 079b718131b0..37f955d71e58 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -112,6 +112,10 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/task.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#endif
+
 /*
  * Minimum number of threads to boot the kernel
  */
@@ -125,14 +129,14 @@
 /*
  * Protected counters by write_lock_irq(&tasklist_lock)
  */
-unsigned long total_forks;	/* Handle normal Linux uptimes. */
-int nr_threads;			/* The idle threads do not count.. */
+unsigned long total_forks; /* Handle normal Linux uptimes. */
+int nr_threads; /* The idle threads do not count.. */
 
-static int max_threads;		/* tunable limit on nr_threads */
+static int max_threads; /* tunable limit on nr_threads */
 
-#define NAMED_ARRAY_INDEX(x)	[x] = __stringify(x)
+#define NAMED_ARRAY_INDEX(x) [x] = __stringify(x)
 
-static const char * const resident_page_types[] = {
+static const char *const resident_page_types[] = {
 	NAMED_ARRAY_INDEX(MM_FILEPAGES),
 	NAMED_ARRAY_INDEX(MM_ANONPAGES),
 	NAMED_ARRAY_INDEX(MM_SWAPENTS),
@@ -141,7 +145,7 @@ static const char * const resident_page_types[] = {
 
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
-__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+__cacheline_aligned DEFINE_RWLOCK(tasklist_lock); /* outer */
 
 #ifdef CONFIG_PROVE_RCU
 int lockdep_tasklist_lock_is_held(void)
@@ -156,7 +160,7 @@ int nr_processes(void)
 	int cpu;
 	int total = 0;
 
-	for_each_possible_cpu(cpu)
+	for_each_possible_cpu (cpu)
 		total += per_cpu(process_counts, cpu);
 
 	return total;
@@ -187,7 +191,7 @@ static inline void free_task_struct(struct task_struct *tsk)
  * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a
  * kmemcache based allocator.
  */
-# if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)
+#if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)
 
 #ifdef CONFIG_VMAP_STACK
 /*
@@ -246,11 +250,11 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 	 * so memcg accounting is performed manually on assigning/releasing
 	 * stacks to tasks. Drop __GFP_ACCOUNT.
 	 */
-	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,
-				     VMALLOC_START, VMALLOC_END,
+	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN, VMALLOC_START,
+				     VMALLOC_END,
 				     THREADINFO_GFP & ~__GFP_ACCOUNT,
-				     PAGE_KERNEL,
-				     0, node, __builtin_return_address(0));
+				     PAGE_KERNEL, 0, node,
+				     __builtin_return_address(0));
 
 	/*
 	 * We can't call find_vm_area() in interrupt context, and
@@ -263,8 +267,8 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 	}
 	return stack;
 #else
-	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
-					     THREAD_SIZE_ORDER);
+	struct page *page =
+		alloc_pages_node(node, THREADINFO_GFP, THREAD_SIZE_ORDER);
 
 	if (likely(page)) {
 		tsk->stack = kasan_reset_tag(page_address(page));
@@ -286,8 +290,8 @@ static inline void free_thread_stack(struct task_struct *tsk)
 			memcg_kmem_uncharge_page(vm->pages[i], 0);
 
 		for (i = 0; i < NR_CACHED_STACKS; i++) {
-			if (this_cpu_cmpxchg(cached_stacks[i],
-					NULL, tsk->stack_vm_area) != NULL)
+			if (this_cpu_cmpxchg(cached_stacks[i], NULL,
+					     tsk->stack_vm_area) != NULL)
 				continue;
 
 			return;
@@ -300,11 +304,10 @@ static inline void free_thread_stack(struct task_struct *tsk)
 
 	__free_pages(virt_to_page(tsk->stack), THREAD_SIZE_ORDER);
 }
-# else
+#else
 static struct kmem_cache *thread_stack_cache;
 
-static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
-						  int node)
+static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 {
 	unsigned long *stack;
 	stack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
@@ -320,12 +323,13 @@ static void free_thread_stack(struct task_struct *tsk)
 
 void thread_stack_cache_init(void)
 {
-	thread_stack_cache = kmem_cache_create_usercopy("thread_stack",
-					THREAD_SIZE, THREAD_SIZE, 0, 0,
-					THREAD_SIZE, NULL);
+	thread_stack_cache =
+		kmem_cache_create_usercopy("thread_stack", THREAD_SIZE,
+					   THREAD_SIZE, 0, 0, THREAD_SIZE,
+					   NULL);
 	BUG_ON(thread_stack_cache == NULL);
 }
-# endif
+#endif
 #endif
 
 /* SLAB cache for signal_struct structures (tsk->signal) */
@@ -358,7 +362,8 @@ struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 
 struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 {
-	struct vm_area_struct *new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	struct vm_area_struct *new =
+		kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 
 	if (new) {
 		ASSERT_EXCLUSIVE_WRITER(orig->vm_flags);
@@ -384,7 +389,6 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 	void *stack = task_stack_page(tsk);
 	struct vm_struct *vm = task_stack_vm_area(tsk);
 
-
 	/* All stack pages are in the same node. */
 	if (vm)
 		mod_lruvec_page_state(vm->pages[0], NR_KERNEL_STACK_KB,
@@ -427,7 +431,7 @@ static int memcg_charge_kernel_stack(struct task_struct *tsk)
 static void release_task_stack(struct task_struct *tsk)
 {
 	if (WARN_ON(tsk->state != TASK_DEAD))
-		return;  /* Better to leak the stack than to free prematurely */
+		return; /* Better to leak the stack than to free prematurely */
 
 	account_kernel_stack(tsk, -1);
 	free_thread_stack(tsk);
@@ -445,8 +449,15 @@ void put_task_stack(struct task_struct *tsk)
 }
 #endif
 
+#ifdef CONFIG_KOI
+extern s64 koi_offset;
+#endif
+
 void free_task(struct task_struct *tsk)
 {
+	#ifdef CONFIG_IEE
+	void *iee_stack;
+	#endif
 #ifdef CONFIG_SECCOMP
 	WARN_ON_ONCE(tsk->seccomp.filter);
 #endif
@@ -473,6 +484,43 @@ void free_task(struct task_struct *tsk)
 #ifdef CONFIG_QOS_SCHED_DYNAMIC_AFFINITY
 	sched_prefer_cpus_free(tsk);
 #endif
+#ifdef CONFIG_IEE
+	// Free iee stack.
+	iee_stack = (void *)iee_read_token_stack(tsk);
+	if (iee_stack) {
+		iee_set_kernel_ppage(
+			(unsigned long)(iee_stack - PAGE_SIZE * 4));
+		free_pages((unsigned long)(iee_stack - PAGE_SIZE * 4), 3);
+	}
+	// Free task_token.
+	// Empty the token
+	iee_free_token(tsk);
+#ifdef CONFIG_KOI
+    // Free koi stack.
+    unsigned long koi_stack = iee_rw_gate(IEE_READ_KOI_STACK_BASE, current);
+    if (koi_stack != 0)
+        free_pages(koi_stack, 2);
+#endif
+#else
+#ifdef CONFIG_KOI
+// free koi stack
+    struct task_token *token = (struct task_token *)((unsigned long)current + koi_offset);
+    unsigned long flags;
+    local_irq_save(flags);
+    asm volatile(
+        "at s1e1r, %0\n"
+        "isb\n"
+        :
+        :"r"(token));
+    unsigned long res = read_sysreg(par_el1);
+    local_irq_restore(flags);
+    if (!(res & 0x1)) {
+        unsigned long koi_stack = token->koi_stack_base;
+        if (koi_stack != 0)
+            free_pages(koi_stack, 2);
+    }
+#endif
+#endif
 #ifdef CONFIG_QOS_SCHED_SMART_GRID
 	sched_grid_qos_free(tsk);
 #endif
@@ -482,7 +530,7 @@ EXPORT_SYMBOL(free_task);
 
 #ifdef CONFIG_MMU
 static __latent_entropy int dup_mmap(struct mm_struct *mm,
-					struct mm_struct *oldmm)
+				     struct mm_struct *oldmm)
 {
 	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
 	struct rb_node **rb_link, *rb_parent;
@@ -578,7 +626,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
 			vma_interval_tree_insert_after(tmp, mpnt,
-					&mapping->i_mmap);
+						       &mapping->i_mmap);
 			flush_dcache_mmap_unlock(mapping);
 			i_mmap_unlock_write(mapping);
 		}
@@ -653,7 +701,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	mmap_write_unlock(oldmm);
 	return 0;
 }
-#define mm_alloc_pgd(mm)	(0)
+#define mm_alloc_pgd(mm) (0)
 #define mm_free_pgd(mm)
 #endif /* CONFIG_MMU */
 
@@ -661,28 +709,30 @@ static void check_mm(struct mm_struct *mm)
 {
 	int i;
 
-	BUILD_BUG_ON_MSG(ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,
-			 "Please make sure 'struct resident_page_types[]' is updated as well");
+	BUILD_BUG_ON_MSG(
+		ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,
+		"Please make sure 'struct resident_page_types[]' is updated as well");
 
 	for (i = 0; i < NR_MM_COUNTERS; i++) {
 		long x = atomic_long_read(&mm->rss_stat.count[i]);
 
 		if (unlikely(x))
-			pr_alert("BUG: Bad rss-counter state mm:%p type:%s val:%ld\n",
-				 mm, resident_page_types[i], x);
+			pr_alert(
+				"BUG: Bad rss-counter state mm:%p type:%s val:%ld\n",
+				mm, resident_page_types[i], x);
 	}
 
 	if (mm_pgtables_bytes(mm))
 		pr_alert("BUG: non-zero pgtables_bytes on freeing mm: %ld\n",
-				mm_pgtables_bytes(mm));
+			 mm_pgtables_bytes(mm));
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
 #endif
 }
 
-#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
-#define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
+#define allocate_mm() (kmem_cache_alloc(mm_cachep, GFP_KERNEL))
+#define free_mm(mm) (kmem_cache_free(mm_cachep, (mm)))
 
 /*
  * Called when the last reference to the mm
@@ -767,7 +817,9 @@ void __put_task_struct_rcu_cb(struct rcu_head *rhp)
 }
 EXPORT_SYMBOL_GPL(__put_task_struct_rcu_cb);
 
-void __init __weak arch_task_cache_init(void) { }
+void __init __weak arch_task_cache_init(void)
+{
+}
 
 /*
  * set_max_threads
@@ -784,8 +836,8 @@ static void set_max_threads(unsigned int max_threads_suggested)
 	if (fls64(nr_pages) + fls64(PAGE_SIZE) > 64)
 		threads = MAX_THREADS;
 	else
-		threads = div64_u64((u64) nr_pages * (u64) PAGE_SIZE,
-				    (u64) THREAD_SIZE * 8UL);
+		threads = div64_u64((u64)nr_pages * (u64)PAGE_SIZE,
+				    (u64)THREAD_SIZE * 8UL);
 
 	if (threads > max_threads_suggested)
 		threads = max_threads_suggested;
@@ -820,17 +872,24 @@ void __init fork_init(void)
 	int i;
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
-#define ARCH_MIN_TASKALIGN	0
+#define ARCH_MIN_TASKALIGN 0
 #endif
 	int align = max_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
 	unsigned long useroffset, usersize;
 
 	/* create a slab on which task_structs can be allocated */
 	task_struct_whitelist(&useroffset, &usersize);
+	#ifdef CONFIG_IEE
 	task_struct_cachep = kmem_cache_create_usercopy("task_struct",
 			arch_task_struct_size, align,
-			SLAB_PANIC|SLAB_ACCOUNT,
+			SLAB_PANIC|SLAB_ACCOUNT|SLAB_RED_ZONE,
 			useroffset, usersize, NULL);
+	#else
+	task_struct_cachep =
+		kmem_cache_create_usercopy("task_struct", arch_task_struct_size,
+					   align, SLAB_PANIC | SLAB_ACCOUNT,
+					   useroffset, usersize, NULL);
+	#endif
 #endif
 
 	/* do the arch specific task caches init */
@@ -838,18 +897,18 @@ void __init fork_init(void)
 
 	set_max_threads(MAX_THREADS);
 
-	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
-	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
+	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads / 2;
+	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads / 2;
 	init_task.signal->rlim[RLIMIT_SIGPENDING] =
 		init_task.signal->rlim[RLIMIT_NPROC];
 
 	for (i = 0; i < UCOUNT_COUNTS; i++) {
-		init_user_ns.ucount_max[i] = max_threads/2;
+		init_user_ns.ucount_max[i] = max_threads / 2;
 	}
 
 #ifdef CONFIG_VMAP_STACK
-	cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "fork:vm_stack_cache",
-			  NULL, free_vm_stack_cache);
+	cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "fork:vm_stack_cache", NULL,
+			  free_vm_stack_cache);
 #endif
 
 	scs_init();
@@ -859,7 +918,7 @@ void __init fork_init(void)
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,
-					       struct task_struct *src)
+				struct task_struct *src)
 {
 	*dst = *src;
 	return 0;
@@ -870,14 +929,14 @@ void set_task_stack_end_magic(struct task_struct *tsk)
 	unsigned long *stackend;
 
 	stackend = end_of_stack(tsk);
-	*stackend = STACK_END_MAGIC;	/* for overflow detection */
+	*stackend = STACK_END_MAGIC; /* for overflow detection */
 }
 
 static bool dup_resvd_task_struct(struct task_struct *dst,
 				  struct task_struct *orig, int node)
 {
-	dst->_resvd = kzalloc_node(sizeof(struct task_struct_resvd),
-					  GFP_KERNEL, node);
+	dst->_resvd = kzalloc_node(sizeof(struct task_struct_resvd), GFP_KERNEL,
+				   node);
 	if (!dst->_resvd)
 		return false;
 
@@ -1052,7 +1111,7 @@ static void mm_init_uprobes_state(struct mm_struct *mm)
 }
 
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
-	struct user_namespace *user_ns)
+				 struct user_namespace *user_ns)
 {
 	mm->mmap = NULL;
 	mm->mm_rb = RB_ROOT;
@@ -1112,7 +1171,8 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	return NULL;
 }
 
-static inline void mm_struct_clear(struct mm_struct *mm) {
+static inline void mm_struct_clear(struct mm_struct *mm)
+{
 	memset(mm, 0, sizeof(*mm));
 
 #if (defined(CONFIG_X86_64))
@@ -1120,9 +1180,10 @@ static inline void mm_struct_clear(struct mm_struct *mm) {
 	 * init the mm_struct_extend extra area at the bottom of
 	 * the allocated mm struct and reset mm->mm_extend accordingly.
 	 */
-	memset((void *)((unsigned long) mm + _MM_STRUCT_SIZE),
-		0, sizeof(struct mm_struct_extend));
-	mm->mm_extend = (struct mm_struct_extend *)((unsigned long) mm + _MM_STRUCT_SIZE);
+	memset((void *)((unsigned long)mm + _MM_STRUCT_SIZE), 0,
+	       sizeof(struct mm_struct_extend));
+	mm->mm_extend = (struct mm_struct_extend *)((unsigned long)mm +
+						    _MM_STRUCT_SIZE);
 #endif
 }
 
@@ -1181,8 +1242,8 @@ EXPORT_SYMBOL_GPL(mmput);
 #ifdef CONFIG_MMU
 static void mmput_async_fn(struct work_struct *work)
 {
-	struct mm_struct *mm = container_of(work, struct mm_struct,
-					    async_put_work);
+	struct mm_struct *mm =
+		container_of(work, struct mm_struct, async_put_work);
 
 	__mmput(mm);
 }
@@ -1312,13 +1373,12 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 	struct mm_struct *mm;
 	int err;
 
-	err =  down_read_killable(&task->signal->exec_update_lock);
+	err = down_read_killable(&task->signal->exec_update_lock);
 	if (err)
 		return ERR_PTR(err);
 
 	mm = get_task_mm(task);
-	if (mm && mm != current->mm &&
-			!ptrace_may_access(task, mode)) {
+	if (mm && mm != current->mm && !ptrace_may_access(task, mode)) {
 		mmput(mm);
 		mm = ERR_PTR(-EACCES);
 	}
@@ -1342,7 +1402,7 @@ static void complete_vfork_done(struct task_struct *tsk)
 }
 
 static int wait_for_vfork_done(struct task_struct *child,
-				struct completion *vfork)
+			       struct completion *vfork)
 {
 	int killed;
 
@@ -1395,8 +1455,8 @@ static void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 			 * not set up a proper pointer then tough luck.
 			 */
 			put_user(0, tsk->clear_child_tid);
-			do_futex(tsk->clear_child_tid, FUTEX_WAKE,
-					1, NULL, NULL, 0, 0);
+			do_futex(tsk->clear_child_tid, FUTEX_WAKE, 1, NULL,
+				 NULL, 0, 0);
 		}
 		tsk->clear_child_tid = NULL;
 	}
@@ -1431,11 +1491,12 @@ static inline void mm_struct_copy(struct mm_struct *mm, struct mm_struct *oldmm)
 	 * the oldmm slab object over to the newly allocated mm struct,
 	 * and reset mm->mm_extend accordingly.
 	 */
-	memcpy((void *)((unsigned long) mm + _MM_STRUCT_SIZE),
-		(void *)((unsigned long) oldmm + _MM_STRUCT_SIZE),
-		sizeof(struct mm_struct_extend));
+	memcpy((void *)((unsigned long)mm + _MM_STRUCT_SIZE),
+	       (void *)((unsigned long)oldmm + _MM_STRUCT_SIZE),
+	       sizeof(struct mm_struct_extend));
 
-	mm->mm_extend = (struct mm_struct_extend *)((unsigned long) mm + _MM_STRUCT_SIZE);
+	mm->mm_extend = (struct mm_struct_extend *)((unsigned long)mm +
+						    _MM_STRUCT_SIZE);
 #endif
 }
 
@@ -1499,6 +1560,10 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 #endif
 
 	tsk->mm = NULL;
+#ifdef CONFIG_IEE
+	iee_set_token_mm(tsk, NULL);
+	iee_set_token_pgd(tsk, NULL);
+#endif
 	tsk->active_mm = NULL;
 
 	/*
@@ -1526,6 +1591,10 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 
 good_mm:
 	tsk->mm = mm;
+#ifdef CONFIG_IEE
+	iee_set_token_mm(tsk, mm);
+	iee_set_token_pgd(tsk, mm->pgd);
+#endif
 	tsk->active_mm = mm;
 	return 0;
 
@@ -1765,8 +1834,8 @@ static inline void init_task_pid_links(struct task_struct *task)
 	}
 }
 
-static inline void
-init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
+static inline void init_task_pid(struct task_struct *task, enum pid_type type,
+				 struct pid *pid)
 {
 	if (type == PIDTYPE_PID)
 		task->thread_pid = pid;
@@ -1941,6 +2010,12 @@ static void copy_oom_score_adj(u64 clone_flags, struct task_struct *tsk)
 	mutex_unlock(&oom_adj_mutex);
 }
 
+#if defined(CONFIG_KOI) && !defined(CONFIG_IEE)
+extern s64 koi_offset;
+extern int koi_add_page_mapping(unsigned long dst, unsigned long src);
+#endif
+
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -1949,11 +2024,9 @@ static void copy_oom_score_adj(u64 clone_flags, struct task_struct *tsk)
  * parts of the process environment (as per the clone
  * flags). The actual kick-off is left to the caller.
  */
-static __latent_entropy struct task_struct *copy_process(
-					struct pid *pid,
-					int trace,
-					int node,
-					struct kernel_clone_args *args)
+static __latent_entropy struct task_struct *
+copy_process(struct pid *pid, int trace, int node,
+	     struct kernel_clone_args *args)
 {
 	int pidfd = -1, retval;
 	struct task_struct *p;
@@ -1961,15 +2034,21 @@ static __latent_entropy struct task_struct *copy_process(
 	struct file *pidfile = NULL;
 	u64 clone_flags = args->flags;
 	struct nsproxy *nsp = current->nsproxy;
+	#ifdef CONFIG_IEE
+	gfp_t gfp;
+	void *pstack;
+	#endif
 
 	/*
 	 * Don't allow sharing the root directory with processes in a different
 	 * namespace
 	 */
-	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
+	if ((clone_flags & (CLONE_NEWNS | CLONE_FS)) ==
+	    (CLONE_NEWNS | CLONE_FS))
 		return ERR_PTR(-EINVAL);
 
-	if ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))
+	if ((clone_flags & (CLONE_NEWUSER | CLONE_FS)) ==
+	    (CLONE_NEWUSER | CLONE_FS))
 		return ERR_PTR(-EINVAL);
 
 	/*
@@ -1994,7 +2073,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 * from creating siblings.
 	 */
 	if ((clone_flags & CLONE_PARENT) &&
-				current->signal->flags & SIGNAL_UNKILLABLE)
+	    current->signal->flags & SIGNAL_UNKILLABLE)
 		return ERR_PTR(-EINVAL);
 
 	/*
@@ -2048,13 +2127,22 @@ static __latent_entropy struct task_struct *copy_process(
 	p = dup_task_struct(current, node);
 	if (!p)
 		goto fork_out;
+
+#ifdef CONFIG_IEE
+	// Alloc iee stack.
+	gfp = GFP_KERNEL;
+	pstack = (void *)__get_free_pages(gfp, 3);
+	iee_set_kernel_upage((unsigned long)pstack);
+	// Init token.
+	iee_init_token(p, NULL, pstack + PAGE_SIZE * 4);
+#endif
 	if (args->io_thread) {
 		/*
 		 * Mark us an IO worker, and block any signal that isn't
 		 * fatal or STOP
 		 */
 		p->flags |= PF_IO_WORKER;
-		siginitsetinv(&p->blocked, sigmask(SIGKILL)|sigmask(SIGSTOP));
+		siginitsetinv(&p->blocked, sigmask(SIGKILL) | sigmask(SIGSTOP));
 	}
 
 	/*
@@ -2063,11 +2151,13 @@ static __latent_entropy struct task_struct *copy_process(
 	 * p->set_child_tid which is (ab)used as a kthread's data pointer for
 	 * kernel threads (PF_KTHREAD).
 	 */
-	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;
+	p->set_child_tid =
+		(clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;
 	/*
 	 * Clear TID on mm_release()?
 	 */
-	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;
+	p->clear_child_tid =
+		(clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;
 
 	ftrace_graph_init_task(p);
 
@@ -2085,7 +2175,7 @@ static __latent_entropy struct task_struct *copy_process(
 #endif
 	retval = -EAGAIN;
 	if (atomic_read(&p->real_cred->user->processes) >=
-			task_rlimit(p, RLIMIT_NPROC)) {
+	    task_rlimit(p, RLIMIT_NPROC)) {
 		if (p->real_cred->user != INIT_USER &&
 		    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))
 			goto bad_fork_free;
@@ -2111,7 +2201,7 @@ static __latent_entropy struct task_struct *copy_process(
 	if (data_race(nr_threads >= max_threads))
 		goto bad_fork_cleanup_count;
 
-	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
+	delayacct_tsk_init(p); /* Must remain after dup_task_struct() */
 	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);
 	p->flags |= PF_FORKNOEXEC;
 	INIT_LIST_HEAD(&p->children);
@@ -2171,10 +2261,10 @@ static __latent_entropy struct task_struct *copy_process(
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	memset(&p->irqtrace, 0, sizeof(p->irqtrace));
-	p->irqtrace.hardirq_disable_ip	= _THIS_IP_;
-	p->irqtrace.softirq_enable_ip	= _THIS_IP_;
-	p->softirqs_enabled		= 1;
-	p->softirq_context		= 0;
+	p->irqtrace.hardirq_disable_ip = _THIS_IP_;
+	p->irqtrace.softirq_enable_ip = _THIS_IP_;
+	p->softirqs_enabled = 1;
+	p->softirq_context = 0;
 #endif
 
 	p->pagefault_disabled = 0;
@@ -2187,8 +2277,8 @@ static __latent_entropy struct task_struct *copy_process(
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
 #ifdef CONFIG_BCACHE
-	p->sequential_io	= 0;
-	p->sequential_io_avg	= 0;
+	p->sequential_io = 0;
+	p->sequential_io_avg = 0;
 #endif
 #ifdef CONFIG_BPF_SYSCALL
 	p->bpf_ctx = NULL;
@@ -2234,7 +2324,8 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_io(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_namespaces;
-	retval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);
+	retval = copy_thread(clone_flags, args->stack, args->stack_size, p,
+			     args->tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
 
@@ -2262,13 +2353,13 @@ static __latent_entropy struct task_struct *copy_process(
 		pidfd = retval;
 
 		pidfile = anon_inode_getfile("[pidfd]", &pidfd_fops, pid,
-					      O_RDWR | O_CLOEXEC);
+					     O_RDWR | O_CLOEXEC);
 		if (IS_ERR(pidfile)) {
 			put_unused_fd(pidfd);
 			retval = PTR_ERR(pidfile);
 			goto bad_fork_free_pid;
 		}
-		get_pid(pid);	/* held by pidfile now */
+		get_pid(pid); /* held by pidfile now */
 
 		retval = put_user(pidfd, args->pidfd);
 		if (retval)
@@ -2283,7 +2374,7 @@ static __latent_entropy struct task_struct *copy_process(
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */
-	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
+	if ((clone_flags & (CLONE_VM | CLONE_VFORK)) == CLONE_VM)
 		sas_ss_reset(p);
 
 	/*
@@ -2355,7 +2446,7 @@ static __latent_entropy struct task_struct *copy_process(
 	write_lock_irq(&tasklist_lock);
 
 	/* CLONE_PARENT re-uses the old parent */
-	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
+	if (clone_flags & (CLONE_PARENT | CLONE_THREAD)) {
 		p->real_parent = current->real_parent;
 		p->parent_exec_id = current->parent_exec_id;
 		if (clone_flags & CLONE_THREAD)
@@ -2417,8 +2508,9 @@ static __latent_entropy struct task_struct *copy_process(
 			 * tasklist_lock with adding child to the process tree
 			 * for propagate_has_child_subreaper optimization.
 			 */
-			p->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||
-							 p->real_parent->signal->is_child_subreaper;
+			p->signal->has_child_subreaper =
+				p->real_parent->signal->has_child_subreaper ||
+				p->real_parent->signal->is_child_subreaper;
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			attach_pid(p, PIDTYPE_TGID);
@@ -2532,7 +2624,7 @@ static inline void init_idle_pids(struct task_struct *idle)
 	}
 }
 
-struct task_struct * __init fork_idle(int cpu)
+struct task_struct *__init fork_idle(int cpu)
 {
 	struct task_struct *task;
 	struct kernel_clone_args args = {
@@ -2556,15 +2648,15 @@ struct task_struct * __init fork_idle(int cpu)
  */
 struct task_struct *create_io_thread(int (*fn)(void *), void *arg, int node)
 {
-	unsigned long flags = CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|
-				CLONE_IO;
+	unsigned long flags = CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
+			      CLONE_THREAD | CLONE_IO;
 	struct kernel_clone_args args = {
-		.flags		= ((lower_32_bits(flags) | CLONE_VM |
-				    CLONE_UNTRACED) & ~CSIGNAL),
-		.exit_signal	= (lower_32_bits(flags) & CSIGNAL),
-		.stack		= (unsigned long)fn,
-		.stack_size	= (unsigned long)arg,
-		.io_thread	= 1,
+		.flags = ((lower_32_bits(flags) | CLONE_VM | CLONE_UNTRACED) &
+			  ~CSIGNAL),
+		.exit_signal = (lower_32_bits(flags) & CSIGNAL),
+		.stack = (unsigned long)fn,
+		.stack_size = (unsigned long)arg,
+		.io_thread = 1,
 	};
 
 	return copy_process(NULL, 0, node, &args);
@@ -2664,11 +2756,11 @@ pid_t kernel_clone(struct kernel_clone_args *args)
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct kernel_clone_args args = {
-		.flags		= ((lower_32_bits(flags) | CLONE_VM |
-				    CLONE_UNTRACED) & ~CSIGNAL),
-		.exit_signal	= (lower_32_bits(flags) & CSIGNAL),
-		.stack		= (unsigned long)fn,
-		.stack_size	= (unsigned long)arg,
+		.flags = ((lower_32_bits(flags) | CLONE_VM | CLONE_UNTRACED) &
+			  ~CSIGNAL),
+		.exit_signal = (lower_32_bits(flags) & CSIGNAL),
+		.stack = (unsigned long)fn,
+		.stack_size = (unsigned long)arg,
 	};
 
 	return kernel_clone(&args);
@@ -2694,8 +2786,8 @@ SYSCALL_DEFINE0(fork)
 SYSCALL_DEFINE0(vfork)
 {
 	struct kernel_clone_args args = {
-		.flags		= CLONE_VFORK | CLONE_VM,
-		.exit_signal	= SIGCHLD,
+		.flags = CLONE_VFORK | CLONE_VM,
+		.exit_signal = SIGCHLD,
 	};
 
 	return kernel_clone(&args);
@@ -2705,35 +2797,30 @@ SYSCALL_DEFINE0(vfork)
 #ifdef __ARCH_WANT_SYS_CLONE
 #ifdef CONFIG_CLONE_BACKWARDS
 SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
-		 int __user *, parent_tidptr,
-		 unsigned long, tls,
-		 int __user *, child_tidptr)
+		int __user *, parent_tidptr, unsigned long, tls, int __user *,
+		child_tidptr)
 #elif defined(CONFIG_CLONE_BACKWARDS2)
 SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
-		 int __user *, parent_tidptr,
-		 int __user *, child_tidptr,
-		 unsigned long, tls)
-#elif defined(CONFIG_CLONE_BACKWARDS3)
-SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,
-		int, stack_size,
-		int __user *, parent_tidptr,
-		int __user *, child_tidptr,
+		int __user *, parent_tidptr, int __user *, child_tidptr,
 		unsigned long, tls)
+#elif defined(CONFIG_CLONE_BACKWARDS3)
+SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp, int,
+		stack_size, int __user *, parent_tidptr, int __user *,
+		child_tidptr, unsigned long, tls)
 #else
 SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
-		 int __user *, parent_tidptr,
-		 int __user *, child_tidptr,
-		 unsigned long, tls)
+		int __user *, parent_tidptr, int __user *, child_tidptr,
+		unsigned long, tls)
 #endif
 {
 	struct kernel_clone_args args = {
-		.flags		= (lower_32_bits(clone_flags) & ~CSIGNAL),
-		.pidfd		= parent_tidptr,
-		.child_tid	= child_tidptr,
-		.parent_tid	= parent_tidptr,
-		.exit_signal	= (lower_32_bits(clone_flags) & CSIGNAL),
-		.stack		= newsp,
-		.tls		= tls,
+		.flags = (lower_32_bits(clone_flags) & ~CSIGNAL),
+		.pidfd = parent_tidptr,
+		.child_tid = child_tidptr,
+		.parent_tid = parent_tidptr,
+		.exit_signal = (lower_32_bits(clone_flags) & CSIGNAL),
+		.stack = newsp,
+		.tls = tls,
 	};
 
 	return kernel_clone(&args);
@@ -2789,21 +2876,21 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		return -EINVAL;
 
 	*kargs = (struct kernel_clone_args){
-		.flags		= args.flags,
-		.pidfd		= u64_to_user_ptr(args.pidfd),
-		.child_tid	= u64_to_user_ptr(args.child_tid),
-		.parent_tid	= u64_to_user_ptr(args.parent_tid),
-		.exit_signal	= args.exit_signal,
-		.stack		= args.stack,
-		.stack_size	= args.stack_size,
-		.tls		= args.tls,
-		.set_tid_size	= args.set_tid_size,
-		.cgroup		= args.cgroup,
+		.flags = args.flags,
+		.pidfd = u64_to_user_ptr(args.pidfd),
+		.child_tid = u64_to_user_ptr(args.child_tid),
+		.parent_tid = u64_to_user_ptr(args.parent_tid),
+		.exit_signal = args.exit_signal,
+		.stack = args.stack,
+		.stack_size = args.stack_size,
+		.tls = args.tls,
+		.set_tid_size = args.set_tid_size,
+		.cgroup = args.cgroup,
 	};
 
 	if (args.set_tid &&
-		copy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),
-			(kargs->set_tid_size * sizeof(pid_t))))
+	    copy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),
+			   (kargs->set_tid_size * sizeof(pid_t))))
 		return -EFAULT;
 
 	kargs->set_tid = kset_tid;
@@ -2898,7 +2985,8 @@ SYSCALL_DEFINE2(clone3, struct clone_args __user *, uargs, size_t, size)
 }
 #endif
 
-void walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data)
+void walk_process_tree(struct task_struct *top, proc_visitor visitor,
+		       void *data)
 {
 	struct task_struct *leader, *parent, *child;
 	int res;
@@ -2906,8 +2994,8 @@ void walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data
 	read_lock(&tasklist_lock);
 	leader = top = top->group_leader;
 down:
-	for_each_thread(leader, parent) {
-		list_for_each_entry(child, &parent->children, sibling) {
+	for_each_thread (leader, parent) {
+		list_for_each_entry (child, &parent->children, sibling) {
 			res = visitor(child, data);
 			if (res) {
 				if (res < 0)
@@ -2915,8 +3003,7 @@ void walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data
 				leader = child;
 				goto down;
 			}
-up:
-			;
+		up:;
 		}
 	}
 
@@ -2946,6 +3033,22 @@ void __init mm_cache_init(void)
 {
 	unsigned int mm_size;
 
+	sighand_cachep =
+		kmem_cache_create("sighand_cache",
+				  sizeof(struct sighand_struct), 0,
+				  SLAB_HWCACHE_ALIGN | SLAB_PANIC |
+					  SLAB_TYPESAFE_BY_RCU | SLAB_ACCOUNT,
+				  sighand_ctor);
+	signal_cachep = kmem_cache_create(
+		"signal_cache", sizeof(struct signal_struct), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	files_cachep = kmem_cache_create(
+		"files_cache", sizeof(struct files_struct), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	fs_cachep = kmem_cache_create(
+		"fs_cache", sizeof(struct fs_struct), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
 	/*
 	 * The mm_cpumask is located at the end of mm_struct, and is
 	 * dynamically sized based on the maximum CPU number this system
@@ -2953,12 +3056,13 @@ void __init mm_cache_init(void)
 	 */
 	mm_size = MM_STRUCT_SIZE;
 
-	mm_cachep = kmem_cache_create_usercopy("mm_struct",
-			mm_size, ARCH_MIN_MMSTRUCT_ALIGN,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
-			OFFSET_OF_MM_SAVED_AUXV,
-			SIZE_OF_MM_SAVED_AUXV,
-			NULL);
+	mm_cachep = kmem_cache_create_usercopy(
+		"mm_struct", mm_size, ARCH_MIN_MMSTRUCT_ALIGN,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT,
+		OFFSET_OF_MM_SAVED_AUXV, SIZE_OF_MM_SAVED_AUXV, NULL);
+	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC | SLAB_ACCOUNT);
+	mmap_init();
+	nsproxy_cache_init();
 }
 
 void __init proc_caches_init(void)
@@ -2990,11 +3094,11 @@ void __init proc_caches_init(void)
  */
 static int check_unshare_flags(unsigned long unshare_flags)
 {
-	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
-				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
-				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+	if (unshare_flags &
+	    ~(CLONE_THREAD | CLONE_FS | CLONE_NEWNS | CLONE_SIGHAND | CLONE_VM |
+	      CLONE_FILES | CLONE_SYSVSEM | CLONE_NEWUTS | CLONE_NEWIPC |
+	      CLONE_NEWNET | CLONE_NEWUSER | CLONE_NEWPID | CLONE_NEWCGROUP |
+	      CLONE_NEWTIME))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -3105,7 +3209,7 @@ int ksys_unshare(unsigned long unshare_flags)
 	 * to a new ipc namespace, the semaphore arrays from the old
 	 * namespace are unreachable.
 	 */
-	if (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))
+	if (unshare_flags & (CLONE_NEWIPC | CLONE_SYSVSEM))
 		do_sysvsem = 1;
 	err = unshare_fs(unshare_flags, &new_fs);
 	if (err)
@@ -3116,8 +3220,8 @@ int ksys_unshare(unsigned long unshare_flags)
 	err = unshare_userns(unshare_flags, &new_cred);
 	if (err)
 		goto bad_unshare_cleanup_fd;
-	err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
-					 new_cred, new_fs);
+	err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy, new_cred,
+					 new_fs);
 	if (err)
 		goto bad_unshare_cleanup_cred;
 
@@ -3211,8 +3315,8 @@ int unshare_files(struct files_struct **displaced)
 	return 0;
 }
 
-int sysctl_max_threads(struct ctl_table *table, int write,
-		       void *buffer, size_t *lenp, loff_t *ppos)
+int sysctl_max_threads(struct ctl_table *table, int write, void *buffer,
+		       size_t *lenp, loff_t *ppos)
 {
 	struct ctl_table t;
 	int ret;
diff --git a/kernel/groups.c b/kernel/groups.c
index fe7e6385530e..d8b69a5bbe38 100644
--- a/kernel/groups.c
+++ b/kernel/groups.c
@@ -11,6 +11,9 @@
 #include <linux/user_namespace.h>
 #include <linux/vmalloc.h>
 #include <linux/uaccess.h>
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
 
 struct group_info *groups_alloc(int gidsetsize)
 {
@@ -124,7 +127,11 @@ void set_groups(struct cred *new, struct group_info *group_info)
 {
 	put_group_info(new->group_info);
 	get_group_info(group_info);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_group_info(new,group_info);
+	#else
 	new->group_info = group_info;
+	#endif
 }
 
 EXPORT_SYMBOL(set_groups);
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 508fe5278285..1620a7f6a3a5 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -30,6 +30,10 @@
 #include <linux/sched/isolation.h>
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#endif
+
 
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
@@ -1325,6 +1329,10 @@ void kthread_use_mm(struct mm_struct *mm)
 		tsk->active_mm = mm;
 	}
 	tsk->mm = mm;
+	#ifdef CONFIG_IEE
+	iee_set_token_mm(tsk, mm);
+	iee_set_token_pgd(tsk, mm->pgd);
+	#endif
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
 	task_unlock(tsk);
@@ -1356,6 +1364,10 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	sync_mm_rss(mm);
 	local_irq_disable();
 	tsk->mm = NULL;
+	#ifdef CONFIG_IEE
+	iee_set_token_mm(tsk, mm);
+	iee_set_token_pgd(tsk, NULL);
+	#endif
 	/* active_mm is still 'mm' */
 	enter_lazy_tlb(mm, tsk);
 	local_irq_enable();
diff --git a/kernel/module.c b/kernel/module.c
index e978debc93df..a83df5309963 100644
--- a/kernel/module.c
+++ b/kernel/module.c
@@ -66,6 +66,10 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
+#ifdef CONFIG_KOI
+#include "asm/koi.h"
+#endif
+
 #ifndef ARCH_SHF_SMALL
 #define ARCH_SHF_SMALL 0
 #endif
@@ -3977,6 +3981,16 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	struct module *mod;
 	long err = 0;
 	char *after_dashes;
+#ifdef CONFIG_KOI
+
+    struct mm_struct *ko_mm;
+	pgd_t *ko_pg_dir;
+	struct shared_variable_descriptor *desc;
+	unsigned int ndx, i, j;
+	unsigned int num_desc;
+	unsigned int *get_val_id, *from_id_get_ad;
+    unsigned long addr;
+#endif
 
 	/*
 	 * Do the signature check (if any) first. All that
@@ -4116,7 +4130,54 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
+#ifdef CONFIG_KOI
+	koi_create_pagetable(mod);
+	/*
+	 * find the shared_vars_section and get the shared var list
+	 */
 
+	ndx = find_sec(info, ".shared_vars_section");
+	if (ndx){
+		struct koi_mem_hash_node *target = NULL;
+		rcu_read_lock();
+		hash_for_each_possible_rcu (koi_mem_htbl, target, node,
+						(unsigned long)mod) {
+			if (target->mod == mod) {
+				break;
+			}
+		}
+		rcu_read_unlock();
+
+		if (target == NULL) {
+			printk("mem node for module: %s not found\n", mod->name);
+			return 0;
+		}
+
+		ko_mm = target->ko_mm;
+		ko_pg_dir = target->pgdp;
+
+		desc = (struct shared_variable_descriptor *)info->sechdrs[ndx].sh_addr;
+
+		num_desc = info->sechdrs[ndx].sh_size / sizeof(struct shared_variable_descriptor);
+		get_val_id = kmalloc(DRIVER_ISOLATION_MAX_VAL, GFP_KERNEL);
+		from_id_get_ad = kmalloc(DRIVER_ISOLATION_MAX_VAL, GFP_KERNEL);
+        
+        for (j = 0; j < num_desc; j++) {
+            unsigned int desc_id = desc[j].id;
+            get_val_id[j] = desc_id;
+			from_id_get_ad[desc_id] = j;
+        }
+		
+		for (i = 0; i < num_desc; i++) {
+			if (desc[i].type == 0) {
+				addr = kallsyms_lookup_name(desc[i].name);
+				koi_copy_pagetable(ko_mm, ko_pg_dir, addr & PAGE_MASK, (addr + desc[i].size + PAGE_SIZE) & PAGE_MASK);
+            }
+		}
+		kfree(get_val_id);
+		kfree(from_id_get_ad);
+	}
+#endif
 	/* Module is ready to execute: parsing args may do that. */
 	after_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,
 				  -32768, 32767, mod,
diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index e4163042c4d6..2a195dee57ce 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -16,6 +16,10 @@
 #include <linux/kthread.h>
 #include <linux/smpboot.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#endif
+
 #include "smpboot.h"
 
 #ifdef CONFIG_SMP
@@ -47,6 +51,7 @@ void __init idle_thread_set_boot_cpu(void)
  *
  * Creates the thread if it does not exist.
  */
+
 static inline void idle_init(unsigned int cpu)
 {
 	struct task_struct *tsk = per_cpu(idle_threads, cpu);
@@ -57,6 +62,11 @@ static inline void idle_init(unsigned int cpu)
 			pr_err("SMP: fork_idle() failed for CPU %u\n", cpu);
 		else
 			per_cpu(idle_threads, cpu) = tsk;
+		#ifdef CONFIG_IEE
+		// Set the secondary __entry_task.
+		*(struct task_struct **)SHIFT_PERCPU_PTR(__entry_task,__per_cpu_offset[cpu]) = tsk;
+		iee_set_logical_mem_ro((unsigned long)SHIFT_PERCPU_PTR(__entry_task,__per_cpu_offset[cpu]));
+		#endif
 	}
 }
 
diff --git a/kernel/stacktrace.c b/kernel/stacktrace.c
index 9c625257023d..f746a271138b 100644
--- a/kernel/stacktrace.c
+++ b/kernel/stacktrace.c
@@ -15,6 +15,10 @@
 #include <linux/stacktrace.h>
 #include <linux/interrupt.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#endif
+
 /**
  * stack_trace_print - Print the entries in the stack trace
  * @entries:	Pointer to storage array
@@ -94,6 +98,23 @@ static bool stack_trace_consume_entry(void *cookie, unsigned long addr)
 	return c->len < c->size;
 }
 
+#ifdef CONFIG_IEE
+static bool stack_trace_consume_entry_iee(void *cookie, unsigned long addr)
+{
+	struct stacktrace_cookie *c = cookie;
+
+	if (c->len >= c->size)
+		return false;
+
+	if (c->skip > 0) {
+		c->skip--;
+		return true;
+	}
+	iee_write_in_byte(&(c->store[c->len++]), addr, 8);
+	return c->len < c->size;
+}
+#endif
+
 static bool stack_trace_consume_entry_nosched(void *cookie, unsigned long addr)
 {
 	if (in_sched_functions(addr))
@@ -124,6 +145,23 @@ unsigned int stack_trace_save(unsigned long *store, unsigned int size,
 }
 EXPORT_SYMBOL_GPL(stack_trace_save);
 
+#ifdef CONFIG_IEE
+unsigned int stack_trace_save_iee(unsigned long *store, unsigned int size,
+			      unsigned int skipnr)
+{
+	stack_trace_consume_fn consume_entry = stack_trace_consume_entry_iee;
+	struct stacktrace_cookie c = {
+		.store	= store,
+		.size	= size,
+		.skip	= skipnr + 1,
+	};
+
+	arch_stack_walk(consume_entry, &c, current, NULL);
+	return c.len;
+}
+EXPORT_SYMBOL_GPL(stack_trace_save_iee);
+#endif
+
 /**
  * stack_trace_save_tsk - Save a task stack trace into a storage array
  * @task:	The task to examine
diff --git a/kernel/sys.c b/kernel/sys.c
index 566ba957014d..1c6220c93ef7 100644
--- a/kernel/sys.c
+++ b/kernel/sys.c
@@ -73,6 +73,10 @@
 #include <asm/io.h>
 #include <asm/unistd.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #include "uid16.h"
 
 #ifndef SET_UNALIGN_CTL
@@ -381,7 +385,11 @@ long __sys_setregid(gid_t rgid, gid_t egid)
 		if (gid_eq(old->gid, krgid) ||
 		    gid_eq(old->egid, krgid) ||
 		    ns_capable_setid(old->user_ns, CAP_SETGID))
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_gid(new,krgid);
+			#else
 			new->gid = krgid;
+			#endif
 		else
 			goto error;
 	}
@@ -390,15 +398,27 @@ long __sys_setregid(gid_t rgid, gid_t egid)
 		    gid_eq(old->egid, kegid) ||
 		    gid_eq(old->sgid, kegid) ||
 		    ns_capable_setid(old->user_ns, CAP_SETGID))
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_egid(new,kegid);
+			#else
 			new->egid = kegid;
+			#endif
 		else
 			goto error;
 	}
 
 	if (rgid != (gid_t) -1 ||
 	    (egid != (gid_t) -1 && !gid_eq(kegid, old->gid)))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_sgid(new,new->egid);
+		#else
 		new->sgid = new->egid;
+		#endif
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsgid(new,new->egid);
+	#else
 	new->fsgid = new->egid;
+	#endif
 
 	retval = security_task_fix_setgid(new, old, LSM_SETID_RE);
 	if (retval < 0)
@@ -440,9 +460,25 @@ long __sys_setgid(gid_t gid)
 
 	retval = -EPERM;
 	if (ns_capable_setid(old->user_ns, CAP_SETGID))
+		#ifdef CONFIG_CREDP
+	{
+		iee_set_cred_fsgid(new,kgid);
+		iee_set_cred_sgid(new,kgid);
+		iee_set_cred_egid(new,kgid);
+		iee_set_cred_gid(new,kgid);
+	}
+		#else
 		new->gid = new->egid = new->sgid = new->fsgid = kgid;
+		#endif
 	else if (gid_eq(kgid, old->gid) || gid_eq(kgid, old->sgid))
+		#ifdef CONFIG_CREDP
+	{
+		iee_set_cred_fsgid(new,kgid);
+		iee_set_cred_egid(new,kgid);
+	}
+		#else
 		new->egid = new->fsgid = kgid;
+		#endif
 	else
 		goto error;
 
@@ -487,7 +523,11 @@ static int set_user(struct cred *new)
 		current->flags &= ~PF_NPROC_EXCEEDED;
 
 	free_uid(new->user);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_user(new,new_user);
+	#else
 	new->user = new_user;
+	#endif
 	return 0;
 }
 
@@ -529,7 +569,11 @@ long __sys_setreuid(uid_t ruid, uid_t euid)
 
 	retval = -EPERM;
 	if (ruid != (uid_t) -1) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_uid(new,kruid);
+		#else
 		new->uid = kruid;
+		#endif
 		if (!uid_eq(old->uid, kruid) &&
 		    !uid_eq(old->euid, kruid) &&
 		    !ns_capable_setid(old->user_ns, CAP_SETUID))
@@ -537,7 +581,11 @@ long __sys_setreuid(uid_t ruid, uid_t euid)
 	}
 
 	if (euid != (uid_t) -1) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_euid(new,keuid);
+		#else
 		new->euid = keuid;
+		#endif
 		if (!uid_eq(old->uid, keuid) &&
 		    !uid_eq(old->euid, keuid) &&
 		    !uid_eq(old->suid, keuid) &&
@@ -552,8 +600,16 @@ long __sys_setreuid(uid_t ruid, uid_t euid)
 	}
 	if (ruid != (uid_t) -1 ||
 	    (euid != (uid_t) -1 && !uid_eq(keuid, old->uid)))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_suid(new,new->euid);
+		#else
 		new->suid = new->euid;
+		#endif
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,new->euid);
+	#else
 	new->fsuid = new->euid;
+	#endif
 
 	retval = security_task_fix_setuid(new, old, LSM_SETID_RE);
 	if (retval < 0)
@@ -601,7 +657,12 @@ long __sys_setuid(uid_t uid)
 
 	retval = -EPERM;
 	if (ns_capable_setid(old->user_ns, CAP_SETUID)) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_uid(new,kuid);
+		iee_set_cred_suid(new,kuid);
+		#else
 		new->suid = new->uid = kuid;
+		#endif
 		if (!uid_eq(kuid, old->uid)) {
 			retval = set_user(new);
 			if (retval < 0)
@@ -611,7 +672,12 @@ long __sys_setuid(uid_t uid)
 		goto error;
 	}
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_euid(new,kuid);
+	iee_set_cred_fsuid(new,kuid);
+	#else
 	new->fsuid = new->euid = kuid;
+	#endif
 
 	retval = security_task_fix_setuid(new, old, LSM_SETID_ID);
 	if (retval < 0)
@@ -680,7 +746,11 @@ long __sys_setresuid(uid_t ruid, uid_t euid, uid_t suid)
 		return -ENOMEM;
 
 	if (ruid != (uid_t) -1) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_uid(new,kruid);
+		#else
 		new->uid = kruid;
+		#endif
 		if (!uid_eq(kruid, old->uid)) {
 			retval = set_user(new);
 			if (retval < 0)
@@ -688,10 +758,22 @@ long __sys_setresuid(uid_t ruid, uid_t euid, uid_t suid)
 		}
 	}
 	if (euid != (uid_t) -1)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_euid(new,keuid);
+		#else
 		new->euid = keuid;
+		#endif
 	if (suid != (uid_t) -1)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_suid(new,ksuid);
+		#else
 		new->suid = ksuid;
+		#endif
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,new->euid);
+	#else
 	new->fsuid = new->euid;
+	#endif
 
 	retval = security_task_fix_setuid(new, old, LSM_SETID_RES);
 	if (retval < 0)
@@ -775,12 +857,29 @@ long __sys_setresgid(gid_t rgid, gid_t egid, gid_t sgid)
 		return -ENOMEM;
 
 	if (rgid != (gid_t) -1)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_gid(new,krgid);
+		#else
 		new->gid = krgid;
+		#endif
 	if (egid != (gid_t) -1)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_egid(new,kegid);
+		#else
 		new->egid = kegid;
+		#endif
 	if (sgid != (gid_t) -1)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_sgid(new,ksgid);
+		#else
 		new->sgid = ksgid;
+		#endif
+
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsgid(new,new->egid);
+	#else
 	new->fsgid = new->egid;
+	#endif
 
 	retval = security_task_fix_setgid(new, old, LSM_SETID_RES);
 	if (retval < 0)
@@ -847,7 +946,11 @@ long __sys_setfsuid(uid_t uid)
 	    uid_eq(kuid, old->suid) || uid_eq(kuid, old->fsuid) ||
 	    ns_capable_setid(old->user_ns, CAP_SETUID)) {
 		if (!uid_eq(kuid, old->fsuid)) {
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_fsuid(new,kuid);
+			#else
 			new->fsuid = kuid;
+			#endif
 			if (security_task_fix_setuid(new, old, LSM_SETID_FS) == 0)
 				goto change_okay;
 		}
@@ -891,7 +994,11 @@ long __sys_setfsgid(gid_t gid)
 	    gid_eq(kgid, old->sgid) || gid_eq(kgid, old->fsgid) ||
 	    ns_capable_setid(old->user_ns, CAP_SETGID)) {
 		if (!gid_eq(kgid, old->fsgid)) {
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_fsgid(new,kgid);
+			#else
 			new->fsgid = kgid;
+			#endif
 			if (security_task_fix_setgid(new,old,LSM_SETID_FS) == 0)
 				goto change_okay;
 		}
diff --git a/kernel/umh.c b/kernel/umh.c
index 3f646613a9d3..082800cfda46 100644
--- a/kernel/umh.c
+++ b/kernel/umh.c
@@ -30,6 +30,10 @@
 
 #include <trace/events/module.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 #define CAP_BSET	(void *)1
 #define CAP_PI		(void *)2
 
@@ -92,9 +96,15 @@ static int call_usermodehelper_exec_async(void *data)
 		goto out;
 
 	spin_lock(&umh_sysctl_lock);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_cap_bset(new,cap_intersect(usermodehelper_bset, new->cap_bset));
+	iee_set_cred_cap_inheritable(new,cap_intersect(usermodehelper_inheritable,
+					     new->cap_inheritable));
+	#else
 	new->cap_bset = cap_intersect(usermodehelper_bset, new->cap_bset);
 	new->cap_inheritable = cap_intersect(usermodehelper_inheritable,
 					     new->cap_inheritable);
+	#endif
 	spin_unlock(&umh_sysctl_lock);
 
 	if (sub_info->init) {
diff --git a/kernel/user_namespace.c b/kernel/user_namespace.c
index 2c15bf6680c3..9ab85b2f9af0 100644
--- a/kernel/user_namespace.c
+++ b/kernel/user_namespace.c
@@ -21,6 +21,10 @@
 #include <linux/bsearch.h>
 #include <linux/sort.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 static struct kmem_cache *user_ns_cachep __read_mostly;
 static DEFINE_MUTEX(userns_state_mutex);
 
@@ -44,6 +48,19 @@ static void set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)
 	/* Start with the same capabilities as init but useless for doing
 	 * anything as the capabilities are bound to the new user namespace.
 	 */
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_securebits(cred,SECUREBITS_DEFAULT);
+	iee_set_cred_cap_inheritable(cred,CAP_EMPTY_SET);
+	iee_set_cred_cap_permitted(cred,CAP_FULL_SET);
+	iee_set_cred_cap_effective(cred,CAP_FULL_SET);
+	iee_set_cred_cap_ambient(cred,CAP_EMPTY_SET);
+	iee_set_cred_cap_bset(cred,CAP_FULL_SET);
+#ifdef CONFIG_KEYS
+	key_put(cred->request_key_auth);
+	iee_set_cred_request_key_auth(cred,NULL);
+#endif
+	iee_set_cred_user_ns(cred,user_ns);
+	#else
 	cred->securebits = SECUREBITS_DEFAULT;
 	cred->cap_inheritable = CAP_EMPTY_SET;
 	cred->cap_permitted = CAP_FULL_SET;
@@ -56,6 +73,7 @@ static void set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)
 #endif
 	/* tgcred will be cleared in our caller bc CLONE_THREAD won't be set */
 	cred->user_ns = user_ns;
+	#endif
 }
 
 /*
diff --git a/mm/Kconfig b/mm/Kconfig
index 70c85533aada..2b725ccd9fad 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -145,6 +145,18 @@ config NUMA_KEEP_MEMINFO
 config MEMORY_ISOLATION
 	bool
 
+# Config for kernel module isolation
+config KOI 
+    depends on ARM64
+    depends on ARM64_VA_BITS_48
+	depends on ARM64_4K_PAGES
+    def_bool n
+
+# Configs for pgtable isolation
+config PTP
+	depends on IEE
+	def_bool y
+
 config COHERENT_DEVICE
 	bool "coherent device memory"
 	def_bool n
diff --git a/mm/damon/ops-common.c b/mm/damon/ops-common.c
index 75409601f934..e375200c93e6 100644
--- a/mm/damon/ops-common.c
+++ b/mm/damon/ops-common.c
@@ -43,7 +43,11 @@ void damon_ptep_mkold(pte_t *pte, struct mm_struct *mm, unsigned long addr)
 
 	if (pte_young(*pte)) {
 		referenced = true;
+		#ifdef CONFIG_PTP
+		set_pte(pte, pte_mkold(*pte));
+		#else
 		*pte = pte_mkold(*pte);
+		#endif
 	}
 
 #ifdef CONFIG_MMU_NOTIFIER
@@ -69,7 +73,11 @@ void damon_pmdp_mkold(pmd_t *pmd, struct mm_struct *mm, unsigned long addr)
 
 	if (pmd_young(*pmd)) {
 		referenced = true;
+		#ifdef CONFIG_PTP
+		set_pmd(pmd, pmd_mkold(*pmd));
+		#else
 		*pmd = pmd_mkold(*pmd);
+		#endif
 	}
 
 #ifdef CONFIG_MMU_NOTIFIER
diff --git a/mm/debug_vm_pgtable.c b/mm/debug_vm_pgtable.c
index 11d3b46ba187..4fea5dc21066 100644
--- a/mm/debug_vm_pgtable.c
+++ b/mm/debug_vm_pgtable.c
@@ -263,7 +263,11 @@ static void __init pmd_huge_tests(pmd_t *pmdp, unsigned long pfn, pgprot_t prot)
 	 * X86 defined pmd_set_huge() verifies that the given
 	 * PMD is not a populated non-leaf entry.
 	 */
+	#ifdef CONFIG_PTP
+	set_pmd(pmdp, __pmd(0));
+	#else
 	WRITE_ONCE(*pmdp, __pmd(0));
+	#endif
 	WARN_ON(!pmd_set_huge(pmdp, __pfn_to_phys(pfn), prot));
 	WARN_ON(!pmd_clear_huge(pmdp));
 	pmd = READ_ONCE(*pmdp);
@@ -411,7 +415,11 @@ static void __init pud_huge_tests(pud_t *pudp, unsigned long pfn, pgprot_t prot)
 	 * X86 defined pud_set_huge() verifies that the given
 	 * PUD is not a populated non-leaf entry.
 	 */
+	#ifdef CONFIG_PTP
+	set_pud(pudp, __pud(0));
+	#else
 	WRITE_ONCE(*pudp, __pud(0));
+	#endif
 	WARN_ON(!pud_set_huge(pudp, __pfn_to_phys(pfn), prot));
 	WARN_ON(!pud_clear_huge(pudp));
 	pud = READ_ONCE(*pudp);
@@ -488,7 +496,11 @@ static void __init pud_clear_tests(struct mm_struct *mm, pud_t *pudp)
 
 	pr_debug("Validating PUD clear\n");
 	pud = __pud(pud_val(pud) | RANDOM_ORVALUE);
+	#ifdef CONFIG_PTP
+	set_pud(pudp, pud);
+	#else
 	WRITE_ONCE(*pudp, pud);
+	#endif
 	pud_clear(pudp);
 	pud = READ_ONCE(*pudp);
 	WARN_ON(!pud_none(pud));
@@ -529,7 +541,11 @@ static void __init p4d_clear_tests(struct mm_struct *mm, p4d_t *p4dp)
 
 	pr_debug("Validating P4D clear\n");
 	p4d = __p4d(p4d_val(p4d) | RANDOM_ORVALUE);
+	#ifdef CONFIG_PTP
+	set_p4d(p4dp, p4d);
+	#else
 	WRITE_ONCE(*p4dp, p4d);
+	#endif
 	p4d_clear(p4dp);
 	p4d = READ_ONCE(*p4dp);
 	WARN_ON(!p4d_none(p4d));
@@ -564,7 +580,11 @@ static void __init pgd_clear_tests(struct mm_struct *mm, pgd_t *pgdp)
 
 	pr_debug("Validating PGD clear\n");
 	pgd = __pgd(pgd_val(pgd) | RANDOM_ORVALUE);
+	#ifdef CONFIG_PTP
+	set_pgd(pgdp, pgd);
+	#else
 	WRITE_ONCE(*pgdp, pgd);
+	#endif
 	pgd_clear(pgdp);
 	pgd = READ_ONCE(*pgdp);
 	WARN_ON(!pgd_none(pgd));
@@ -625,7 +645,11 @@ static void __init pmd_clear_tests(struct mm_struct *mm, pmd_t *pmdp)
 
 	pr_debug("Validating PMD clear\n");
 	pmd = __pmd(pmd_val(pmd) | RANDOM_ORVALUE);
+	#ifdef CONFIG_PTP
+	set_pmd(pmdp, pmd);
+	#else
 	WRITE_ONCE(*pmdp, pmd);
+	#endif
 	pmd_clear(pmdp);
 	pmd = READ_ONCE(*pmdp);
 	WARN_ON(!pmd_none(pmd));
diff --git a/mm/early_ioremap.c b/mm/early_ioremap.c
index a0018ad1a1f6..325e9b202139 100644
--- a/mm/early_ioremap.c
+++ b/mm/early_ioremap.c
@@ -153,7 +153,11 @@ __early_ioremap(resource_size_t phys_addr, unsigned long size, pgprot_t prot)
 		if (after_paging_init)
 			__late_set_fixmap(idx, phys_addr, prot);
 		else
+			#ifdef CONFIG_PTP
+			__iee_set_fixmap_pre_init(idx, phys_addr, prot);
+			#else
 			__early_set_fixmap(idx, phys_addr, prot);
+			#endif
 		phys_addr += PAGE_SIZE;
 		--idx;
 		--nrpages;
@@ -205,13 +209,66 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)
 		if (after_paging_init)
 			__late_clear_fixmap(idx);
 		else
+			#ifdef CONFIG_PTP
+			__iee_set_fixmap_pre_init(idx, 0, FIXMAP_PAGE_CLEAR);
+			#else
 			__early_set_fixmap(idx, 0, FIXMAP_PAGE_CLEAR);
+			#endif
 		--idx;
 		--nrpages;
 	}
 	prev_map[slot] = NULL;
 }
 
+#ifdef CONFIG_PTP
+void __init early_iounmap_after_init(void __iomem *addr, unsigned long size)
+{
+	unsigned long virt_addr;
+	unsigned long offset;
+	unsigned int nrpages;
+	enum fixed_addresses idx;
+	int i, slot;
+
+	slot = -1;
+	for (i = 0; i < FIX_BTMAPS_SLOTS; i++) {
+		if (prev_map[i] == addr) {
+			slot = i;
+			break;
+		}
+	}
+
+	if (WARN(slot < 0, "early_iounmap(%p, %08lx) not found slot\n",
+		 addr, size))
+		return;
+
+	if (WARN(prev_size[slot] != size,
+		 "early_iounmap(%p, %08lx) [%d] size not consistent %08lx\n",
+		 addr, size, slot, prev_size[slot]))
+		return;
+
+	WARN(early_ioremap_debug, "early_iounmap(%p, %08lx) [%d]\n",
+	     addr, size, slot);
+
+	virt_addr = (unsigned long)addr;
+	if (WARN_ON(virt_addr < fix_to_virt(FIX_BTMAP_BEGIN)))
+		return;
+
+	offset = offset_in_page(virt_addr);
+	nrpages = PAGE_ALIGN(offset + size) >> PAGE_SHIFT;
+
+	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
+	while (nrpages > 0) {
+		if (after_paging_init)
+			__late_clear_fixmap(idx);
+		else
+			__early_set_fixmap(idx, 0, FIXMAP_PAGE_CLEAR);
+		--idx;
+		--nrpages;
+	}
+	prev_map[slot] = NULL;
+}
+#endif
+
 /* Remap an IO device */
 void __init __iomem *
 early_ioremap(resource_size_t phys_addr, unsigned long size)
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index be00c9886b2a..33c451c03975 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -35,6 +35,10 @@
 #include <linux/page_owner.h>
 #include <linux/dynamic_hugetlb.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
 #include "internal.h"
@@ -2013,6 +2017,10 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 	pgtable_t pgtable;
 	pmd_t _pmd, old_pmd;
 	int i;
+	#ifdef CONFIG_PTP
+	pte_t *pte;
+	unsigned long iee_addr;
+	#endif
 
 	/*
 	 * Leave pmd empty until pte is filled note that it is fine to delay
@@ -2025,7 +2033,14 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 	old_pmd = pmdp_huge_clear_flush(vma, haddr, pmd);
 
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
-	pmd_populate(mm, &_pmd, pgtable);
+	#ifdef CONFIG_PTP
+	pte = (pte_t *)page_address(pgtable);
+	iee_addr = __phys_to_iee(__pa(pte));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)pte);
+	#endif
+	//pmd_populate(mm, &_pmd, pgtable);
+	_pmd = __pmd(__phys_to_pmd_val(page_to_phys(pgtable)) | PMD_TYPE_TABLE);
 
 	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 		pte_t *pte, entry;
@@ -2052,6 +2067,10 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	bool young, write, soft_dirty, pmd_migration = false, uffd_wp = false;
 	unsigned long addr;
 	int i;
+	#ifdef CONFIG_PTP
+	pte_t *pte;
+	unsigned long iee_addr;
+	#endif
 
 	VM_BUG_ON(haddr & ~HPAGE_PMD_MASK);
 	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
@@ -2152,7 +2171,14 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	 * This's critical for some architectures (Power).
 	 */
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
-	pmd_populate(mm, &_pmd, pgtable);
+	#ifdef CONFIG_PTP
+	pte = (pte_t *)page_to_virt(pgtable);
+	iee_addr = __phys_to_iee(__pa(pte));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)pte);
+	#endif
+	//pmd_populate(mm, &_pmd, pgtable);
+	_pmd = __pmd(__phys_to_pmd_val(page_to_phys(pgtable)) | PMD_TYPE_TABLE);
 
 	for (i = 0, addr = haddr; i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE) {
 		pte_t entry, *pte;
diff --git a/mm/init-mm.c b/mm/init-mm.c
index 2dfed38be177..d721f25f3357 100644
--- a/mm/init-mm.c
+++ b/mm/init-mm.c
@@ -44,3 +44,20 @@ struct mm_struct init_mm = {
 #endif
 	INIT_MM_CONTEXT(init_mm)
 };
+
+#ifdef CONFIG_KOI
+/*
+ * This is used to init ko_mm when creating pgtable for a ko to be isolated
+ * the ko_mm belongs to a specific ko, pgdp is allocated by koi_pgd_alloc
+ */
+void init_ko_mm(struct mm_struct *ko_mm, pgd_t *pgdp) {
+	ko_mm->mm_rb = RB_ROOT;
+	ko_mm->pgd = pgdp;
+	ko_mm->mm_users = (atomic_t)ATOMIC_INIT(2);
+	ko_mm->mm_count = (atomic_t)ATOMIC_INIT(1);
+	ko_mm->mmap_lock = (struct rw_semaphore)__RWSEM_INITIALIZER(ko_mm->mmap_lock);
+	ko_mm->page_table_lock = __SPIN_LOCK_UNLOCKED(ko_mm.page_table_lock);
+	ko_mm->arg_lock = __SPIN_LOCK_UNLOCKED(ko_mm->arg_lock);
+	ko_mm->mmlist = (struct list_head)LIST_HEAD_INIT(ko_mm->mmlist);		
+}
+#endif
diff --git a/mm/memory.c b/mm/memory.c
index aa780ac0e114..b8966706f1ed 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -74,7 +74,10 @@
 #include <linux/ptrace.h>
 #include <linux/vmalloc.h>
 #include <linux/userswap.h>
-#include <linux/pbha.h>
+
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
 
 #include <trace/events/kmem.h>
 
@@ -448,6 +451,12 @@ int __pte_alloc(struct mm_struct *mm, pmd_t *pmd)
 
 	ptl = pmd_lock(mm, pmd);
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
+		#ifdef CONFIG_PTP
+		pte_t *pte = (pte_t *)page_address(new);
+		unsigned long iee_addr = __phys_to_iee(__pa(pte));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pte);
+		#endif
 		mm_inc_nr_ptes(mm);
 		pmd_populate(mm, pmd, new);
 		new = NULL;
@@ -468,6 +477,11 @@ int __pte_alloc_kernel(pmd_t *pmd)
 
 	spin_lock(&init_mm.page_table_lock);
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
+		#ifdef CONFIG_PTP
+		unsigned long iee_addr = __phys_to_iee(__pa(new));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)new);
+		#endif
 		pmd_populate_kernel(&init_mm, pmd, new);
 		new = NULL;
 	}
@@ -3858,6 +3872,10 @@ static int pmd_devmap_trans_unstable(pmd_t *pmd)
 static vm_fault_t pte_alloc_one_map(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
+	#ifdef CONFIG_PTP
+	pte_t *pte;
+	unsigned long iee_addr;
+	#endif
 
 	if (!pmd_none(*vmf->pmd))
 		goto map_pte;
@@ -3869,6 +3887,12 @@ static vm_fault_t pte_alloc_one_map(struct vm_fault *vmf)
 		}
 
 		mm_inc_nr_ptes(vma->vm_mm);
+		#ifdef CONFIG_PTP
+		pte = (pte_t *)page_address(vmf->prealloc_pte);
+		iee_addr = __phys_to_iee(__pa(pte));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)pte);
+		#endif
 		pmd_populate(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);
 		spin_unlock(vmf->ptl);
 		vmf->prealloc_pte = NULL;
@@ -4884,6 +4908,11 @@ int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)
 
 	spin_lock(&mm->page_table_lock);
 	if (!p4d_present(*p4d)) {
+		#ifdef CONFIG_PTP
+		unsigned long iee_addr = __phys_to_iee(__pa(new));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)new);
+		#endif
 		mm_inc_nr_puds(mm);
 		p4d_populate(mm, p4d, new);
 	} else	/* Another has populated it */
@@ -4909,6 +4938,11 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 
 	ptl = pud_lock(mm, pud);
 	if (!pud_present(*pud)) {
+		#ifdef CONFIG_PTP
+		unsigned long iee_addr = __phys_to_iee(__pa(new));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)new);
+		#endif
 		mm_inc_nr_pmds(mm);
 		pud_populate(mm, pud, new);
 	} else	/* Another has populated it */
diff --git a/mm/slub.c b/mm/slub.c
index ec1c3a376d36..3a6302ddcdd6 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -37,10 +37,21 @@
 #include <linux/memcontrol.h>
 #include <linux/random.h>
 
+#ifdef CONFIG_IEE
+#include <linux/iee-func.h>
+#include <asm/iee-access.h>
+#endif
+
 #include <trace/events/kmem.h>
+#include <asm/iee-slab.h>
 
 #include "internal.h"
 
+#ifdef CONFIG_KOI
+extern s64 koi_offset;
+extern int koi_add_page_mapping(unsigned long dst, unsigned long src);
+extern void koi_remove_page_mapping(unsigned long addr);
+#endif
 /*
  * Lock order:
  *   1. slab_mutex (Global Mutex)
@@ -219,15 +230,6 @@ static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
  * Tracking user of a slab.
  */
 #define TRACK_ADDRS_COUNT 16
-struct track {
-	unsigned long addr;	/* Called from address */
-#ifdef CONFIG_STACKTRACE
-	unsigned long addrs[TRACK_ADDRS_COUNT];	/* Called from address */
-#endif
-	int cpu;		/* Was running on cpu */
-	int pid;		/* Pid context */
-	unsigned long when;	/* When did the operation occur */
-};
 
 enum track_item { TRACK_ALLOC, TRACK_FREE };
 
@@ -328,7 +330,14 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 	BUG_ON(object == fp); /* naive detection of double free or corruption */
 #endif
 
+	#ifdef CONFIG_IEE
+	if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+		iee_set_freeptr((void **)freeptr_addr, freelist_ptr(s, fp, freeptr_addr));
+	else
+		*(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
+	#else
 	*(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);
+	#endif
 }
 
 /* Loop over all objects in a slab */
@@ -601,24 +610,73 @@ static void set_track(struct kmem_cache *s, void *object,
 			enum track_item alloc, unsigned long addr)
 {
 	struct track *p = get_track(s, object, alloc);
+	#ifdef CONFIG_IEE
+	struct track tmp;
+	#endif
 
 	if (addr) {
 #ifdef CONFIG_STACKTRACE
 		unsigned int nr_entries;
 
 		metadata_access_enable();
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+		{
+			nr_entries = stack_trace_save_iee(p->addrs, TRACK_ADDRS_COUNT, 3);
+		}
+		else
+			nr_entries = stack_trace_save(p->addrs, TRACK_ADDRS_COUNT, 3);
+		#else
 		nr_entries = stack_trace_save(p->addrs, TRACK_ADDRS_COUNT, 3);
+		#endif
 		metadata_access_disable();
 
 		if (nr_entries < TRACK_ADDRS_COUNT)
+			#ifdef CONFIG_IEE
+			if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+			{
+				tmp = *p;
+				tmp.addrs[nr_entries] = 0;
+				iee_set_track(p,&tmp);
+			}
+			else
+				p->addrs[nr_entries] = 0;
+			#else
 			p->addrs[nr_entries] = 0;
+			#endif
 #endif
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+		{
+			tmp = *p;
+			tmp.addr = addr;
+			tmp.cpu = smp_processor_id();
+			tmp.pid = current->pid;
+			tmp.when = jiffies;
+			iee_set_track(p,&tmp);
+		}
+		else
+		{
+			p->addr = addr;
+			p->cpu = smp_processor_id();
+			p->pid = current->pid;
+			p->when = jiffies;
+		}
+		#else
 		p->addr = addr;
 		p->cpu = smp_processor_id();
 		p->pid = current->pid;
 		p->when = jiffies;
+		#endif
 	} else {
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CRED_ISO) && strcmp(s->name, "cred_jar") == 0)
+			iee_memset(p, 0, sizeof(struct track));
+		else
+			memset(p, 0, sizeof(struct track));
+		#else
 		memset(p, 0, sizeof(struct track));
+		#endif
 	}
 }
 
@@ -775,15 +833,42 @@ static void init_object(struct kmem_cache *s, void *object, u8 val)
 	u8 *p = object;
 
 	if (s->flags & SLAB_RED_ZONE)
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CRED_ISO) && strcmp(s->name, "cred_jar") == 0)
+			iee_memset(p - s->red_left_pad, val, s->red_left_pad);
+		else
+			memset(p - s->red_left_pad, val, s->red_left_pad);
+		#else
 		memset(p - s->red_left_pad, val, s->red_left_pad);
+		#endif
 
 	if (s->flags & __OBJECT_POISON) {
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+		{
+			iee_memset(p, POISON_FREE, s->object_size - 1);
+			iee_memset(&p[s->object_size - 1],POISON_END,1);
+		}
+		else
+		{
+			memset(p, POISON_FREE, s->object_size - 1);
+			p[s->object_size - 1] = POISON_END;
+		}
+		#else
 		memset(p, POISON_FREE, s->object_size - 1);
 		p[s->object_size - 1] = POISON_END;
+		#endif
 	}
 
 	if (s->flags & SLAB_RED_ZONE)
+		#ifdef CONFIG_IEE
+		if(IS_ENABLED(CONFIG_CRED_ISO) && strcmp(s->name, "cred_jar") == 0)
+			iee_memset(p + s->object_size, val, s->inuse - s->object_size);
+		else
+			memset(p + s->object_size, val, s->inuse - s->object_size);
+		#else
 		memset(p + s->object_size, val, s->inuse - s->object_size);
+		#endif
 }
 
 static void restore_bytes(struct kmem_cache *s, char *message, u8 data,
@@ -1143,7 +1228,14 @@ void setup_page_debug(struct kmem_cache *s, struct page *page, void *addr)
 		return;
 
 	metadata_access_enable();
+	#ifdef CONFIG_IEE
+	if(IS_ENABLED(CONFIG_CRED_ISO) && strcmp(s->name, "cred_jar") == 0)
+		iee_memset(addr, POISON_INUSE, page_size(page));
+	else
+		memset(addr, POISON_INUSE, page_size(page));
+	#else
 	memset(addr, POISON_INUSE, page_size(page));
+	#endif
 	metadata_access_disable();
 }
 
@@ -1653,6 +1745,50 @@ static inline struct page *alloc_slab_page(struct kmem_cache *s,
 	else
 		page = __alloc_pages_node(node, flags, order);
 
+	#ifdef CONFIG_IEE
+	if(!page)
+		return page;
+
+	if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+	{
+		int i;
+		for(i = 0; i < (0x1 << order); i++)
+		{
+			unsigned long iee_addr = __phys_to_iee(page_to_phys(page)+i*PAGE_SIZE);
+			set_iee_page_valid(iee_addr);
+			iee_set_logical_mem_ro((unsigned long)page_address(page + i));
+		}
+	}
+
+	// If the page belongs to a task_struct, alloc token for it and set iee&lm va.
+	if(strcmp(s->name, "task_struct") == 0)
+	{
+		int i;
+		for(i = 0; i < (0x1 << order); i++)
+		{
+			void *token_addr = (void *)__phys_to_iee(page_to_phys(page + i));
+			// Get lm va of the page.
+			void *alloc_token = (void *)__get_free_page(GFP_KERNEL | __GFP_ZERO);
+			iee_set_token_page_valid(token_addr, alloc_token);
+			set_iee_page_valid(__phys_to_iee(__pa(alloc_token)));
+			iee_set_logical_mem_ro((unsigned long)alloc_token);
+		}
+	}
+    #else 
+    #ifdef CONFIG_KOI
+    if (!page)
+        return page;
+    if (strcmp(s->name, "task_struct") == 0) {
+        int i;
+        for (i = 0; i < (0x1 << order); i++) {
+            void *token_addr = __phys_to_virt(page_to_phys(page + i)) + koi_offset;
+            void *alloc_token = __get_free_page(GFP_KERNEL | __GFP_ZERO);
+            koi_add_page_mapping(token_addr, alloc_token);
+        }
+    }
+    #endif
+	#endif
+
 	if (page)
 		account_slab_page(page, order, s);
 
@@ -1878,6 +2014,67 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	if (current->reclaim_state)
 		current->reclaim_state->reclaimed_slab += pages;
 	unaccount_slab_page(page, order, s);
+
+	#ifdef CONFIG_IEE
+	if(IS_ENABLED(CONFIG_CREDP) && strcmp(s->name, "cred_jar") == 0)
+	{
+		int i;
+		for(i = 0; i < (0x1 << order); i++)
+		{
+			unsigned long iee_addr = __phys_to_iee(page_to_phys(page)+i*PAGE_SIZE);
+			set_iee_page_invalid(iee_addr);
+			iee_set_logical_mem_rw((unsigned long)page_address(page + i));
+		}
+	}
+	// If the page containing this token is empty, free it and restore iee&lm va.
+	if(strcmp(s->name, "task_struct") == 0)
+	{
+		int i;
+		for(i = 0; i < (0x1 << order); i++)
+		{
+			void *token_addr = (void *)__phys_to_iee(page_to_phys(page + i));
+			unsigned long flags;
+			unsigned long res;
+			local_irq_save(flags);
+			asm volatile("at s1e1r, %0"::"r"(token_addr));
+			isb();
+			res = read_sysreg(par_el1);
+			local_irq_restore(flags);
+			if(!(res & 0x1))
+			{
+				// Get lm va of the page.
+				void *token_page = __va(res & PTE_ADDR_MASK);
+				iee_set_token_page_invalid(token_addr);
+				set_iee_page_invalid(__phys_to_iee(__pa(token_page)));
+				iee_set_logical_mem_rw((unsigned long)token_page);
+				free_page((unsigned long)token_page);
+			}
+		}
+	}
+    #else
+    #ifdef CONFIG_KOI
+	if(strcmp(s->name, "task_struct") == 0)
+	{
+		int i;
+		for(i = 0; i < (0x1 << order); i++)
+		{
+			void *token_addr = __phys_to_virt(page_to_phys(page + i)) + koi_offset;
+			unsigned long flags;
+			local_irq_save(flags);
+			asm volatile("at s1e1r, %0"::"r"(token_addr));
+			isb();
+			unsigned long res = read_sysreg(par_el1);
+			local_irq_restore(flags);
+			if(!(res & 0x1))
+			{
+                koi_remove_page_mapping(token_addr);
+				free_page(__va(res & PTE_ADDR_MASK));
+			}
+		}
+	}
+    #endif
+	#endif
+
 	__free_pages(page, order);
 }
 
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 6803c89c5d21..a8a09fa170f2 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -30,6 +30,10 @@
 #include <linux/pgtable.h>
 #include <linux/bootmem_info.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 #include <asm/dma.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
@@ -571,6 +575,9 @@ pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,
 				       struct vmem_altmap *altmap)
 {
 	pte_t *pte = pte_offset_kernel(pmd, addr);
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 	if (pte_none(*pte)) {
 		pte_t entry;
 		void *p;
@@ -578,6 +585,13 @@ pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,
 		p = vmemmap_alloc_block_buf(PAGE_SIZE, node, altmap);
 		if (!p)
 			return NULL;
+
+		#ifdef CONFIG_PTP
+		iee_addr = __phys_to_iee(__pa(p));
+		set_iee_page_valid(iee_addr);
+		iee_set_logical_mem_ro((unsigned long)p);
+		#endif
+
 		entry = pfn_pte(__pa(p) >> PAGE_SHIFT, PAGE_KERNEL);
 		set_pte_at(&init_mm, addr, pte, entry);
 	}
@@ -587,11 +601,20 @@ pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,
 static void * __meminit vmemmap_alloc_block_zero(unsigned long size, int node)
 {
 	void *p = vmemmap_alloc_block(size, node);
+	#ifdef CONFIG_PTP
+	unsigned long iee_addr;
+	#endif
 
 	if (!p)
 		return NULL;
 	memset(p, 0, size);
 
+	#ifdef CONFIG_PTP
+	iee_addr = __phys_to_iee(__pa(p));
+	set_iee_page_valid(iee_addr);
+	iee_set_logical_mem_ro((unsigned long)p);
+	#endif
+	
 	return p;
 }
 
diff --git a/mm/swap.c b/mm/swap.c
index c37fac5a73e8..d4e47984668b 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -37,6 +37,10 @@
 #include <linux/page_idle.h>
 #include <linux/local_lock.h>
 
+#ifdef CONFIG_PTP
+#include <linux/iee-func.h>
+#endif
+
 #include "internal.h"
 
 #define CREATE_TRACE_POINTS
@@ -113,6 +117,15 @@ static void __put_compound_page(struct page *page)
 
 void __put_page(struct page *page)
 {
+	#ifdef CONFIG_PTP
+	if(page_count(page) == 0)
+	{
+		unsigned long iee_addr = __phys_to_iee(__pa(page_address(page)));
+	    set_iee_page_invalid(iee_addr);
+		iee_set_logical_mem_rw((unsigned long)page_address(page));
+	}
+	#endif
+	
 	if (is_zone_device_page(page)) {
 		put_dev_pagemap(page->pgmap);
 
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 6d802924d9e8..1003a4bb7c64 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -2853,7 +2853,11 @@ static int vmap_pfn_apply(pte_t *pte, unsigned long addr, void *private)
 
 	if (WARN_ON_ONCE(pfn_valid(data->pfns[data->idx])))
 		return -EINVAL;
+	#ifdef CONFIG_PTP
+	set_pte(pte, pte_mkspecial(pfn_pte(data->pfns[data->idx++], data->prot)));
+	#else
 	*pte = pte_mkspecial(pfn_pte(data->pfns[data->idx++], data->prot));
+	#endif
 	return 0;
 }
 
diff --git a/net/dns_resolver/dns_key.c b/net/dns_resolver/dns_key.c
index 3aced951d5ab..ae3ea09abf02 100644
--- a/net/dns_resolver/dns_key.c
+++ b/net/dns_resolver/dns_key.c
@@ -34,6 +34,10 @@
 #include <keys/user-type.h>
 #include "internal.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 MODULE_DESCRIPTION("DNS Resolver");
 MODULE_AUTHOR("Wang Lei");
 MODULE_LICENSE("GPL");
@@ -358,8 +362,13 @@ static int __init init_dns_resolver(void)
 	/* instruct request_key() to use this special keyring as a cache for
 	 * the results it looks up */
 	set_bit(KEY_FLAG_ROOT_CAN_CLEAR, &keyring->flags);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(cred,keyring);
+	iee_set_cred_jit_keyring(cred,KEY_REQKEY_DEFL_THREAD_KEYRING);
+	#else
 	cred->thread_keyring = keyring;
 	cred->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;
+	#endif
 	dns_resolver_cache = cred;
 
 	kdebug("DNS resolver keyring: %d\n", key_serial(keyring));
diff --git a/security/commoncap.c b/security/commoncap.c
index b44b69796c0b..0c13d4b9bc97 100644
--- a/security/commoncap.c
+++ b/security/commoncap.c
@@ -25,6 +25,10 @@
 #include <linux/binfmts.h>
 #include <linux/personality.h>
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 /*
  * If a non-root user executes a setuid-root binary in
  * !secure(SECURE_NOROOT) mode, then we raise capabilities.
@@ -265,6 +269,15 @@ int cap_capset(struct cred *new,
 	if (!cap_issubset(*effective, *permitted))
 		return -EPERM;
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_cap_effective(new,*effective);
+	iee_set_cred_cap_inheritable(new,*inheritable);
+	iee_set_cred_cap_permitted(new,*permitted);
+
+	iee_set_cred_cap_ambient(new,cap_intersect(new->cap_ambient,
+					 cap_intersect(*permitted,
+						       *inheritable)));
+	#else
 	new->cap_effective   = *effective;
 	new->cap_inheritable = *inheritable;
 	new->cap_permitted   = *permitted;
@@ -276,6 +289,7 @@ int cap_capset(struct cred *new,
 	new->cap_ambient = cap_intersect(new->cap_ambient,
 					 cap_intersect(*permitted,
 						       *inheritable));
+	#endif
 	if (WARN_ON(!cap_ambient_invariant_ok(new)))
 		return -EINVAL;
 	return 0;
@@ -569,9 +583,16 @@ static inline int bprm_caps_from_vfs_caps(struct cpu_vfs_cap_data *caps,
 		 * pP' = (X & fP) | (pI & fI)
 		 * The addition of pA' is handled later.
 		 */
+		#ifdef CONFIG_CREDP
+		kernel_cap_t temp = new->cap_permitted;
+		temp.cap[i] = (new->cap_bset.cap[i] & permitted) |
+			(new->cap_inheritable.cap[i] & inheritable);
+		iee_set_cred_cap_permitted(new,temp);
+		#else
 		new->cap_permitted.cap[i] =
 			(new->cap_bset.cap[i] & permitted) |
 			(new->cap_inheritable.cap[i] & inheritable);
+		#endif
 
 		if (permitted & ~new->cap_permitted.cap[i])
 			/* insufficient to execute correctly */
@@ -674,7 +695,13 @@ static int get_file_caps(struct linux_binprm *bprm, struct file *file,
 	int rc = 0;
 	struct cpu_vfs_cap_data vcaps;
 
+	#ifdef CONFIG_CREDP
+	do {
+		iee_set_cred_cap_permitted(bprm->cred, __cap_empty_set);
+	} while (0);
+	#else
 	cap_clear(bprm->cred->cap_permitted);
+	#endif
 
 	if (!file_caps_enabled)
 		return 0;
@@ -704,7 +731,13 @@ static int get_file_caps(struct linux_binprm *bprm, struct file *file,
 
 out:
 	if (rc)
+		#ifdef CONFIG_CREDP
+		do {
+			iee_set_cred_cap_permitted(bprm->cred, __cap_empty_set);
+		} while (0);
+		#else
 		cap_clear(bprm->cred->cap_permitted);
+		#endif
 
 	return rc;
 }
@@ -756,8 +789,13 @@ static void handle_privileged_root(struct linux_binprm *bprm, bool has_fcap,
 	 */
 	if (__is_eff(root_uid, new) || __is_real(root_uid, new)) {
 		/* pP' = (cap_bset & ~0) | (pI & ~0) */
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_permitted(new,cap_combine(old->cap_bset,
+						 old->cap_inheritable));
+		#else
 		new->cap_permitted = cap_combine(old->cap_bset,
 						 old->cap_inheritable);
+		#endif
 	}
 	/*
 	 * If only the real uid is 0, we do not set the effective bit.
@@ -864,34 +902,69 @@ int cap_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
 		/* downgrade; they get no more than they had, and maybe less */
 		if (!ns_capable(new->user_ns, CAP_SETUID) ||
 		    (bprm->unsafe & LSM_UNSAFE_NO_NEW_PRIVS)) {
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_euid(new,new->uid);
+			iee_set_cred_egid(new,new->gid);
+			#else
 			new->euid = new->uid;
 			new->egid = new->gid;
+			#endif
 		}
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_permitted(new,cap_intersect(new->cap_permitted,
+						   old->cap_permitted));
+		#else
 		new->cap_permitted = cap_intersect(new->cap_permitted,
 						   old->cap_permitted);
+		#endif
 	}
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_fsuid(new,new->euid);
+	iee_set_cred_suid(new,new->euid);
+	iee_set_cred_fsgid(new,new->egid);
+	iee_set_cred_sgid(new,new->egid);
+	#else
 	new->suid = new->fsuid = new->euid;
 	new->sgid = new->fsgid = new->egid;
+	#endif
 
 	/* File caps or setid cancels ambient. */
 	if (has_fcap || is_setid)
+		#ifdef CONFIG_CREDP
+		do {
+			iee_set_cred_cap_ambient(new, __cap_empty_set);
+		} while (0);
+		#else
 		cap_clear(new->cap_ambient);
+		#endif
 
 	/*
 	 * Now that we've computed pA', update pP' to give:
 	 *   pP' = (X & fP) | (pI & fI) | pA'
 	 */
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_cap_permitted(new,cap_combine(new->cap_permitted, new->cap_ambient));
+	#else
 	new->cap_permitted = cap_combine(new->cap_permitted, new->cap_ambient);
+	#endif
 
 	/*
 	 * Set pE' = (fE ? pP' : pA').  Because pA' is zero if fE is set,
 	 * this is the same as pE' = (fE ? pP' : 0) | pA'.
 	 */
 	if (effective)
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,new->cap_permitted);
+		#else
 		new->cap_effective = new->cap_permitted;
+		#endif
 	else
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,new->cap_ambient);
+		#else
 		new->cap_effective = new->cap_ambient;
+		#endif
 
 	if (WARN_ON(!cap_ambient_invariant_ok(new)))
 		return -EPERM;
@@ -902,7 +975,11 @@ int cap_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
 			return ret;
 	}
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_securebits(new,new->securebits & ~issecure_mask(SECURE_KEEP_CAPS));
+	#else
 	new->securebits &= ~issecure_mask(SECURE_KEEP_CAPS);
+	#endif
 
 	if (WARN_ON(!cap_ambient_invariant_ok(new)))
 		return -EPERM;
@@ -1028,8 +1105,17 @@ static inline void cap_emulate_setxuid(struct cred *new, const struct cred *old)
 	     !uid_eq(new->euid, root_uid) &&
 	     !uid_eq(new->suid, root_uid))) {
 		if (!issecure(SECURE_KEEP_CAPS)) {
+			#ifdef CONFIG_CREDP
+			do {
+				iee_set_cred_cap_permitted(new, __cap_empty_set);
+			} while (0);
+			do {
+				iee_set_cred_cap_effective(new, __cap_empty_set);
+			} while (0);
+			#else
 			cap_clear(new->cap_permitted);
 			cap_clear(new->cap_effective);
+			#endif
 		}
 
 		/*
@@ -1037,12 +1123,28 @@ static inline void cap_emulate_setxuid(struct cred *new, const struct cred *old)
 		 * by exec to drop capabilities.  We should make sure that
 		 * this remains the case.
 		 */
+		#ifdef CONFIG_CREDP
+		do {
+			iee_set_cred_cap_ambient(new, __cap_empty_set);
+		} while (0);
+		#else
 		cap_clear(new->cap_ambient);
+		#endif
 	}
 	if (uid_eq(old->euid, root_uid) && !uid_eq(new->euid, root_uid))
+		#ifdef CONFIG_CREDP
+		do {
+			iee_set_cred_cap_effective(new, __cap_empty_set);
+		} while (0);
+		#else
 		cap_clear(new->cap_effective);
+		#endif
 	if (!uid_eq(old->euid, root_uid) && uid_eq(new->euid, root_uid))
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_cap_effective(new,new->cap_permitted);
+		#else
 		new->cap_effective = new->cap_permitted;
+		#endif
 }
 
 /**
@@ -1076,13 +1178,22 @@ int cap_task_fix_setuid(struct cred *new, const struct cred *old, int flags)
 		if (!issecure(SECURE_NO_SETUID_FIXUP)) {
 			kuid_t root_uid = make_kuid(old->user_ns, 0);
 			if (uid_eq(old->fsuid, root_uid) && !uid_eq(new->fsuid, root_uid))
+				#ifdef CONFIG_CRED_ISO
+				iee_set_cred_cap_effective(new,cap_drop_fs_set(new->cap_effective));
+				#else
 				new->cap_effective =
 					cap_drop_fs_set(new->cap_effective);
+				#endif
 
 			if (!uid_eq(old->fsuid, root_uid) && uid_eq(new->fsuid, root_uid))
+				#ifdef CONFIG_CRED_ISO
+				iee_set_cred_cap_effective(new,cap_raise_fs_set(new->cap_effective,
+							 new->cap_permitted));
+				#else
 				new->cap_effective =
 					cap_raise_fs_set(new->cap_effective,
 							 new->cap_permitted);
+				#endif
 		}
 		break;
 
@@ -1171,7 +1282,15 @@ static int cap_prctl_drop(unsigned long cap)
 	new = prepare_creds();
 	if (!new)
 		return -ENOMEM;
+	#ifdef CONFIG_CREDP
+	{
+		kernel_cap_t tmp = new->cap_bset;
+		cap_lower(tmp, cap);
+		iee_set_cred_cap_bset(new, tmp);
+	}
+	#else
 	cap_lower(new->cap_bset, cap);
+	#endif
 	return commit_creds(new);
 }
 
@@ -1244,7 +1363,11 @@ int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,
 		new = prepare_creds();
 		if (!new)
 			return -ENOMEM;
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_securebits(new,arg2);
+		#else
 		new->securebits = arg2;
+		#endif
 		return commit_creds(new);
 
 	case PR_GET_SECUREBITS:
@@ -1263,9 +1386,17 @@ int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,
 		if (!new)
 			return -ENOMEM;
 		if (arg2)
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_securebits(new,new->securebits | issecure_mask(SECURE_KEEP_CAPS));
+			#else
 			new->securebits |= issecure_mask(SECURE_KEEP_CAPS);
+			#endif
 		else
+			#ifdef CONFIG_CRED_ISO
+			iee_set_cred_securebits(new,new->securebits & ~issecure_mask(SECURE_KEEP_CAPS));
+			#else
 			new->securebits &= ~issecure_mask(SECURE_KEEP_CAPS);
+			#endif
 		return commit_creds(new);
 
 	case PR_CAP_AMBIENT:
@@ -1276,7 +1407,13 @@ int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,
 			new = prepare_creds();
 			if (!new)
 				return -ENOMEM;
+			#ifdef CONFIG_CREDP
+			do {
+				iee_set_cred_cap_ambient(new, __cap_empty_set);
+			} while (0);
+			#else
 			cap_clear(new->cap_ambient);
+			#endif
 			return commit_creds(new);
 		}
 
@@ -1300,9 +1437,25 @@ int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,
 			if (!new)
 				return -ENOMEM;
 			if (arg2 == PR_CAP_AMBIENT_RAISE)
+				#ifdef CONFIG_CREDP
+				{
+					kernel_cap_t tmp = new->cap_ambient;
+					cap_raise(tmp, arg3);
+					iee_set_cred_cap_ambient(new, tmp);
+				}
+				#else
 				cap_raise(new->cap_ambient, arg3);
+				#endif
 			else
+				#ifdef CONFIG_CREDP
+				{
+					kernel_cap_t tmp = new->cap_ambient;
+					cap_lower(tmp, arg3);
+					iee_set_cred_cap_ambient(new, tmp);
+				}
+				#else
 				cap_lower(new->cap_ambient, arg3);
+				#endif
 			return commit_creds(new);
 		}
 
diff --git a/security/keys/keyctl.c b/security/keys/keyctl.c
index e3ffaf5ad639..44120d7ba8f3 100644
--- a/security/keys/keyctl.c
+++ b/security/keys/keyctl.c
@@ -23,6 +23,9 @@
 #include <linux/uaccess.h>
 #include <keys/request_key_auth-type.h>
 #include "internal.h"
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
 
 #define KEY_MAX_DESC_SIZE 4096
 
@@ -1155,7 +1158,11 @@ static int keyctl_change_reqkey_auth(struct key *key)
 		return -ENOMEM;
 
 	key_put(new->request_key_auth);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_request_key_auth(new,key_get(key));
+	#else
 	new->request_key_auth = key_get(key);
+	#endif
 
 	return commit_creds(new);
 }
@@ -1432,7 +1439,11 @@ long keyctl_set_reqkey_keyring(int reqkey_defl)
 	}
 
 set:
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_jit_keyring(new,reqkey_defl);
+	#else
 	new->jit_keyring = reqkey_defl;
+	#endif
 	commit_creds(new);
 	return old_setting;
 error:
@@ -1644,9 +1655,17 @@ long keyctl_session_to_parent(void)
 	cred = cred_alloc_blank();
 	if (!cred)
 		goto error_keyring;
+	#ifdef CONFIG_CREDP
+	newwork = (struct rcu_head *)(cred->rcu.func);
+	#else
 	newwork = &cred->rcu;
+	#endif
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_session_keyring(cred,key_ref_to_ptr(keyring_r));
+	#else
 	cred->session_keyring = key_ref_to_ptr(keyring_r);
+	#endif
 	keyring_r = NULL;
 	init_task_work(newwork, key_change_session_keyring);
 
@@ -1705,7 +1724,11 @@ long keyctl_session_to_parent(void)
 	write_unlock_irq(&tasklist_lock);
 	rcu_read_unlock();
 	if (oldwork)
+		#ifdef CONFIG_CREDP
+		put_cred(*(struct cred **)(oldwork + 1));
+		#else
 		put_cred(container_of(oldwork, struct cred, rcu));
+		#endif
 	if (newwork)
 		put_cred(cred);
 	return ret;
diff --git a/security/keys/process_keys.c b/security/keys/process_keys.c
index 1fe8b934f656..f5a74c5b421d 100644
--- a/security/keys/process_keys.c
+++ b/security/keys/process_keys.c
@@ -19,6 +19,10 @@
 #include <keys/request_key_auth-type.h>
 #include "internal.h"
 
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
+
 /* Session keyring create vs join semaphore */
 static DEFINE_MUTEX(key_session_mutex);
 
@@ -232,7 +236,11 @@ int install_thread_keyring_to_cred(struct cred *new)
 	if (IS_ERR(keyring))
 		return PTR_ERR(keyring);
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_thread_keyring(new,keyring);
+	#else
 	new->thread_keyring = keyring;
+	#endif
 	return 0;
 }
 
@@ -279,7 +287,11 @@ int install_process_keyring_to_cred(struct cred *new)
 	if (IS_ERR(keyring))
 		return PTR_ERR(keyring);
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_process_keyring(new,keyring);
+	#else
 	new->process_keyring = keyring;
+	#endif
 	return 0;
 }
 
@@ -338,7 +350,11 @@ int install_session_keyring_to_cred(struct cred *cred, struct key *keyring)
 
 	/* install the keyring */
 	old = cred->session_keyring;
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_session_keyring(cred,keyring);
+	#else
 	cred->session_keyring = keyring;
+	#endif
 
 	if (old)
 		key_put(old);
@@ -910,13 +926,41 @@ long join_session_keyring(const char *name)
 void key_change_session_keyring(struct callback_head *twork)
 {
 	const struct cred *old = current_cred();
+	#ifdef CONFIG_CREDP
+	struct cred *new = *(struct cred **)(twork + 1);
+	#else
 	struct cred *new = container_of(twork, struct cred, rcu);
+	#endif
 
 	if (unlikely(current->flags & PF_EXITING)) {
 		put_cred(new);
 		return;
 	}
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_uid(new,old->  uid);
+	iee_set_cred_euid(new,old-> euid);
+	iee_set_cred_suid(new,old-> suid);
+	iee_set_cred_fsuid(new,old->fsuid);
+	iee_set_cred_gid(new,old->  gid);
+	iee_set_cred_egid(new,old-> egid);
+	iee_set_cred_sgid(new,old-> sgid);
+	iee_set_cred_fsgid(new,old->fsgid);
+	iee_set_cred_user(new,get_uid(old->user));
+	iee_set_cred_user_ns(new,get_user_ns(old->user_ns));
+	iee_set_cred_group_info(new,get_group_info(old->group_info));
+
+	iee_set_cred_securebits(new,old->securebits);
+	iee_set_cred_cap_inheritable(new,old->cap_inheritable);
+	iee_set_cred_cap_permitted(new,old->cap_permitted);
+	iee_set_cred_cap_effective(new,old->cap_effective);
+	iee_set_cred_cap_ambient(new,old->cap_ambient);
+	iee_set_cred_cap_bset(new,old->cap_bset);
+
+	iee_set_cred_jit_keyring(new,old->jit_keyring);
+	iee_set_cred_thread_keyring(new,key_get(old->thread_keyring));
+	iee_set_cred_process_keyring(new,key_get(old->process_keyring));
+	#else
 	new->  uid	= old->  uid;
 	new-> euid	= old-> euid;
 	new-> suid	= old-> suid;
@@ -939,6 +983,7 @@ void key_change_session_keyring(struct callback_head *twork)
 	new->jit_keyring	= old->jit_keyring;
 	new->thread_keyring	= key_get(old->thread_keyring);
 	new->process_keyring	= key_get(old->process_keyring);
+	#endif
 
 	security_transfer_creds(new, old);
 
diff --git a/security/security.c b/security/security.c
index 606601190116..80071ac3a457 100644
--- a/security/security.c
+++ b/security/security.c
@@ -29,6 +29,9 @@
 #include <linux/string.h>
 #include <linux/msg.h>
 #include <net/flow.h>
+#ifdef CONFIG_CREDP
+#include <asm/iee-cred.h>
+#endif
 
 #define MAX_LSM_EVM_XATTR	2
 
@@ -531,11 +534,19 @@ EXPORT_SYMBOL(unregister_blocking_lsm_notifier);
 static int lsm_cred_alloc(struct cred *cred, gfp_t gfp)
 {
 	if (blob_sizes.lbs_cred == 0) {
+		#ifdef CONFIG_CRED_ISO
+		iee_set_cred_security(cred,NULL);
+		#else
 		cred->security = NULL;
+		#endif
 		return 0;
 	}
 
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_security(cred,kzalloc(blob_sizes.lbs_cred, gfp));
+	#else
 	cred->security = kzalloc(blob_sizes.lbs_cred, gfp);
+	#endif
 	if (cred->security == NULL)
 		return -ENOMEM;
 	return 0;
@@ -1681,7 +1692,11 @@ void security_cred_free(struct cred *cred)
 	call_void_hook(cred_free, cred);
 
 	kfree(cred->security);
+	#ifdef CONFIG_CRED_ISO
+	iee_set_cred_security(cred,NULL);
+	#else
 	cred->security = NULL;
+	#endif
 }
 
 int security_prepare_creds(struct cred *new, const struct cred *old, gfp_t gfp)
-- 
2.33.0

